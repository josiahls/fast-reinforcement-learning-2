{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp basic_agents\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "from dataclasses import asdict,dataclass\n",
    "from typing import Callable,Tuple,Union\n",
    "# from fastai.torch_core import *\n",
    "# from fastai.basic_data import *\n",
    "# from fastai.basic_train import *\n",
    "from fastai.basics import *\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "from fastcore.foundation import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import pytest\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Action Selection\n",
    "\n",
    "> Methods of exploratively selecting actions based on a model state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ActionSelector:\n",
    "    \"Abstract class which converts scores to the actions.\"\n",
    "    def __call__(self,scores):raise NotImplementedError\n",
    "\n",
    "class ArgmaxActionSelector(ActionSelector):\n",
    "    \"Selects actions using argmax.\"\n",
    "    def __call__(self,scores):\n",
    "        assert isinstance(scores,np.ndarray)\n",
    "        return np.argmax(scores,axis=1)\n",
    "\n",
    "@dataclass\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "    epsilon:float=0.05\n",
    "    selector:ActionSelector=ArgmaxActionSelector()\n",
    "\n",
    "    def __call__(self,scores):\n",
    "        assert isinstance(scores,np.ndarray)\n",
    "        bs,n_a=scores.shape\n",
    "        a=self.selector(scores)\n",
    "        mask=np.random.random(size=bs)<self.epsilon\n",
    "        rand_a=np.random.choice(n_a, sum(mask))\n",
    "        a[mask]=rand_a\n",
    "        return a\n",
    "\n",
    "class ProbabilityActionSelector(ActionSelector):\n",
    "    \"Converts probabilities of actions into action by sampling them.\"\n",
    "    def __call__(self,probs):\n",
    "        assert isinstance(probs,np.ndarray)\n",
    "        actions=[np.random.choice(len(prob),p=prob) for prob in probs]\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Agents\n",
    "\n",
    "> Basic Agent classes for handling models and actions. Details, please ref `basic_train`\n",
    "\n",
    "There is an important difference between `Learner`'s, `nn.Module`'s, and `Agent`'s. \n",
    "\n",
    "`Learners`:\n",
    "- Ref `basic_train`\n",
    "\n",
    "`nn.Module`:\n",
    "- Contain only `pytorch` related code.\n",
    "- Function as the brain of any of these agents and are the objects to be optimized.\n",
    "- Are highly portable, however for runtime usage are too \"dumb\" or simple to be practical. If by themselves, extra code needs to wrap them to handle environments.\n",
    "\n",
    "`Agent` (`agent_core`):\n",
    "- Contain a `nn.Module` and a limited number of `fastrl` objects. Unlike `nn.Module`, these can maintain a state.\n",
    "- Function as the interface between the `nn.Module` and the environments. They have 2 goals:\n",
    "    - Convert states into something the `nn.Module` can interpret.\n",
    "    - Modify the `nn.Module` output (actions) for randomized exploration.\n",
    "- Designed to be highly portable only requiring `basic_agents` as a dependency. These should allow for easily saving, and using in environments where `fastrl` might not necessarily be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def default_states_preprocessor(s,dtype=np.float32):\n",
    "    \"Convert list of states into the form suitable for model. By default we assume Variable.\"\n",
    "    np_s=np.expand_dims(s,0) if len(np.array(s).shape)==1 else np.array(s, copy=False)\n",
    "    return torch.tensor(np_s.astype(dtype))\n",
    "\n",
    "def float32_preprocessor(s):\n",
    "    np_s=np.array(s, dtype=np.float32)\n",
    "    return torch.tensor(np_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ptan\n",
    "\n",
    "@dataclass\n",
    "class BaseAgent(ptan.agent.BaseAgent):\n",
    "    model:nn.Module=None # If None, learner will set\n",
    "    def initial_state(self):return None\n",
    "    def __call__(self,sl,asl,include_batch_dim=False):\n",
    "        assert isinstance(sl,(list,np.ndarray))\n",
    "        assert isinstance(asl,(list,np.ndarray))\n",
    "        assert len(asl)==len(sl)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "@dataclass\n",
    "class TestAgent(BaseAgent):\n",
    "    env:object=None\n",
    "    def initial_state(self):return None\n",
    "    def __call__(self,sl,asl=None,include_batch_dim=False):\n",
    "        if type(self.env)!=list:return self.env.action_space.sample(),None\n",
    "        return self.env[0].action_space.sample(),None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(BaseAgent.initial_state,\"Should create initial empty state for the agent. It will be called for the start of the episode.\")\n",
    "add_docs(BaseAgent.__call__,textwrap.fill(\"\"\"Convert observations and state list `sl` into actions to take. Agent state list `asl` may also be used by the agent.\n",
    "         It is expected that `asl` is likely either going to be an internal state tracked by the agent, or it is simply a parameter used during subclassing.\n",
    "         The `include_batch_dim` should toggle whether to remove/include the batch dim of an action. Naturally, `gym` envs don't understand batch dimensions.\"\"\"\n",
    "        ))\n",
    "add_docs(BaseAgent,\"Abstract Agent interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7FF5D8B9A3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image\n",
    "\n",
    "env=gym.make('CartPole-v1')\n",
    "agent=TestAgent(env=env)\n",
    "\n",
    "done,episode_count,max_episodes=True,0,10\n",
    "\n",
    "while True:\n",
    "    if done:s=env.reset()\n",
    "    s,done,r,_=env.step(agent(s)[0])\n",
    "    display.clear_output(wait=True)\n",
    "    im=env.render(mode='rgb_array')\n",
    "    new_im=PIL.Image.fromarray(im)\n",
    "    display.display(new_im)\n",
    "    if done and episode_count>max_episodes:break\n",
    "    if done:episode_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class DiscreteAgent(BaseAgent):\n",
    "    \"DiscreteAgent a simple discrete action selector.\"\n",
    "    a_selector:ActionSelector=None\n",
    "    device:str=None\n",
    "    preprocessor:Callable=default_states_preprocessor\n",
    "    apply_softmax:bool=False\n",
    "        \n",
    "    def safe_unbatch(self,o:np.array)->np.array:return o[0] if o.shape[0]==1 and len(o.shape)>1 else o\n",
    "    def split_v(self,v,asl): return v,asl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self,x,asl=None,include_batch_dim=False):\n",
    "        x=self.preprocessor(x) if self.preprocessor is not None else s\n",
    "        asl= np.zeros(x.shape) if asl is None else asl\n",
    "        if torch.is_tensor(x): \n",
    "            x=x.to(self.device)\n",
    "        v=self.model(x)\n",
    "        if type(v)==tuple:v,asl=self.split_v(v,asl)\n",
    "        if self.apply_softmax:\n",
    "            v=F.softmax(v,dim=1)\n",
    "        q=v.data.cpu().numpy()\n",
    "        al=self.a_selector(q)\n",
    "        if not include_batch_dim:al=self.safe_unbatch(al).tolist()\n",
    "        return al if len(al)!=1 or include_batch_dim else al[0],asl\n",
    "\n",
    "@dataclass\n",
    "class DQNAgent(DiscreteAgent):\n",
    "    \"DQNAgent is a memoryless DQN agent which calculates Q values from the observations and  converts them into the actions using a_selector.\"\n",
    "    def __post_init__(self):\n",
    "        self.a_selector=ifnone(self.a_selector,ArgmaxActionSelector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(DQNAgent,__call__='DQNAgents will likely never have `asl` passed and used. This is however here for novel DQN implimentations.',\n",
    "         safe_unbatch='Will remove the batch dim from `o` if `o` represents a single item.',\n",
    "         split_v='In the event that `v` is a tuple, then there is multle ouputs from the `model`. Primarly used for A2C.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7FF5D8B37FD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "agent=DQNAgent(model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2)).to(default_device()),device=default_device())\n",
    "\n",
    "done,episode_count,max_episodes=True,0,10\n",
    "\n",
    "while True:\n",
    "    if done:s=env.reset()\n",
    "    s,done,r,_=env.step(agent(s)[0])\n",
    "    display.clear_output(wait=True)\n",
    "    im=env.render(mode='rgb_array')\n",
    "    new_im=PIL.Image.fromarray(im)\n",
    "    display.display(new_im)\n",
    "    if done and episode_count>max_episodes:break\n",
    "    if done:episode_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TargetNet:\n",
    "    \"Wrapper around model which provides copy of it instead of trained weights.\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def sync(self):self.target_model.load_state_dict(self.model.state_dict())\n",
    "    def alpha_sync(self,alpha):\n",
    "        \"Blend params of target net with params from the model.\"\n",
    "        assert isinstance(alpha,float)\n",
    "        assert 0.0<alpha<=1.0\n",
    "        state=self.model.state_dict()\n",
    "        tgt_state=self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k]=tgt_state[k]*alpha+(1-alpha)*v\n",
    "        self.target_model.load_state_dict(tgt_state)\n",
    "\n",
    "@dataclass\n",
    "class PolicyAgent(DiscreteAgent):\n",
    "    \"Policy agent gets action probabilities from the model and samples actions from it.\"\n",
    "    def __post_init__(self):\n",
    "        self.a_selector=ifnone(self.a_selector,ProbabilityActionSelector())\n",
    "        self.apply_softmax=True\n",
    "\n",
    "class ActorCriticAgent(PolicyAgent):\n",
    "    \"Policy agent which returns policy and value tensors from observations. Value are stored in agent's state \\\n",
    "     and could be reused for rollouts calculations by ExperienceSource.\"\n",
    "    def split_v(self,v,asl):\n",
    "#         v=v\n",
    "        return v[0],v[1].cpu().detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(LinearA2C, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx=x.float()\n",
    "        return self.policy(fx)\n",
    "model=LinearA2C((4,),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0442, -0.0213,  0.0335, -0.0114]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, array([[0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent=PolicyAgent(model=model.to(default_device()),device=default_device())\n",
    "agent([ 0.044186,-0.021265,0.033516,-0.011447])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_data.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 06_basic_train.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14_actorcritic.sac.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/03_basic_agents.ipynb\n",
      "converting: /opt/project/fastrl/nbs/16_actorcritic.a2c.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
