{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp basic_agents\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "from dataclasses import asdict,dataclass\n",
    "from typing import Callable,Tuple,Union\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "from fastcore.foundation import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s line:%(lineno)d %(levelname)s - %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "_logger=logging.getLogger(__name__)\n",
    "_logger.setLevel('INFO')\n",
    "\n",
    "from fastai.torch_core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "import pytest\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Action Selection\n",
    "\n",
    "> Methods of exploratively selecting actions based on a model state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ActionSelector:\n",
    "    \"Abstract class which converts scores to the actions.\"\n",
    "    def __call__(self,scores):raise NotImplementedError\n",
    "\n",
    "class ArgmaxActionSelector(ActionSelector):\n",
    "    \"Selects actions using argmax.\"\n",
    "    def __call__(self,scores):\n",
    "        assert isinstance(scores,np.ndarray)\n",
    "        return np.argmax(scores,axis=1)\n",
    "\n",
    "@dataclass\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "    epsilon:float=0.05\n",
    "    selector:ActionSelector=ArgmaxActionSelector()\n",
    "\n",
    "    def __call__(self,scores):\n",
    "        assert isinstance(scores,np.ndarray)\n",
    "        bs,n_a=scores.shape\n",
    "        a=self.selector(scores)\n",
    "        mask=np.random.random(size=bs)<self.epsilon\n",
    "        rand_a=np.random.choice(n_a, sum(mask))\n",
    "        a[mask]=rand_a\n",
    "        return a\n",
    "\n",
    "class ProbabilityActionSelector(ActionSelector):\n",
    "    \"Converts probabilities of actions into action by sampling them.\"\n",
    "    def __call__(self,probs):\n",
    "        assert isinstance(probs,np.ndarray)\n",
    "        actions=[np.random.choice(len(prob),p=prob) for prob in probs]\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Agents\n",
    "\n",
    "> Basic Agent classes for handling models and actions. Details, please ref `basic_train`\n",
    "\n",
    "There is an important difference between `Learner`'s, `nn.Module`'s, and `Agent`'s. \n",
    "\n",
    "`Learners`:\n",
    "- Ref `basic_train`\n",
    "\n",
    "`nn.Module`:\n",
    "- Contain only `pytorch` related code.\n",
    "- Function as the brain of any of these agents and are the objects to be optimized.\n",
    "- Are highly portable, however for runtime usage are too \"dumb\" or simple to be practical. If by themselves, extra code needs to wrap them to handle environments.\n",
    "\n",
    "`Agent` (`agent_core`):\n",
    "- Contain a `nn.Module` and a limited number of `fastrl` objects. Unlike `nn.Module`, these can maintain a state.\n",
    "- Function as the interface between the `nn.Module` and the environments. They have 2 goals:\n",
    "    - Convert states into something the `nn.Module` can interpret.\n",
    "    - Modify the `nn.Module` output (actions) for randomized exploration.\n",
    "- Designed to be highly portable only requiring `basic_agents` as a dependency. These should allow for easily saving, and using in environments where `fastrl` might not necessarily be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def default_states_preprocessor(s,dtype=np.float32):\n",
    "    \"Convert list of states into the form suitable for model. By default we assume Variable.\"\n",
    "    np_s=np.expand_dims(s,0) if len(np.array(s).shape)==1 else np.array(s, copy=False)\n",
    "    return torch.tensor(np_s.astype(dtype))\n",
    "\n",
    "def float32_preprocessor(s):\n",
    "    np_s=np.array(s, dtype=np.float32)\n",
    "    return torch.tensor(np_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class BaseAgent(object):\n",
    "    model:nn.Module=None # If None, learner will set\n",
    "    def initial_state(self):return None\n",
    "    def __call__(self,sl,asl,include_batch_dim=False):\n",
    "        assert isinstance(sl,(list,np.ndarray))\n",
    "        assert isinstance(asl,(list,np.ndarray))\n",
    "        assert len(asl)==len(sl)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "@dataclass\n",
    "class TestAgent(BaseAgent):\n",
    "    env:object=None\n",
    "    def initial_state(self):return None\n",
    "    def __call__(self,sl,asl=None,include_batch_dim=False):\n",
    "        if type(self.env)!=list:return self.env.action_space.sample(),None\n",
    "        return self.env[0].action_space.sample(),None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(BaseAgent.initial_state,\"Should create initial empty state for the agent. It will be called for the start of the episode.\")\n",
    "add_docs(BaseAgent.__call__,textwrap.fill(\"\"\"Convert observations and state list `sl` into actions to take. Agent state list `asl` may also be used by the agent.\n",
    "         It is expected that `asl` is likely either going to be an internal state tracked by the agent, or it is simply a parameter used during subclassing.\n",
    "         The `include_batch_dim` should toggle whether to remove/include the batch dim of an action. Naturally, `gym` envs don't understand batch dimensions.\"\"\"\n",
    "        ))\n",
    "add_docs(BaseAgent,\"Abstract Agent interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7EFCD807BC50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image\n",
    "\n",
    "env=gym.make('CartPole-v1')\n",
    "agent=TestAgent(env=env)\n",
    "\n",
    "done,episode_count,max_episodes=True,0,10\n",
    "\n",
    "while True:\n",
    "    if done:s=env.reset()\n",
    "    s,done,r,_=env.step(agent(s)[0])\n",
    "    display.clear_output(wait=True)\n",
    "    im=env.render(mode='rgb_array')\n",
    "    new_im=PIL.Image.fromarray(im)\n",
    "    display.display(new_im)\n",
    "    if done and episode_count>max_episodes:break\n",
    "    if done:episode_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class DiscreteAgent(BaseAgent):\n",
    "    \"DQNAgent is a memoryless DQN agent which calculates Q values from the observations and  converts them into the actions using a_selector.\"\n",
    "    a_selector:ActionSelector=None\n",
    "    device:str=None\n",
    "    preprocessor:Callable=default_states_preprocessor\n",
    "    apply_softmax:bool=False\n",
    "        \n",
    "    def safe_unbatch(self,o:np.array)->np.array:return o[0] if o.shape[0]==1 and len(o.shape)>1 else o\n",
    "    def split_v(v,asl): return v,asl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self,s,asl=None,include_batch_dim=False)->Tuple[np.array,np.array]:\n",
    "        s=self.preprocessor(s) if self.preprocessor is not None else s\n",
    "        asl= np.zeros(s.shape) if asl is None else asl\n",
    "        s=s.to(self.device) if torch.is_tensor(s) else s\n",
    "        v=self.model(s)\n",
    "        if type(v)==tuple:v,asl=self.split(v,asl)\n",
    "        if self.apply_softmax:\n",
    "            v=F.softmax(v,dim=1)\n",
    "        q=v.data.cpu().numpy()\n",
    "        al=self.a_selector(q)\n",
    "        if not include_batch_dim:al=self.safe_unbatch(al).tolist()\n",
    "        return al if len(al)!=1 or include_batch_dim else al[0],asl\n",
    "\n",
    "@dataclass\n",
    "class DQNAgent(DiscreteAgent):\n",
    "    \"DQNAgent is a memoryless DQN agent which calculates Q values from the observations and  converts them into the actions using a_selector.\"\n",
    "    def __post_init__(self):\n",
    "        self.a_selector=ifnone(self.a_selector,ArgmaxActionSelector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(DQNAgent,__call__='DQNAgents will likely never have `asl` passed and used. This is however here for novel DQN implimentations.',\n",
    "         safe_unbatch='Will remove the batch dim from `o` if `o` represents a single item.',\n",
    "         split_v='In the event that `v` is a tuple, then there is multle ouputs from the `model`. Primarly used for A2C.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7EFCB0610550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "agent=DQNAgent(model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2)))\n",
    "\n",
    "done,episode_count,max_episodes=True,0,10\n",
    "\n",
    "while True:\n",
    "    if done:s=env.reset()\n",
    "    s,done,r,_=env.step(agent(s)[0])\n",
    "    display.clear_output(wait=True)\n",
    "    im=env.render(mode='rgb_array')\n",
    "    new_im=PIL.Image.fromarray(im)\n",
    "    display.display(new_im)\n",
    "    if done and episode_count>max_episodes:break\n",
    "    if done:episode_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNet:\n",
    "    \"Wrapper around model which provides copy of it instead of trained weights.\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def sync(self):self.target_model.load_state_dict(self.model.state_dict())\n",
    "    def alpha_sync(self,alpha):\n",
    "        \"Blend params of target net with params from the model.\"\n",
    "        assert isinstance(alpha,float)\n",
    "        assert 0.0<alpha<=1.0\n",
    "        state=self.model.state_dict()\n",
    "        tgt_state=self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k]=tgt_state[k]*alpha+(1-alpha)*v\n",
    "        self.target_model.load_state_dict(tgt_state)\n",
    "\n",
    "@dataclass\n",
    "class PolicyAgent(DiscreteAgent):\n",
    "    \"Policy agent gets action probabilities from the model and samples actions from it.\"\n",
    "    def __post_init__(self):\n",
    "        self.a_selector=ifnone(self.a_selector,actions.ProbabilityActionSelector())\n",
    "        self.apply_softmax=True\n",
    "\n",
    "class ActorCriticAgent(PolicyAgent):\n",
    "    \"Policy agent which returns policy and value tensors from observations. Value are stored in agent's state \\\n",
    "     and could be reused for rollouts calculations by ExperienceSource.\"\n",
    "    def split_v(v,asl):return v,v.data.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 02_callbacks.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 05_data_block.ipynb.\n",
      "Converted 06_basic_train.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n",
      "converting: /opt/project/fastrl/nbs/03_basic_agents.ipynb\n",
      "converting: /opt/project/fastrl/nbs/05_data_block.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
