{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev']  # upgrade fastrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Local modules\n",
    "from fastrl.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time,sys\n",
    "import torch.multiprocessing as mp\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-fight",
   "metadata": {},
   "source": [
    "# Data Block\n",
    "> Fastrl transforms for iterating through environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DQN(Module):\n",
    "    def __init__(self):\n",
    "        self.policy=nn.Sequential(\n",
    "            nn.Linear(4,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x): \n",
    "        return torch.argmax(self.policy(x),dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-walnut",
   "metadata": {},
   "source": [
    "Development of this was helped by [IterableData documentation on multiple workers](https://github.com/pytorch/pytorch/blob/4949eea0ffb60dc81a0a78402fa59fdf68206718/torch/utils/data/dataset.py#L64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-bunch",
   "metadata": {},
   "source": [
    "This code is heavily modifed from https://github.com/Shmuma/ptan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-reply",
   "metadata": {},
   "source": [
    "Reference for env [semantics related to vectorized environments](https://github.com/openai/universe/blob/master/doc/env_semantics.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-orientation",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "- [torch multiprocessing](https://github.com/pytorch/pytorch/blob/a61a8d059efa0fb139a09e479b1a2c8dd1cf1a44/torch/utils/data/dataloader.py#L564)\n",
    "- [torch worker](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/worker.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def o2tensor_batch(o): \n",
    "    if not isinstance(o,Tensor): o=Tensor(o if is_listy(o) or isinstance(o,np.ndarray) else [o])\n",
    "    if o.size()[0]==1 and len(o.size())>1: return o\n",
    "    return o.unsqueeze(0)\n",
    "\n",
    "def init_experience(but='',**kwargs): \n",
    "    \"Returns dictionary with default values that can be overridden.\"\n",
    "    experience=D(\n",
    "        state=0,action=0,next_state=0,reward=0,done=False,\n",
    "        step=0,env=0,image=0\n",
    "    )\n",
    "    for s in but.split(','):\n",
    "        if s in experience:del experience[s]\n",
    "    return D(merge(experience,kwargs)).mapv(o2tensor_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-greeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': tensor([[0.]]),\n",
       " 'action': tensor([[0.]]),\n",
       " 'next_state': tensor([[0.]]),\n",
       " 'reward': tensor([[0.]]),\n",
       " 'done': tensor([[0.]]),\n",
       " 'step': tensor([[0.]]),\n",
       " 'env': tensor([[0.]]),\n",
       " 'image': tensor([[0.]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-authority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': tensor([[0.]]),\n",
       " 'action': tensor([[0.]]),\n",
       " 'next_state': tensor([[0.]]),\n",
       " 'reward': tensor([[0.]]),\n",
       " 'done': tensor([[0.]]),\n",
       " 'env': tensor([[0.]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_experience(but='image,step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'action': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'next_state': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'reward': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'done': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'step': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'env': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 'image': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([init_experience(),init_experience()],init_experience())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _state2experience(s,**kwargs):   return init_experience(state=s,step=torch.zeros((1,1)),**kwargs)\n",
    "def _env_reset(o):                   return o.reset()\n",
    "def _env_seed(o,seed):               return o.seed(seed)\n",
    "def _env_render(o,mode='rgb_array'): return Tensor(o.render(mode=mode).copy())\n",
    "def _env_step(o,*args,**kwargs):     return o.step(*args,**kwargs)\n",
    "\n",
    "class FakeAgent:\n",
    "    def __init__(self,action_space): store_attr()\n",
    "    def __call__(self,state,**kwargs):\n",
    "        return L([self.action_space.sample() for _ in range(state.shape[0])]),D(kwargs)\n",
    "\n",
    "class ExperienceSource(Stateful):\n",
    "    _stateattrs=('pool',)\n",
    "    def __init__(self,env:str,agent=None,n_envs:int=1,steps_count:int=1,steps_delta:int=1,\n",
    "                 seed:int=None,render=None,num_workers=0,but='',**kwargs):\n",
    "        store_attr()\n",
    "        self.env_kwargs=kwargs\n",
    "        self.pool=L()\n",
    "        if self.render is None: self.but+=',image'\n",
    "\n",
    "    def _init_state(self):\n",
    "        \"Inits the histories, experiences, and the environment pool when sent to a `Process`\"\n",
    "        self.history,self.pool=L((deque(maxlen=self.steps_count),\n",
    "                                  gym.make(self.env,**self.env_kwargs)) \n",
    "                                  for _ in range(self.n_envs)).zip().map(L) \n",
    "        self.pool.map(_env_seed,seed=self.seed)\n",
    "        if self.agent is None: self.agent=FakeAgent(self.pool[0].action_space)\n",
    "        self.reset_all()\n",
    "        \n",
    "    def reset_all(self):\n",
    "        self.experiences=self.pool.map(_env_reset)\n",
    "        self.experiences=self.experiences.map(_state2experience,but=self.but)\n",
    "        self.experiences=sum(self.experiences[1:],self.experiences[0])\n",
    "        self.attempt_render(self.experiences)\n",
    "        \n",
    "    def attempt_render(self,experiences,indexes=None):\n",
    "        if self.render is not None: \n",
    "            pool=self.pool if indexes is None else self.pool[indexes]\n",
    "            renders=pool.map(_env_render,mode=self.render)\n",
    "            # No idea why we have to do this, but multiprocessing hangs forever otherwise\n",
    "            if self.num_workers>0:sleep(0.1) \n",
    "            experiences['image']=torch.stack(tuple(renders))\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"Iterates through a list of environments.\"\n",
    "        if not self.pool:self._init_state()\n",
    "        while True: \n",
    "#             try:\n",
    "            # Only work on envs that are not done\n",
    "            not_done_idxs=(self.experiences['done']==False).nonzero()[:,0]\n",
    "            if len(not_done_idxs)==0: \n",
    "                self.reset_all()\n",
    "                not_done_idxs=(self.experiences['done']==False).nonzero()[:,0]\n",
    "            not_done_experiences=self.experiences.filter(indexes=not_done_idxs)\n",
    "            # Pass current experiences into agent\n",
    "            actions,experiences=self.agent(**not_done_experiences)\n",
    "            # Step through all envs.\n",
    "            step_res=self.pool[not_done_idxs].zipwith(actions).starmap(_env_step)\n",
    "            next_states,rewards,dones=step_res.zip()[:3].map(Tensor)\n",
    "            # Add the image field if available\n",
    "            self.attempt_render(self.experiences,not_done_idxs)\n",
    "            \n",
    "            new_exp=D(next_state=next_states,reward=rewards,done=dones,\n",
    "                      env=not_done_idxs,step=not_done_experiences['step']+1)\n",
    "            experiences=D(merge(not_done_experiences,experiences,new_exp))\n",
    "            # TODO: Ugly, I shouldn't have to do this\n",
    "            for k in experiences: \n",
    "                if self.experiences[k].shape!=experiences[k].shape:\n",
    "                    self.experiences[k]=torch.zeros((self.experiences[k].shape[0],\n",
    "                                                     *experiences[k].shape[1:]))\n",
    "                if torch.is_floating_point(experiences[k]): \n",
    "                    self.experiences[k]=self.experiences[k].float()\n",
    "                else:\n",
    "                    self.experiences[k]=self.experiences[k].long()\n",
    "                self.experiences[k][not_done_idxs]=experiences[k]\n",
    "            \n",
    "            if self.n_envs>1:\n",
    "                experiences=parallel(partial(experiences.subset),not_done_idxs,\n",
    "                                       threadpool=True,n_workers=0 if self.num_workers>0 else 2,progress=False)\n",
    "            else:\n",
    "                experiences=[experiences.subset(not_done_idxs)]\n",
    "            # TODO: Ugly, I shouldn't have to do this\n",
    "            if 'image' in experiences[0]: \n",
    "                experiences=[merge(d,{'image':d['image'].unsqueeze(0)}) for d in experiences]\n",
    "                \n",
    "                \n",
    "            experiences=[{k:(e[k].unsqueeze(0) if not e[k].shape or e[k].shape[0]!=1 else e[k]) for k in e} for e in experiences]\n",
    "\n",
    "            for idx in not_done_idxs: \n",
    "                self.history[idx].append(experiences[idx])                \n",
    "                if len(self.history[idx])==self.steps_count and \\\n",
    "                       int(experiences[idx]['step'])%self.steps_delta==0:\n",
    "                    yield tuple(self.history[idx])\n",
    "                \n",
    "                if bool(experiences[idx]['done']):\n",
    "                    if 0<len(self.history[idx])<self.steps_count:\n",
    "                        yield tuple(self.history[idx])\n",
    "                    while len(self.history[idx])>1:\n",
    "                        self.history[idx].popleft()\n",
    "                        yield tuple(self.history[idx])\n",
    "\n",
    "#             except ValueError:\n",
    "#                 self.reset_all()\n",
    "            \n",
    "add_docs(ExperienceSource,\n",
    "        \"\"\"Iterates through `n_envs` of `env` feeding experience or states into `agent`.\n",
    "           If `agent` is None, then random actions will be taken instead.\n",
    "           It will return `steps_count` experiences every `steps_delta`.\n",
    "           At the end of an env, it will return `steps_count-1` experiences per next. \"\"\",\n",
    "        reset_all=\"resets the envs and experience\",\n",
    "        attempt_render=\"Updates `experiences` with images if `render is not None`. Optionally indexes can be passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SourceDataset(IterableDataset):\n",
    "    \"Iterates through a `source` object. Allows for re-initing source connections when `num_workers>0`\"\n",
    "    def __init__(self,source=None): store_attr('source')\n",
    "    def __iter__(self):             return iter(self.source)\n",
    "    def wif(self):                  self.source._init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-container",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-430f201ea2c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersistent_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwif\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwif\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/data/load.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_idxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# called in context of main process (not workers/subprocesses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/data/load.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self, samps)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunkify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/data/load.py\u001b[0m in \u001b[0;36mdo_item\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprebatched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSkipItemException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchunkify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprebatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/data/load.py\u001b[0m in \u001b[0;36mcreate_item\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index an iterable dataset numerically - must use `None`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfa_collate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfa_convert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprebatched\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-184-2adf11b5ed6e>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_done_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "source=ExperienceSource('CartPole-v1',None,n_envs=2,render='rgb_array')\n",
    "dataset=SourceDataset(source)\n",
    "\n",
    "data=None\n",
    "for x in DataLoader(dataset,num_workers=0,n=50,persistent_workers=True,wif=dataset.wif):\n",
    "    data=D(x) if data is None else data+D(x)\n",
    "data.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "source=ExperienceSource('CartPole-v1',None,n_envs=1,render='rgb_array')\n",
    "dataset=SourceDataset(source)\n",
    "\n",
    "data=None\n",
    "for x in DataLoader(dataset,num_workers=0,n=10,persistent_workers=True,wif=dataset.wif):\n",
    "    data=D(x) if data is None else data+D(x)\n",
    "data.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-incidence",
   "metadata": {},
   "source": [
    "`ExperienceSource` is designed for iterating through `n_envs` environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-wyoming",
   "metadata": {},
   "source": [
    "A single experience is a `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "D(state=None,action=None,next_state=None,reward=None,rewards=None,\n",
    "             step=None,steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-mixer",
   "metadata": {},
   "source": [
    "However, an agent has full power to add fields to this dict wile running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ExperienceSource._init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ExperienceSource.__iter__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-entertainment",
   "metadata": {},
   "source": [
    "If the `self.pool` field is empty, it will call `_init_state` to reinitialize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "from queue import deque\n",
    "\n",
    "class ExperienceSource(object):\n",
    "    def __init__(self, env:str,agent,n_envs:int=1,steps_count:int=1,steps_delta:int=1,\n",
    "                 vectorized:bool=False,seed:int=None):\n",
    "        store_attr()\n",
    "        self.pool:List[gym.Env]=[gym.make(self.env) for _ in range(self.n_envs)]\n",
    "    \n",
    "    def init_env(self,env,states,histories,cur_rewards,cur_steps,env_lens):\n",
    "        env.seed(self.seed)\n",
    "        obs=env.reset()\n",
    "        if self.vectorized:\n",
    "            obs_len = len(obs)\n",
    "            states.extend(obs)\n",
    "        else:\n",
    "            obs_len = 1\n",
    "            states.append(obs)\n",
    "        env_lens.append(obs_len)\n",
    "\n",
    "        for _ in range(obs_len):\n",
    "            histories.append(deque(maxlen=self.steps_count))\n",
    "            cur_rewards.append(0.0)\n",
    "            cur_steps.append(0)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        states,histories,cur_rewards,cur_steps,env_lens=[],[],[],[],[]\n",
    "\n",
    "        for env in self.pool: self.init_env(env,states,histories,cur_rewards,cur_steps,env_lens)\n",
    "\n",
    "        iter_idx = 0\n",
    "        while True:\n",
    "            actions = [None] * len(states)\n",
    "            states_input = []\n",
    "            states_indices = []\n",
    "            for idx, state in enumerate(states):\n",
    "                if state is None:\n",
    "                    actions[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
    "                else:\n",
    "                    states_input.append(state)\n",
    "                    states_indices.append(idx)\n",
    "            if states_input:\n",
    "                states_actions = self.agent(states_input)\n",
    "                for idx, action in enumerate(states_actions):\n",
    "                    g_idx = states_indices[idx]\n",
    "                    actions[g_idx] = action\n",
    "#             grouped_actions = _group_list(actions, env_lens)\n",
    "            grouped_actions=np.split(actions,env_lens[:])\n",
    "\n",
    "            global_ofs = 0\n",
    "            for env_idx, (env, action_n) in enumerate(zip(self.pool, grouped_actions)):\n",
    "                if self.vectorized:\n",
    "                    next_state_n, r_n, is_done_n, _ = env.step(action_n)\n",
    "                else:\n",
    "                    next_state, r, is_done, _ = env.step(action_n[0])\n",
    "                    next_state_n, r_n, is_done_n = [next_state], [r], [is_done]\n",
    "\n",
    "                for ofs, (action, next_state, r, is_done) in enumerate(zip(action_n, next_state_n, r_n, is_done_n)):\n",
    "                    idx = global_ofs + ofs\n",
    "                    state = states[idx]\n",
    "                    history = histories[idx]\n",
    "\n",
    "                    cur_rewards[idx] += r\n",
    "                    cur_steps[idx] += 1\n",
    "                    if state is not None:\n",
    "                        history.append(dict(state=state,next_state=next_state, action=action, reward=r, done=is_done,steps=cur_steps[idx],episode_reward=cur_rewards[idx],env=env_idx))\n",
    "                    if len(history) == self.steps_count and iter_idx % self.steps_delta == 0:\n",
    "                        yield tuple(history)\n",
    "                    states[idx] = next_state\n",
    "                    if is_done:\n",
    "                        # in case of very short episode (shorter than our steps count), send gathered history\n",
    "                        if 0 < len(history) < self.steps_count:\n",
    "                            yield tuple(history)\n",
    "                        # generate tail of history\n",
    "                        while len(history) > 1:\n",
    "                            history.popleft()\n",
    "                            yield tuple(history)\n",
    "                        cur_rewards[idx] = 0.0\n",
    "                        cur_steps[idx] = 0\n",
    "                        # vectorized envs are reset automatically\n",
    "                        env.seed(self.seed)\n",
    "                        states[idx]=env.reset() if not self.vectorized else None\n",
    "                        history.clear()\n",
    "                global_ofs += len(action_n)\n",
    "            iter_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-jesus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting /home/fastrl_user/fastrl/nbs/index.ipynb to README.md\n",
      "Converted .data.block_old.ipynb.\n",
      "Converted 00_core.ipynb.\n",
      "Converted 00_nbdev_extension.ipynb.\n",
      "Converted 05_data.block.ipynb.\n",
      "Converted 05_data.test_async.ipynb.\n",
      "Converted 20_test_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nbdev_template.ipynb.\n",
      "converting: /home/fastrl_user/fastrl/nbs/05_data.block.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():   \n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbdev.cli import make_readme\n",
    "    make_readme()\n",
    "    notebook2script()\n",
    "    notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-skiing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
