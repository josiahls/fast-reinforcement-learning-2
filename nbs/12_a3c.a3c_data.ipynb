{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp a3c.a3c_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C Data\n",
    "\n",
    "> A decoupled actor critic agent which trains on data collected from environments running in a completely separate process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# from fastai.basic_data import *\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_core import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.callback import *\n",
    "from fastai.basic_data import *\n",
    "from fastrl.wrappers import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.basic_train import *\n",
    "from fastrl.data_block import *\n",
    "from fastrl.metrics import *\n",
    "from dataclasses import asdict\n",
    "from functools import partial\n",
    "from fastprogress.fastprogress import IN_NOTEBOOK\n",
    "from fastcore.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "from queue import Empty\n",
    "import textwrap\n",
    "import logging\n",
    "import gym\n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s line:%(lineno)d %(levelname)s - %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "_logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "_logger.setLevel('INFO')\n",
    "from fastcore.foundation import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def a3c_data_fitter(model:Optional[nn.Module],learner_cls:Optional['AgentLearner'],agent:Optional['BaseAgent'],ds_cls:ExperienceSourceDataset,\n",
    "            pause_event:mp.Event,cancel_event:mp.Event,main_queue:Optional[mp.JoinableQueue],metric_queue:Optional[mp.JoinableQueue],display=False,\n",
    "            rows=1,cols=1,max_w=800):\n",
    "    \"A3C fitter for AsyncExperienceSourceDataset.\"\n",
    "    ds=ds_cls()\n",
    "    if display:ds=DatasetDisplayWrapper(ds,rows=rows,cols=cols,max_w=max_w)\n",
    "    dl=DataLoader(ds,batch_size=1,num_workers=0)\n",
    "    if learner_cls is not None:\n",
    "        learn=learner_cls(data=DataBunch(dl,dl),model=model,agent=agent)\n",
    "        ds.learn=learn\n",
    "    try:\n",
    "        while not cancel_event.is_set():\n",
    "            for xb,yb in ds:\n",
    "#                 print(yb)\n",
    "                while pause_event.is_set() and not self.cancel_event.is_set():cancel_event.wait(0.1)\n",
    "                if main_queue is not None:main_queue.put(yb)\n",
    "            if metric_queue is not None:\n",
    "                total_rewards=ds.pop_total_rewards()\n",
    "                if total_rewards:\n",
    "#                     print(total_rewards)\n",
    "                    sys.stdout.flush()\n",
    "                    if metric_queue.full():_logger.warning('Metric queue is full. Increase its size,empty it, or set metric_queue to None.')\n",
    "                    metric_queue.put(TotalRewards(total_rewards))                    \n",
    "            while pause_event.is_set():pass\n",
    "    finally:\n",
    "        main_queue.put(None)\n",
    "        metric_queue.put(None)\n",
    "        cancel_event.set()\n",
    "        sys.stdout.flush()\n",
    "\n",
    "@dataclass\n",
    "class A3CLearner(AgentLearner):\n",
    "    fitter_fn:Callable=a3c_data_fitter\n",
    "    batch_sz:int=128\n",
    "    discount:float=0.99\n",
    "    entropy_beta:float=0.01\n",
    "    clip_grad:float=0.1\n",
    "    def init(self, init):print('skipping')\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        super(A3CLearner,self).__post_init__()\n",
    "        if self.model is None:self.model=self.agent.model\n",
    "        if self.agent.model is None: self.agent.model=self.model\n",
    "        self.model.share_memory()\n",
    "        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))\n",
    "        \n",
    "    def predict(self,s):\n",
    "        out=self.agent(s)\n",
    "        if type(out)==tuple:return out[0],None\n",
    "        return out,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=[\n",
    " Experience(s=tensor([[-0.0285,  0.1640, -0.0033, -0.3421]]),sp=tensor([[-0.0285,  0.1640, -0.0033, -0.3421]]),\n",
    "            a=tensor([1]),r=tensor([1.]),d=tensor([0.]),agent_s=tensor([[[0.]]])),\n",
    " Experience(s=tensor([[-0.0252, -0.0311, -0.0101, -0.0504]]),sp=tensor([[-0.0252, -0.0311, -0.0101, -0.0504]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([0.]),agent_s=tensor([[[0.]]])),\n",
    " Experience(s=tensor([[-0.0258, -0.2261, -0.0111,  0.2391]]),sp=tensor([[-0.0258, -0.2261, -0.0111,  0.2391]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([0.]),agent_s=tensor([[[0.]]])),\n",
    " Experience(s=tensor([[-0.0517, -0.2260,  0.0195,  0.2377]]),sp=tensor([[-0.0517, -0.2260,  0.0195,  0.2377]]),\n",
    "            a=tensor([1]),r=tensor([1.]),d=tensor([0.]),agent_s=tensor([[[0.]]])),\n",
    " Experience(s=tensor([[-0.0562, -0.4214,  0.0242,  0.5365]]),sp=tensor([[-0.0562, -0.4214,  0.0242,  0.5365]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([0.]),agent_s=tensor([[[0.]]])),\n",
    " Experience(s=tensor([[-0.0647, -0.6169,  0.0349,  0.8367]]),sp=tensor([[-0.0647, -0.6169,  0.0349,  0.8367]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([1.]),agent_s=tensor([[[0.]]]))\n",
    "]\n",
    "class LinearA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(LinearA2C, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o=self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx=x.float()\n",
    "#         batch_sz=fx.shape[0]\n",
    "#         t=torch.full((batch_sz,2),0,dtype=float)\n",
    "#         t[:,0]=1.0\n",
    "        \n",
    "        return self.policy(fx),self.value(fx)\n",
    "model=LinearA2C((4,),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBack(var_grad_fn):\n",
    "    print(var_grad_fn)\n",
    "    for n in var_grad_fn.next_functions:\n",
    "        if n[0]:\n",
    "            try:\n",
    "                tensor = getattr(n[0], 'variable')\n",
    "                print(n[0])\n",
    "                print('Tensor with grad found:', tensor)\n",
    "                print(' - gradient:', tensor.grad)\n",
    "                print()\n",
    "            except AttributeError as e:\n",
    "                getBack(n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def r_estimate(s,r,d_mask,model,val_gamma,device):\n",
    "    \"Returns rewards `r` estimated direction by `model` from states `s`\"\n",
    "    r_np=np.array(r,dtype=np.float32)\n",
    "#     print(len(d_mask),len(r),len(s))\n",
    "    if len(d_mask)!=0:\n",
    "        s_v=torch.FloatTensor(s).to(device)\n",
    "        v=model(s_v)[1] # Remember that models are going to return the actions and the values\n",
    "        v_np=v.data.cpu().numpy()[:,0]\n",
    "        r_np[d_mask]+=val_gamma*v_np\n",
    "        \n",
    "    return r_np\n",
    "\n",
    "def unbatch(batch,model,last_val_gamma,device='cpu')->Tuple(List,List,List):\n",
    "    s,a,r,d_mask,sp=[],[],[],[],[]\n",
    "    for i,exp in enumerate(batch):\n",
    "        s.append(exp.s.numpy())\n",
    "        a.append(int(exp.a.numpy())) # TODO can we change this to toggle between discrete and continuous actions?\n",
    "        r.append(exp.r.numpy().astype(np.float32))\n",
    "        if not bool(exp.d):\n",
    "            d_mask.append(i)\n",
    "            sp.append(exp.sp.numpy())\n",
    "    s_t=torch.FloatTensor(s).to(device)\n",
    "    a_t=torch.LongTensor(a).to(device)\n",
    "    \n",
    "    r_np=r_estimate(sp,r,d_mask,model,last_val_gamma,device)\n",
    "    estimated_r=torch.FloatTensor(r_np).to(device)\n",
    "    return s_t,a_t,estimated_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0285,  0.1640, -0.0033, -0.3421]],\n",
       " \n",
       "         [[-0.0252, -0.0311, -0.0101, -0.0504]],\n",
       " \n",
       "         [[-0.0258, -0.2261, -0.0111,  0.2391]],\n",
       " \n",
       "         [[-0.0517, -0.2260,  0.0195,  0.2377]],\n",
       " \n",
       "         [[-0.0562, -0.4214,  0.0242,  0.5365]],\n",
       " \n",
       "         [[-0.0647, -0.6169,  0.0349,  0.8367]]]),\n",
       " tensor([1, 0, 0, 1, 0, 0]),\n",
       " tensor([[0.8744],\n",
       "         [0.8577],\n",
       "         [0.8597],\n",
       "         [0.8421],\n",
       "         [0.8626],\n",
       "         [1.0000]]))"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbatch(batch,model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "debug_batch=[]\n",
    "\n",
    "class A3CTrainer(LearnerCallback):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(A3CTrainer,self).__init__(*args,**kwargs)\n",
    "        self.batch=[]\n",
    "        \n",
    "    @property\n",
    "    def skip_process_batch(self):return len(self.batch)<self.learn.data.bs\n",
    "    def on_train_begin(self,**kwargs):self.batch.clear()\n",
    "    \n",
    "    def on_batch_begin(self,last_target,**kwargs):\n",
    "        self.batch.extend([Experience(**{k:v[i] if len(v)!=0 else None for k,v in last_target.items()}) for i in range(len(last_target['s']))])\n",
    "        \n",
    "    def on_backward_begin(self,last_loss,**kwargs):\n",
    "        if self.skip_process_batch:return {'skip_bwd':self.skip_process_batch}\n",
    "        s_t,a_t,r_est=unbatch(self.batch,self.learn.model,self.learn.discount**self.data.ds_kwargs['skip_n_steps'])\n",
    "        self.learn.opt.zero_grad()\n",
    "        logits_v,value_v=self.learn.model(s_t)\n",
    "\n",
    "        loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)\n",
    "#         print((r_est.mean(),value_v.mean()))\n",
    "        log_prob_v=F.log_softmax(logits_v,dim=1)\n",
    "        adv_v=r_est-value_v.detach()\n",
    "        log_prob_actions_v=adv_v*log_prob_v[range(self.learn.data.bs),a_t]\n",
    "        loss_policy_v=-log_prob_actions_v.mean()\n",
    "        \n",
    "        prob_v=F.softmax(logits_v,dim=1)\n",
    "#         print(prob_v.max(),log_prob_v.max(),prob_v.min(),log_prob_v.min())\n",
    "        entropy_loss_v=self.learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()\n",
    "    \n",
    "        print(entropy_loss_v,loss_policy_v,loss_value_v)\n",
    "        loss_v=entropy_loss_v+loss_policy_v+loss_value_v\n",
    "        \n",
    "        self.learn.loss_func.loss=loss_v.detach()\n",
    "        return {'last_loss':loss_v,'skip_bwd':self.skip_process_batch}\n",
    "        \n",
    "            \n",
    "    def on_backward_end(self,*args,**kwargs): \n",
    "        if not self.skip_process_batch:nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)\n",
    "        return {'skip_bwd':self.skip_process_batch,\n",
    "                'skip_step':self.skip_process_batch,\n",
    "                'skip_zero':self.skip_process_batch}\n",
    "    def on_step_end(self,last_loss,*args,**kwargs):\n",
    "        getBack(last_loss.grad_fn)\n",
    "        if self.skip_process_batch:return\n",
    "        self.batch.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>train_reward</th>\n",
       "      <th>train_n_games</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>#na#</td>\n",
       "      <td>17.296296</td>\n",
       "      <td>27</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0068, grad_fn=<MulBackward0>) tensor(2.4953, grad_fn=<NegBackward>) tensor(13.6656, grad_fn=<MseLossBackward>)\n",
      "<AddBackward0 object at 0x7fb4b7b1db50>\n",
      "<AddBackward0 object at 0x7fb4b7b1d2d0>\n",
      "<MulBackward0 object at 0x7fb4b7b20d10>\n",
      "<MeanBackward0 object at 0x7fb42b51c2d0>\n",
      "<SumBackward1 object at 0x7fb4b7b20f50>\n",
      "<MulBackward0 object at 0x7fb4b7b92850>\n",
      "<SoftmaxBackward object at 0x7fb42b51c9d0>\n",
      "<AddmmBackward object at 0x7fb42b51ce10>\n",
      "<AccumulateGrad object at 0x7fb42b5840d0>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([0.0304, 0.0143], requires_grad=True)\n",
      " - gradient: tensor([ 0.0008, -0.0008])\n",
      "\n",
      "<ReluBackward0 object at 0x7fb42b584610>\n",
      "<AddmmBackward object at 0x7fb42b5848d0>\n",
      "<AccumulateGrad object at 0x7fb42b51ce90>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([ 0.0836,  0.1201,  0.3913, -0.4708,  0.1459,  0.2671, -0.0359,  0.2574,\n",
      "         0.4952, -0.0368, -0.3959, -0.3020,  0.3118,  0.3358,  0.1539, -0.4785,\n",
      "        -0.4129,  0.0446, -0.1756,  0.4835,  0.4915, -0.3973,  0.1037,  0.1898,\n",
      "         0.4363, -0.3464,  0.0620, -0.1739,  0.3425, -0.1533,  0.1993,  0.3633,\n",
      "         0.0843,  0.1211,  0.3498,  0.1681,  0.3676,  0.2115,  0.4674,  0.2326,\n",
      "        -0.3346, -0.1176, -0.2113, -0.4495, -0.3470,  0.3860,  0.0573,  0.3063,\n",
      "        -0.4439, -0.2461, -0.2272, -0.1683, -0.1909,  0.1479, -0.0597, -0.1091,\n",
      "        -0.1934,  0.2324, -0.0354,  0.1991, -0.3690, -0.4826,  0.1946, -0.0871,\n",
      "         0.1291, -0.4522, -0.3160,  0.2630,  0.3498, -0.2587,  0.0279, -0.3408,\n",
      "         0.3226,  0.2125, -0.4604, -0.1560, -0.0347,  0.0404,  0.1804, -0.0224,\n",
      "        -0.3534,  0.2811,  0.2626, -0.3701, -0.2960, -0.3046,  0.0219, -0.1198,\n",
      "        -0.4140,  0.3908,  0.0461, -0.2172,  0.3942,  0.4233, -0.4196,  0.1689,\n",
      "         0.3650, -0.4269, -0.0369, -0.3067, -0.4417, -0.4151,  0.2616,  0.1498,\n",
      "        -0.4283, -0.1717, -0.4473,  0.1233, -0.4644, -0.4687, -0.2934, -0.4000,\n",
      "        -0.3685, -0.1792, -0.2775,  0.2510,  0.1357,  0.0069,  0.2857,  0.3933,\n",
      "         0.1654, -0.3851, -0.0755, -0.4759, -0.4915,  0.4459, -0.3034,  0.2906,\n",
      "         0.2900,  0.4937, -0.3221, -0.2144, -0.3886,  0.3191, -0.4876, -0.3652,\n",
      "         0.3490,  0.0984, -0.3171, -0.2651,  0.4516,  0.3145,  0.0015, -0.3286,\n",
      "         0.1114, -0.3040,  0.2521, -0.1331,  0.2922,  0.3933, -0.1744, -0.4901,\n",
      "        -0.3057,  0.0713, -0.4555,  0.4353,  0.3059,  0.3596,  0.0623,  0.1541,\n",
      "        -0.3760, -0.4785, -0.3927,  0.2805, -0.3923, -0.0644,  0.0854, -0.1968,\n",
      "         0.1114,  0.0407, -0.2102, -0.3769,  0.4809, -0.2539,  0.4103, -0.1010,\n",
      "         0.4072,  0.3728,  0.0574, -0.3198,  0.3949, -0.4903,  0.2637, -0.3264,\n",
      "        -0.3623, -0.0717,  0.2134,  0.1785,  0.3292,  0.2256, -0.3100, -0.1961,\n",
      "        -0.4485,  0.4104,  0.3819,  0.0430, -0.4650,  0.4618, -0.2138,  0.2528,\n",
      "        -0.4245,  0.3318, -0.4590, -0.0863, -0.3847,  0.3796,  0.3232,  0.4695,\n",
      "         0.2984, -0.0836,  0.0482,  0.4229, -0.2129,  0.2359,  0.1178,  0.1183,\n",
      "         0.2310, -0.0345, -0.2704,  0.4266, -0.2784, -0.4628,  0.3170, -0.0621,\n",
      "         0.1038,  0.4499, -0.2339, -0.0101, -0.4206,  0.4886, -0.3913, -0.1837,\n",
      "         0.0342,  0.0607,  0.1452, -0.0788,  0.4838,  0.1925,  0.0524,  0.2410,\n",
      "         0.2798,  0.4843,  0.2896,  0.4449,  0.0737, -0.1303,  0.1929, -0.0907,\n",
      "        -0.0520,  0.3719, -0.4793,  0.3978, -0.0143, -0.0567, -0.1887, -0.2277,\n",
      "         0.1942, -0.4367, -0.3477, -0.0504, -0.2108,  0.0433, -0.1850,  0.4378,\n",
      "         0.3196,  0.0840, -0.0690, -0.4070,  0.4929, -0.2717, -0.3933,  0.2984,\n",
      "        -0.0181,  0.1961, -0.1680,  0.3318, -0.3509, -0.1173,  0.3255, -0.3857,\n",
      "        -0.3023,  0.0645, -0.2970,  0.1276, -0.1310, -0.4644, -0.2819,  0.3859,\n",
      "         0.2503, -0.2184, -0.2089,  0.2583,  0.3238, -0.1235, -0.4893,  0.0641,\n",
      "         0.3571,  0.0888, -0.3076,  0.3594, -0.4102,  0.2305, -0.0202, -0.3252,\n",
      "         0.2335, -0.3692,  0.2389, -0.4916, -0.0365,  0.0402,  0.2245,  0.0583,\n",
      "         0.4531, -0.3001, -0.4821, -0.3203, -0.3823,  0.2183,  0.3845, -0.4296,\n",
      "        -0.4567, -0.1915, -0.2142,  0.0218, -0.0309,  0.4453, -0.4203, -0.2752,\n",
      "         0.2268,  0.3277, -0.2689, -0.4621, -0.2239, -0.2595, -0.3639, -0.4399,\n",
      "        -0.3379, -0.3549,  0.3601, -0.2752,  0.2462,  0.4119, -0.4212,  0.4698,\n",
      "         0.4719, -0.1430, -0.0725,  0.1860, -0.3070,  0.0802, -0.2508,  0.2254,\n",
      "         0.4213,  0.0239, -0.4746, -0.0123, -0.1043,  0.4478, -0.1762,  0.4976,\n",
      "         0.0217, -0.0891, -0.2247, -0.0114,  0.1388,  0.2436,  0.0469,  0.3821,\n",
      "        -0.3866, -0.0517, -0.3217,  0.4172,  0.2929, -0.3503,  0.4116,  0.1146,\n",
      "         0.1528,  0.0206,  0.1911,  0.1625, -0.2440,  0.1614,  0.4337,  0.3073,\n",
      "        -0.3003,  0.2351, -0.0974, -0.0076,  0.0212, -0.1825,  0.1219, -0.0410,\n",
      "        -0.1847, -0.4130, -0.1323,  0.1678,  0.0212, -0.0952, -0.3334, -0.4821,\n",
      "        -0.2706,  0.0537,  0.4572,  0.4866, -0.0385, -0.2521,  0.3174,  0.0082,\n",
      "         0.1059, -0.0188, -0.2053, -0.3943, -0.3875,  0.0307, -0.0499,  0.0293,\n",
      "        -0.3876, -0.4962,  0.1875, -0.1148,  0.2897,  0.0369,  0.4173,  0.4194,\n",
      "        -0.3274, -0.2538,  0.4459, -0.4948,  0.4447,  0.1152,  0.0992, -0.0110,\n",
      "         0.1374, -0.1414,  0.0085, -0.2994,  0.4711, -0.3588, -0.1394,  0.1996,\n",
      "         0.4288,  0.4630,  0.4356, -0.2831, -0.0762, -0.1669,  0.4988, -0.3871,\n",
      "        -0.4040, -0.3893,  0.3146,  0.3632,  0.3587,  0.3331, -0.2225,  0.0325,\n",
      "        -0.3996, -0.1156,  0.1463,  0.4661, -0.3104, -0.4605, -0.2749, -0.4925,\n",
      "        -0.1833, -0.4126, -0.2014, -0.0716, -0.1839,  0.4160, -0.2475, -0.4047,\n",
      "         0.1038,  0.2200,  0.4885,  0.3791,  0.2512,  0.0601,  0.2680,  0.4170,\n",
      "        -0.0420,  0.3225, -0.4039, -0.0183, -0.4965,  0.1191,  0.3034, -0.2433,\n",
      "         0.3667,  0.1454, -0.2418,  0.4600,  0.4698, -0.3946, -0.3335,  0.3939,\n",
      "         0.1892,  0.3304,  0.0140, -0.4216, -0.4335, -0.2401,  0.0483,  0.0603,\n",
      "        -0.4004, -0.4962, -0.4113,  0.3255, -0.4179,  0.3235,  0.0282,  0.1130],\n",
      "       requires_grad=True)\n",
      " - gradient: tensor([-5.5395e-06, -2.1103e-05, -3.3668e-05,  2.2950e-05,  2.7552e-05,\n",
      "         6.7456e-06, -3.9756e-05, -9.3146e-08,  4.3260e-05, -3.3086e-06,\n",
      "        -1.1633e-06,  0.0000e+00, -1.2658e-06, -2.2893e-07, -9.2255e-06,\n",
      "        -7.8480e-06,  0.0000e+00,  3.4819e-05, -4.1653e-05,  1.8067e-05,\n",
      "         2.9579e-06,  6.4237e-07, -3.1728e-05,  5.6861e-05,  2.6782e-05,\n",
      "         0.0000e+00, -1.0867e-05,  2.1712e-05, -1.7744e-05,  1.4234e-05,\n",
      "         2.3992e-05,  2.5051e-05, -1.3845e-05,  6.6667e-05,  2.5481e-05,\n",
      "        -3.8339e-06, -2.9678e-05, -3.2914e-05,  1.4886e-06, -1.0968e-06,\n",
      "        -1.4918e-07, -1.0137e-05, -4.9699e-06,  1.7838e-06,  1.6942e-05,\n",
      "        -3.1692e-05, -1.3105e-05, -7.0346e-05,  9.1989e-06,  0.0000e+00,\n",
      "         1.3555e-05,  6.0132e-05,  0.0000e+00,  7.1362e-06, -3.7176e-05,\n",
      "         8.6269e-06, -6.6942e-05, -2.7308e-05, -8.0061e-06,  1.5280e-05,\n",
      "         1.2221e-07, -5.6354e-06,  3.8701e-05,  4.3698e-05, -6.1100e-07,\n",
      "        -9.0578e-06, -7.1856e-06, -7.2338e-05, -6.3836e-06, -4.2891e-06,\n",
      "        -4.6756e-05, -3.0511e-06, -7.0261e-05,  6.6354e-06, -6.4156e-07,\n",
      "        -9.2307e-06, -2.9799e-05, -2.9041e-06,  1.0829e-05,  1.4640e-06,\n",
      "        -9.0344e-07, -3.0583e-06, -3.1963e-05,  2.4695e-06, -1.2296e-05,\n",
      "        -1.4429e-06,  1.8337e-05, -1.1468e-07,  0.0000e+00,  9.6786e-06,\n",
      "         6.1790e-07, -1.7311e-05,  1.4886e-05,  5.4242e-05,  0.0000e+00,\n",
      "        -1.4872e-05,  7.0680e-06,  0.0000e+00, -9.0705e-06, -6.6738e-06,\n",
      "         0.0000e+00,  2.5462e-05,  2.6095e-05, -1.5841e-06,  5.5961e-06,\n",
      "        -1.1783e-05, -2.8445e-06,  3.4865e-06,  0.0000e+00,  8.2411e-06,\n",
      "         3.3642e-06, -2.3953e-05,  9.9557e-06,  4.3321e-05, -3.7365e-06,\n",
      "        -1.7976e-05,  1.5068e-05, -1.2244e-05,  5.3373e-05, -1.0431e-05,\n",
      "         3.0783e-05,  7.5035e-06,  3.2532e-05,  6.3996e-07,  2.1201e-06,\n",
      "        -1.1730e-06, -7.5770e-06,  5.4622e-05, -8.7737e-06, -2.2859e-05,\n",
      "        -1.6449e-05,  1.8989e-05,  0.0000e+00,  1.9957e-05,  0.0000e+00,\n",
      "        -8.9059e-06,  2.2950e-05, -6.9170e-06, -4.7682e-05, -5.5867e-06,\n",
      "        -1.5707e-05,  2.5580e-05, -8.6706e-05,  2.1614e-06, -1.1215e-05,\n",
      "        -2.4201e-06, -3.9807e-05, -1.3879e-05,  7.6395e-06,  3.2419e-05,\n",
      "        -5.9092e-07,  0.0000e+00, -3.8412e-06, -6.2166e-05,  0.0000e+00,\n",
      "         2.0314e-07, -1.0679e-05,  6.3801e-06, -8.0621e-06, -2.2495e-05,\n",
      "        -2.5794e-06,  5.4744e-06,  0.0000e+00, -1.5991e-06,  1.9810e-06,\n",
      "        -1.4802e-05,  3.5290e-06, -7.0094e-07,  5.0785e-06,  4.7210e-05,\n",
      "        -6.2715e-05,  1.5353e-05, -8.0912e-05,  1.2344e-06,  2.8245e-05,\n",
      "         1.6297e-06,  9.8054e-07, -1.4500e-06,  9.1233e-06,  0.0000e+00,\n",
      "        -5.7548e-05,  7.9784e-07,  3.4731e-05, -4.5935e-05,  0.0000e+00,\n",
      "        -3.3991e-05, -3.8135e-05,  1.0840e-05,  7.6413e-06,  4.4740e-05,\n",
      "         1.1568e-06,  8.7894e-10,  0.0000e+00,  5.0240e-06,  1.5717e-05,\n",
      "         3.0172e-06,  0.0000e+00,  2.4443e-05, -1.3932e-06, -8.7192e-07,\n",
      "        -1.3301e-06,  9.8972e-08,  0.0000e+00, -2.5029e-05,  5.3791e-06,\n",
      "         1.5695e-05,  4.5583e-07, -7.6430e-07,  1.6138e-05, -1.4537e-05,\n",
      "         1.8660e-05,  1.4328e-05,  6.9026e-06,  1.1992e-06,  5.5921e-06,\n",
      "        -3.0571e-09, -4.0208e-06, -9.6747e-05,  2.1192e-06,  2.5140e-05,\n",
      "         6.4067e-06, -1.6023e-05, -1.6353e-05, -3.8285e-06, -2.0638e-06,\n",
      "        -1.3787e-05,  1.4410e-05,  1.6168e-06,  0.0000e+00,  2.4901e-06,\n",
      "         0.0000e+00, -6.1390e-05,  2.6121e-05,  2.2791e-05, -3.1632e-07,\n",
      "        -2.0822e-05, -4.1406e-05,  2.3772e-06, -5.0922e-06,  7.1827e-05,\n",
      "        -3.6756e-06,  9.9112e-06,  1.7665e-05, -1.8764e-07, -8.2462e-05,\n",
      "         2.4424e-05, -1.8893e-05, -2.4933e-06,  2.7231e-05, -1.2233e-05,\n",
      "         1.1845e-06,  9.9137e-07,  1.6564e-06,  2.0533e-06, -7.1500e-07,\n",
      "        -3.9646e-05, -4.3920e-05, -3.8459e-06,  2.6964e-05, -3.5132e-05,\n",
      "         2.2194e-07, -4.1074e-06, -1.3880e-05, -3.1970e-06,  2.4049e-05,\n",
      "        -3.6069e-06, -1.5321e-05, -1.4787e-05, -1.3686e-07, -1.1467e-05,\n",
      "         0.0000e+00,  1.3728e-07,  1.8778e-05, -2.9077e-05, -1.5609e-05,\n",
      "        -1.9530e-05, -7.1013e-07,  5.6913e-05,  1.0717e-06, -2.1381e-05,\n",
      "        -5.6700e-06,  1.0015e-04, -1.9864e-05,  5.4379e-05,  1.9938e-05,\n",
      "        -9.3073e-07,  0.0000e+00,  1.0250e-06, -7.3005e-06,  6.9748e-06,\n",
      "         5.7047e-05,  1.2496e-05, -3.2120e-05, -1.7795e-06,  0.0000e+00,\n",
      "         1.6306e-05, -3.3071e-06,  2.2218e-05,  1.7924e-06,  3.9045e-06,\n",
      "        -2.5771e-05,  5.0300e-05, -7.3440e-06, -2.3401e-05,  1.2955e-06,\n",
      "        -1.5173e-06,  1.2538e-06,  5.2046e-05,  3.9561e-05, -6.1005e-05,\n",
      "        -5.5012e-06, -1.2580e-05, -2.2842e-05,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.6366e-06, -1.7024e-05,  4.4695e-05,  1.5917e-07,\n",
      "        -5.7369e-06, -9.1032e-06, -4.9880e-06,  1.2506e-06, -3.6711e-06,\n",
      "         6.3483e-08,  8.3124e-06,  6.8066e-08, -4.8726e-07,  6.7613e-07,\n",
      "        -2.2803e-05,  4.7221e-05, -2.9878e-05,  0.0000e+00, -3.2675e-06,\n",
      "        -5.2494e-05,  1.2600e-05,  2.3590e-05, -4.5195e-08,  1.2084e-05,\n",
      "         6.5317e-05, -1.9959e-05, -9.9063e-07,  3.8354e-05,  8.9053e-06,\n",
      "         4.8397e-05,  7.0327e-06, -1.2775e-06, -2.0333e-05, -3.3122e-06,\n",
      "         4.7001e-05, -6.4541e-05,  7.4413e-05,  1.3623e-05, -2.2343e-05,\n",
      "         5.6330e-06, -5.5653e-06,  5.9903e-05, -2.4342e-05,  7.7037e-06,\n",
      "         3.2755e-05, -6.2526e-06, -3.3198e-06, -3.9569e-05, -1.8499e-05,\n",
      "        -8.1332e-06,  8.7519e-06, -2.1425e-05,  2.2847e-05, -3.3200e-05,\n",
      "         1.1753e-05,  3.0872e-05,  6.5473e-06, -1.4419e-05,  1.7467e-05,\n",
      "         5.7889e-05,  3.1520e-05, -5.6288e-06, -9.1647e-06,  1.6757e-05,\n",
      "         0.0000e+00, -2.8972e-05, -3.0664e-05,  7.9971e-06, -4.5715e-07,\n",
      "         5.1550e-06,  2.3444e-05, -3.0392e-07,  1.9583e-05,  2.9333e-05,\n",
      "        -3.8898e-06, -6.6011e-06,  2.5532e-05, -4.7621e-06, -3.2689e-06,\n",
      "        -6.2244e-06,  5.5225e-06,  5.4578e-05,  0.0000e+00,  0.0000e+00,\n",
      "        -3.6249e-06, -1.0404e-05,  1.2260e-05,  5.6485e-06,  1.8393e-05,\n",
      "         0.0000e+00, -6.2603e-06, -7.1133e-06, -5.1811e-05, -2.3774e-05,\n",
      "         2.1392e-05,  0.0000e+00,  0.0000e+00,  4.1541e-05,  1.4280e-05,\n",
      "         1.6552e-05,  0.0000e+00,  6.1999e-06,  1.1486e-06,  1.0785e-05,\n",
      "         4.6740e-05, -6.3256e-05, -6.4956e-05, -1.2181e-05,  0.0000e+00,\n",
      "        -1.6742e-06,  1.6604e-07,  5.3331e-06,  2.1492e-05,  2.2762e-05,\n",
      "        -1.3252e-06,  1.3765e-05,  1.3179e-05, -2.6177e-05,  2.9614e-06,\n",
      "        -6.3315e-06,  6.6043e-06,  1.7297e-06,  1.4531e-05, -1.4466e-06,\n",
      "        -3.1344e-05,  4.6003e-05, -5.1784e-06,  2.3043e-05, -7.0156e-06,\n",
      "         1.7836e-05, -6.3008e-06,  0.0000e+00, -5.4676e-06, -2.5680e-07,\n",
      "        -5.2855e-05,  7.2997e-05,  1.1458e-06,  1.6095e-05,  0.0000e+00,\n",
      "         3.5814e-05, -3.3796e-06,  2.0375e-05, -6.1299e-07,  7.9336e-07,\n",
      "        -1.1082e-05,  1.1538e-05, -2.9224e-05,  7.0791e-07,  7.8512e-06,\n",
      "        -2.4021e-05, -2.2157e-06, -9.0387e-06, -2.2548e-05,  4.2905e-05,\n",
      "         3.5104e-06, -1.4264e-06, -8.3500e-06, -3.2470e-08,  3.7577e-05,\n",
      "         2.3524e-06,  6.8589e-05, -1.5428e-06, -4.2747e-05,  1.7781e-06,\n",
      "         8.7205e-07,  9.2870e-06, -2.7251e-07, -1.2001e-05, -3.4366e-06,\n",
      "         4.4332e-07,  8.8611e-06, -1.6704e-06, -1.8096e-06,  6.2001e-07,\n",
      "         0.0000e+00, -7.3987e-06, -5.7508e-05, -2.0613e-05, -1.7233e-06,\n",
      "        -7.3989e-07,  2.2224e-05,  1.3277e-06,  2.3963e-05,  1.1860e-05,\n",
      "        -8.4531e-06,  1.1590e-05, -2.6392e-07, -8.8386e-05, -1.1518e-06,\n",
      "         0.0000e+00,  7.4797e-07, -1.9657e-05,  4.1970e-06, -4.5585e-05,\n",
      "         1.1099e-05,  5.6770e-05])\n",
      "\n",
      "<TBackward object at 0x7fb42b51cf10>\n",
      "<AccumulateGrad object at 0x7fb42b532910>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.1266, -0.1425,  0.2257,  0.2979],\n",
      "        [ 0.1117, -0.3096,  0.1358,  0.4590],\n",
      "        [ 0.4122,  0.1309, -0.0993, -0.4918],\n",
      "        ...,\n",
      "        [ 0.3904,  0.4327, -0.4662, -0.2754],\n",
      "        [-0.2200,  0.2023, -0.0253,  0.3765],\n",
      "        [-0.4129, -0.2818, -0.4028, -0.2308]], requires_grad=True)\n",
      " - gradient: tensor([[ 3.0934e-07,  7.8290e-06,  4.4565e-07, -9.3330e-06],\n",
      "        [ 1.1785e-06,  2.9825e-05,  1.6977e-06, -3.5555e-05],\n",
      "        [-1.8282e-06, -3.1999e-05,  2.3822e-06,  4.9341e-05],\n",
      "        ...,\n",
      "        [-2.9343e-06, -4.6731e-05,  3.7075e-06,  7.1715e-05],\n",
      "        [-5.8789e-07, -1.7307e-05, -5.7119e-07,  2.2572e-05],\n",
      "        [ 5.2735e-06,  9.4497e-05, -5.6215e-06, -1.4112e-04]])\n",
      "\n",
      "<TBackward object at 0x7fb42b5847d0>\n",
      "<AccumulateGrad object at 0x7fb42b532ed0>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.0190,  0.0440, -0.0216,  ..., -0.0135, -0.0299,  0.0425],\n",
      "        [ 0.0013, -0.0225,  0.0084,  ...,  0.0287,  0.0114, -0.0242]],\n",
      "       requires_grad=True)\n",
      " - gradient: tensor([[-0.0002, -0.0004,  0.0015,  ...,  0.0014, -0.0001,  0.0002],\n",
      "        [ 0.0002,  0.0004, -0.0015,  ..., -0.0014,  0.0001, -0.0002]])\n",
      "\n",
      "<LogSoftmaxBackward object at 0x7fb42b51c510>\n",
      "<AddmmBackward object at 0x7fb42b532ed0>\n",
      "<AccumulateGrad object at 0x7fb42b5847d0>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([0.0304, 0.0143], requires_grad=True)\n",
      " - gradient: tensor([ 0.0008, -0.0008])\n",
      "\n",
      "<ReluBackward0 object at 0x7fb42b584210>\n",
      "<AddmmBackward object at 0x7fb42b5848d0>\n",
      "<AccumulateGrad object at 0x7fb42b532710>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([ 0.0836,  0.1201,  0.3913, -0.4708,  0.1459,  0.2671, -0.0359,  0.2574,\n",
      "         0.4952, -0.0368, -0.3959, -0.3020,  0.3118,  0.3358,  0.1539, -0.4785,\n",
      "        -0.4129,  0.0446, -0.1756,  0.4835,  0.4915, -0.3973,  0.1037,  0.1898,\n",
      "         0.4363, -0.3464,  0.0620, -0.1739,  0.3425, -0.1533,  0.1993,  0.3633,\n",
      "         0.0843,  0.1211,  0.3498,  0.1681,  0.3676,  0.2115,  0.4674,  0.2326,\n",
      "        -0.3346, -0.1176, -0.2113, -0.4495, -0.3470,  0.3860,  0.0573,  0.3063,\n",
      "        -0.4439, -0.2461, -0.2272, -0.1683, -0.1909,  0.1479, -0.0597, -0.1091,\n",
      "        -0.1934,  0.2324, -0.0354,  0.1991, -0.3690, -0.4826,  0.1946, -0.0871,\n",
      "         0.1291, -0.4522, -0.3160,  0.2630,  0.3498, -0.2587,  0.0279, -0.3408,\n",
      "         0.3226,  0.2125, -0.4604, -0.1560, -0.0347,  0.0404,  0.1804, -0.0224,\n",
      "        -0.3534,  0.2811,  0.2626, -0.3701, -0.2960, -0.3046,  0.0219, -0.1198,\n",
      "        -0.4140,  0.3908,  0.0461, -0.2172,  0.3942,  0.4233, -0.4196,  0.1689,\n",
      "         0.3650, -0.4269, -0.0369, -0.3067, -0.4417, -0.4151,  0.2616,  0.1498,\n",
      "        -0.4283, -0.1717, -0.4473,  0.1233, -0.4644, -0.4687, -0.2934, -0.4000,\n",
      "        -0.3685, -0.1792, -0.2775,  0.2510,  0.1357,  0.0069,  0.2857,  0.3933,\n",
      "         0.1654, -0.3851, -0.0755, -0.4759, -0.4915,  0.4459, -0.3034,  0.2906,\n",
      "         0.2900,  0.4937, -0.3221, -0.2144, -0.3886,  0.3191, -0.4876, -0.3652,\n",
      "         0.3490,  0.0984, -0.3171, -0.2651,  0.4516,  0.3145,  0.0015, -0.3286,\n",
      "         0.1114, -0.3040,  0.2521, -0.1331,  0.2922,  0.3933, -0.1744, -0.4901,\n",
      "        -0.3057,  0.0713, -0.4555,  0.4353,  0.3059,  0.3596,  0.0623,  0.1541,\n",
      "        -0.3760, -0.4785, -0.3927,  0.2805, -0.3923, -0.0644,  0.0854, -0.1968,\n",
      "         0.1114,  0.0407, -0.2102, -0.3769,  0.4809, -0.2539,  0.4103, -0.1010,\n",
      "         0.4072,  0.3728,  0.0574, -0.3198,  0.3949, -0.4903,  0.2637, -0.3264,\n",
      "        -0.3623, -0.0717,  0.2134,  0.1785,  0.3292,  0.2256, -0.3100, -0.1961,\n",
      "        -0.4485,  0.4104,  0.3819,  0.0430, -0.4650,  0.4618, -0.2138,  0.2528,\n",
      "        -0.4245,  0.3318, -0.4590, -0.0863, -0.3847,  0.3796,  0.3232,  0.4695,\n",
      "         0.2984, -0.0836,  0.0482,  0.4229, -0.2129,  0.2359,  0.1178,  0.1183,\n",
      "         0.2310, -0.0345, -0.2704,  0.4266, -0.2784, -0.4628,  0.3170, -0.0621,\n",
      "         0.1038,  0.4499, -0.2339, -0.0101, -0.4206,  0.4886, -0.3913, -0.1837,\n",
      "         0.0342,  0.0607,  0.1452, -0.0788,  0.4838,  0.1925,  0.0524,  0.2410,\n",
      "         0.2798,  0.4843,  0.2896,  0.4449,  0.0737, -0.1303,  0.1929, -0.0907,\n",
      "        -0.0520,  0.3719, -0.4793,  0.3978, -0.0143, -0.0567, -0.1887, -0.2277,\n",
      "         0.1942, -0.4367, -0.3477, -0.0504, -0.2108,  0.0433, -0.1850,  0.4378,\n",
      "         0.3196,  0.0840, -0.0690, -0.4070,  0.4929, -0.2717, -0.3933,  0.2984,\n",
      "        -0.0181,  0.1961, -0.1680,  0.3318, -0.3509, -0.1173,  0.3255, -0.3857,\n",
      "        -0.3023,  0.0645, -0.2970,  0.1276, -0.1310, -0.4644, -0.2819,  0.3859,\n",
      "         0.2503, -0.2184, -0.2089,  0.2583,  0.3238, -0.1235, -0.4893,  0.0641,\n",
      "         0.3571,  0.0888, -0.3076,  0.3594, -0.4102,  0.2305, -0.0202, -0.3252,\n",
      "         0.2335, -0.3692,  0.2389, -0.4916, -0.0365,  0.0402,  0.2245,  0.0583,\n",
      "         0.4531, -0.3001, -0.4821, -0.3203, -0.3823,  0.2183,  0.3845, -0.4296,\n",
      "        -0.4567, -0.1915, -0.2142,  0.0218, -0.0309,  0.4453, -0.4203, -0.2752,\n",
      "         0.2268,  0.3277, -0.2689, -0.4621, -0.2239, -0.2595, -0.3639, -0.4399,\n",
      "        -0.3379, -0.3549,  0.3601, -0.2752,  0.2462,  0.4119, -0.4212,  0.4698,\n",
      "         0.4719, -0.1430, -0.0725,  0.1860, -0.3070,  0.0802, -0.2508,  0.2254,\n",
      "         0.4213,  0.0239, -0.4746, -0.0123, -0.1043,  0.4478, -0.1762,  0.4976,\n",
      "         0.0217, -0.0891, -0.2247, -0.0114,  0.1388,  0.2436,  0.0469,  0.3821,\n",
      "        -0.3866, -0.0517, -0.3217,  0.4172,  0.2929, -0.3503,  0.4116,  0.1146,\n",
      "         0.1528,  0.0206,  0.1911,  0.1625, -0.2440,  0.1614,  0.4337,  0.3073,\n",
      "        -0.3003,  0.2351, -0.0974, -0.0076,  0.0212, -0.1825,  0.1219, -0.0410,\n",
      "        -0.1847, -0.4130, -0.1323,  0.1678,  0.0212, -0.0952, -0.3334, -0.4821,\n",
      "        -0.2706,  0.0537,  0.4572,  0.4866, -0.0385, -0.2521,  0.3174,  0.0082,\n",
      "         0.1059, -0.0188, -0.2053, -0.3943, -0.3875,  0.0307, -0.0499,  0.0293,\n",
      "        -0.3876, -0.4962,  0.1875, -0.1148,  0.2897,  0.0369,  0.4173,  0.4194,\n",
      "        -0.3274, -0.2538,  0.4459, -0.4948,  0.4447,  0.1152,  0.0992, -0.0110,\n",
      "         0.1374, -0.1414,  0.0085, -0.2994,  0.4711, -0.3588, -0.1394,  0.1996,\n",
      "         0.4288,  0.4630,  0.4356, -0.2831, -0.0762, -0.1669,  0.4988, -0.3871,\n",
      "        -0.4040, -0.3893,  0.3146,  0.3632,  0.3587,  0.3331, -0.2225,  0.0325,\n",
      "        -0.3996, -0.1156,  0.1463,  0.4661, -0.3104, -0.4605, -0.2749, -0.4925,\n",
      "        -0.1833, -0.4126, -0.2014, -0.0716, -0.1839,  0.4160, -0.2475, -0.4047,\n",
      "         0.1038,  0.2200,  0.4885,  0.3791,  0.2512,  0.0601,  0.2680,  0.4170,\n",
      "        -0.0420,  0.3225, -0.4039, -0.0183, -0.4965,  0.1191,  0.3034, -0.2433,\n",
      "         0.3667,  0.1454, -0.2418,  0.4600,  0.4698, -0.3946, -0.3335,  0.3939,\n",
      "         0.1892,  0.3304,  0.0140, -0.4216, -0.4335, -0.2401,  0.0483,  0.0603,\n",
      "        -0.4004, -0.4962, -0.4113,  0.3255, -0.4179,  0.3235,  0.0282,  0.1130],\n",
      "       requires_grad=True)\n",
      " - gradient: tensor([-5.5395e-06, -2.1103e-05, -3.3668e-05,  2.2950e-05,  2.7552e-05,\n",
      "         6.7456e-06, -3.9756e-05, -9.3146e-08,  4.3260e-05, -3.3086e-06,\n",
      "        -1.1633e-06,  0.0000e+00, -1.2658e-06, -2.2893e-07, -9.2255e-06,\n",
      "        -7.8480e-06,  0.0000e+00,  3.4819e-05, -4.1653e-05,  1.8067e-05,\n",
      "         2.9579e-06,  6.4237e-07, -3.1728e-05,  5.6861e-05,  2.6782e-05,\n",
      "         0.0000e+00, -1.0867e-05,  2.1712e-05, -1.7744e-05,  1.4234e-05,\n",
      "         2.3992e-05,  2.5051e-05, -1.3845e-05,  6.6667e-05,  2.5481e-05,\n",
      "        -3.8339e-06, -2.9678e-05, -3.2914e-05,  1.4886e-06, -1.0968e-06,\n",
      "        -1.4918e-07, -1.0137e-05, -4.9699e-06,  1.7838e-06,  1.6942e-05,\n",
      "        -3.1692e-05, -1.3105e-05, -7.0346e-05,  9.1989e-06,  0.0000e+00,\n",
      "         1.3555e-05,  6.0132e-05,  0.0000e+00,  7.1362e-06, -3.7176e-05,\n",
      "         8.6269e-06, -6.6942e-05, -2.7308e-05, -8.0061e-06,  1.5280e-05,\n",
      "         1.2221e-07, -5.6354e-06,  3.8701e-05,  4.3698e-05, -6.1100e-07,\n",
      "        -9.0578e-06, -7.1856e-06, -7.2338e-05, -6.3836e-06, -4.2891e-06,\n",
      "        -4.6756e-05, -3.0511e-06, -7.0261e-05,  6.6354e-06, -6.4156e-07,\n",
      "        -9.2307e-06, -2.9799e-05, -2.9041e-06,  1.0829e-05,  1.4640e-06,\n",
      "        -9.0344e-07, -3.0583e-06, -3.1963e-05,  2.4695e-06, -1.2296e-05,\n",
      "        -1.4429e-06,  1.8337e-05, -1.1468e-07,  0.0000e+00,  9.6786e-06,\n",
      "         6.1790e-07, -1.7311e-05,  1.4886e-05,  5.4242e-05,  0.0000e+00,\n",
      "        -1.4872e-05,  7.0680e-06,  0.0000e+00, -9.0705e-06, -6.6738e-06,\n",
      "         0.0000e+00,  2.5462e-05,  2.6095e-05, -1.5841e-06,  5.5961e-06,\n",
      "        -1.1783e-05, -2.8445e-06,  3.4865e-06,  0.0000e+00,  8.2411e-06,\n",
      "         3.3642e-06, -2.3953e-05,  9.9557e-06,  4.3321e-05, -3.7365e-06,\n",
      "        -1.7976e-05,  1.5068e-05, -1.2244e-05,  5.3373e-05, -1.0431e-05,\n",
      "         3.0783e-05,  7.5035e-06,  3.2532e-05,  6.3996e-07,  2.1201e-06,\n",
      "        -1.1730e-06, -7.5770e-06,  5.4622e-05, -8.7737e-06, -2.2859e-05,\n",
      "        -1.6449e-05,  1.8989e-05,  0.0000e+00,  1.9957e-05,  0.0000e+00,\n",
      "        -8.9059e-06,  2.2950e-05, -6.9170e-06, -4.7682e-05, -5.5867e-06,\n",
      "        -1.5707e-05,  2.5580e-05, -8.6706e-05,  2.1614e-06, -1.1215e-05,\n",
      "        -2.4201e-06, -3.9807e-05, -1.3879e-05,  7.6395e-06,  3.2419e-05,\n",
      "        -5.9092e-07,  0.0000e+00, -3.8412e-06, -6.2166e-05,  0.0000e+00,\n",
      "         2.0314e-07, -1.0679e-05,  6.3801e-06, -8.0621e-06, -2.2495e-05,\n",
      "        -2.5794e-06,  5.4744e-06,  0.0000e+00, -1.5991e-06,  1.9810e-06,\n",
      "        -1.4802e-05,  3.5290e-06, -7.0094e-07,  5.0785e-06,  4.7210e-05,\n",
      "        -6.2715e-05,  1.5353e-05, -8.0912e-05,  1.2344e-06,  2.8245e-05,\n",
      "         1.6297e-06,  9.8054e-07, -1.4500e-06,  9.1233e-06,  0.0000e+00,\n",
      "        -5.7548e-05,  7.9784e-07,  3.4731e-05, -4.5935e-05,  0.0000e+00,\n",
      "        -3.3991e-05, -3.8135e-05,  1.0840e-05,  7.6413e-06,  4.4740e-05,\n",
      "         1.1568e-06,  8.7894e-10,  0.0000e+00,  5.0240e-06,  1.5717e-05,\n",
      "         3.0172e-06,  0.0000e+00,  2.4443e-05, -1.3932e-06, -8.7192e-07,\n",
      "        -1.3301e-06,  9.8972e-08,  0.0000e+00, -2.5029e-05,  5.3791e-06,\n",
      "         1.5695e-05,  4.5583e-07, -7.6430e-07,  1.6138e-05, -1.4537e-05,\n",
      "         1.8660e-05,  1.4328e-05,  6.9026e-06,  1.1992e-06,  5.5921e-06,\n",
      "        -3.0571e-09, -4.0208e-06, -9.6747e-05,  2.1192e-06,  2.5140e-05,\n",
      "         6.4067e-06, -1.6023e-05, -1.6353e-05, -3.8285e-06, -2.0638e-06,\n",
      "        -1.3787e-05,  1.4410e-05,  1.6168e-06,  0.0000e+00,  2.4901e-06,\n",
      "         0.0000e+00, -6.1390e-05,  2.6121e-05,  2.2791e-05, -3.1632e-07,\n",
      "        -2.0822e-05, -4.1406e-05,  2.3772e-06, -5.0922e-06,  7.1827e-05,\n",
      "        -3.6756e-06,  9.9112e-06,  1.7665e-05, -1.8764e-07, -8.2462e-05,\n",
      "         2.4424e-05, -1.8893e-05, -2.4933e-06,  2.7231e-05, -1.2233e-05,\n",
      "         1.1845e-06,  9.9137e-07,  1.6564e-06,  2.0533e-06, -7.1500e-07,\n",
      "        -3.9646e-05, -4.3920e-05, -3.8459e-06,  2.6964e-05, -3.5132e-05,\n",
      "         2.2194e-07, -4.1074e-06, -1.3880e-05, -3.1970e-06,  2.4049e-05,\n",
      "        -3.6069e-06, -1.5321e-05, -1.4787e-05, -1.3686e-07, -1.1467e-05,\n",
      "         0.0000e+00,  1.3728e-07,  1.8778e-05, -2.9077e-05, -1.5609e-05,\n",
      "        -1.9530e-05, -7.1013e-07,  5.6913e-05,  1.0717e-06, -2.1381e-05,\n",
      "        -5.6700e-06,  1.0015e-04, -1.9864e-05,  5.4379e-05,  1.9938e-05,\n",
      "        -9.3073e-07,  0.0000e+00,  1.0250e-06, -7.3005e-06,  6.9748e-06,\n",
      "         5.7047e-05,  1.2496e-05, -3.2120e-05, -1.7795e-06,  0.0000e+00,\n",
      "         1.6306e-05, -3.3071e-06,  2.2218e-05,  1.7924e-06,  3.9045e-06,\n",
      "        -2.5771e-05,  5.0300e-05, -7.3440e-06, -2.3401e-05,  1.2955e-06,\n",
      "        -1.5173e-06,  1.2538e-06,  5.2046e-05,  3.9561e-05, -6.1005e-05,\n",
      "        -5.5012e-06, -1.2580e-05, -2.2842e-05,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.6366e-06, -1.7024e-05,  4.4695e-05,  1.5917e-07,\n",
      "        -5.7369e-06, -9.1032e-06, -4.9880e-06,  1.2506e-06, -3.6711e-06,\n",
      "         6.3483e-08,  8.3124e-06,  6.8066e-08, -4.8726e-07,  6.7613e-07,\n",
      "        -2.2803e-05,  4.7221e-05, -2.9878e-05,  0.0000e+00, -3.2675e-06,\n",
      "        -5.2494e-05,  1.2600e-05,  2.3590e-05, -4.5195e-08,  1.2084e-05,\n",
      "         6.5317e-05, -1.9959e-05, -9.9063e-07,  3.8354e-05,  8.9053e-06,\n",
      "         4.8397e-05,  7.0327e-06, -1.2775e-06, -2.0333e-05, -3.3122e-06,\n",
      "         4.7001e-05, -6.4541e-05,  7.4413e-05,  1.3623e-05, -2.2343e-05,\n",
      "         5.6330e-06, -5.5653e-06,  5.9903e-05, -2.4342e-05,  7.7037e-06,\n",
      "         3.2755e-05, -6.2526e-06, -3.3198e-06, -3.9569e-05, -1.8499e-05,\n",
      "        -8.1332e-06,  8.7519e-06, -2.1425e-05,  2.2847e-05, -3.3200e-05,\n",
      "         1.1753e-05,  3.0872e-05,  6.5473e-06, -1.4419e-05,  1.7467e-05,\n",
      "         5.7889e-05,  3.1520e-05, -5.6288e-06, -9.1647e-06,  1.6757e-05,\n",
      "         0.0000e+00, -2.8972e-05, -3.0664e-05,  7.9971e-06, -4.5715e-07,\n",
      "         5.1550e-06,  2.3444e-05, -3.0392e-07,  1.9583e-05,  2.9333e-05,\n",
      "        -3.8898e-06, -6.6011e-06,  2.5532e-05, -4.7621e-06, -3.2689e-06,\n",
      "        -6.2244e-06,  5.5225e-06,  5.4578e-05,  0.0000e+00,  0.0000e+00,\n",
      "        -3.6249e-06, -1.0404e-05,  1.2260e-05,  5.6485e-06,  1.8393e-05,\n",
      "         0.0000e+00, -6.2603e-06, -7.1133e-06, -5.1811e-05, -2.3774e-05,\n",
      "         2.1392e-05,  0.0000e+00,  0.0000e+00,  4.1541e-05,  1.4280e-05,\n",
      "         1.6552e-05,  0.0000e+00,  6.1999e-06,  1.1486e-06,  1.0785e-05,\n",
      "         4.6740e-05, -6.3256e-05, -6.4956e-05, -1.2181e-05,  0.0000e+00,\n",
      "        -1.6742e-06,  1.6604e-07,  5.3331e-06,  2.1492e-05,  2.2762e-05,\n",
      "        -1.3252e-06,  1.3765e-05,  1.3179e-05, -2.6177e-05,  2.9614e-06,\n",
      "        -6.3315e-06,  6.6043e-06,  1.7297e-06,  1.4531e-05, -1.4466e-06,\n",
      "        -3.1344e-05,  4.6003e-05, -5.1784e-06,  2.3043e-05, -7.0156e-06,\n",
      "         1.7836e-05, -6.3008e-06,  0.0000e+00, -5.4676e-06, -2.5680e-07,\n",
      "        -5.2855e-05,  7.2997e-05,  1.1458e-06,  1.6095e-05,  0.0000e+00,\n",
      "         3.5814e-05, -3.3796e-06,  2.0375e-05, -6.1299e-07,  7.9336e-07,\n",
      "        -1.1082e-05,  1.1538e-05, -2.9224e-05,  7.0791e-07,  7.8512e-06,\n",
      "        -2.4021e-05, -2.2157e-06, -9.0387e-06, -2.2548e-05,  4.2905e-05,\n",
      "         3.5104e-06, -1.4264e-06, -8.3500e-06, -3.2470e-08,  3.7577e-05,\n",
      "         2.3524e-06,  6.8589e-05, -1.5428e-06, -4.2747e-05,  1.7781e-06,\n",
      "         8.7205e-07,  9.2870e-06, -2.7251e-07, -1.2001e-05, -3.4366e-06,\n",
      "         4.4332e-07,  8.8611e-06, -1.6704e-06, -1.8096e-06,  6.2001e-07,\n",
      "         0.0000e+00, -7.3987e-06, -5.7508e-05, -2.0613e-05, -1.7233e-06,\n",
      "        -7.3989e-07,  2.2224e-05,  1.3277e-06,  2.3963e-05,  1.1860e-05,\n",
      "        -8.4531e-06,  1.1590e-05, -2.6392e-07, -8.8386e-05, -1.1518e-06,\n",
      "         0.0000e+00,  7.4797e-07, -1.9657e-05,  4.1970e-06, -4.5585e-05,\n",
      "         1.1099e-05,  5.6770e-05])\n",
      "\n",
      "<TBackward object at 0x7fb42b532250>\n",
      "<AccumulateGrad object at 0x7fb4b7b8d250>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.1266, -0.1425,  0.2257,  0.2979],\n",
      "        [ 0.1117, -0.3096,  0.1358,  0.4590],\n",
      "        [ 0.4122,  0.1309, -0.0993, -0.4918],\n",
      "        ...,\n",
      "        [ 0.3904,  0.4327, -0.4662, -0.2754],\n",
      "        [-0.2200,  0.2023, -0.0253,  0.3765],\n",
      "        [-0.4129, -0.2818, -0.4028, -0.2308]], requires_grad=True)\n",
      " - gradient: tensor([[ 3.0934e-07,  7.8290e-06,  4.4565e-07, -9.3330e-06],\n",
      "        [ 1.1785e-06,  2.9825e-05,  1.6977e-06, -3.5555e-05],\n",
      "        [-1.8282e-06, -3.1999e-05,  2.3822e-06,  4.9341e-05],\n",
      "        ...,\n",
      "        [-2.9343e-06, -4.6731e-05,  3.7075e-06,  7.1715e-05],\n",
      "        [-5.8789e-07, -1.7307e-05, -5.7119e-07,  2.2572e-05],\n",
      "        [ 5.2735e-06,  9.4497e-05, -5.6215e-06, -1.4112e-04]])\n",
      "\n",
      "<TBackward object at 0x7fb42b5843d0>\n",
      "<AccumulateGrad object at 0x7fb4b7b8d510>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.0190,  0.0440, -0.0216,  ..., -0.0135, -0.0299,  0.0425],\n",
      "        [ 0.0013, -0.0225,  0.0084,  ...,  0.0287,  0.0114, -0.0242]],\n",
      "       requires_grad=True)\n",
      " - gradient: tensor([[-0.0002, -0.0004,  0.0015,  ...,  0.0014, -0.0001,  0.0002],\n",
      "        [ 0.0002,  0.0004, -0.0015,  ..., -0.0014,  0.0001, -0.0002]])\n",
      "\n",
      "<NegBackward object at 0x7fb4b7b20850>\n",
      "<MeanBackward0 object at 0x7fb4b7b7f5d0>\n",
      "<MulBackward0 object at 0x7fb4b7b8d390>\n",
      "<IndexBackward object at 0x7fb4b7b8da10>\n",
      "<LogSoftmaxBackward object at 0x7fb4bfd2e390>\n",
      "<AddmmBackward object at 0x7fb4bfc28450>\n",
      "<AccumulateGrad object at 0x7fb4bfc283d0>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([0.0304, 0.0143], requires_grad=True)\n",
      " - gradient: tensor([ 0.0008, -0.0008])\n",
      "\n",
      "<ReluBackward0 object at 0x7fb4b7c3b250>\n",
      "<AddmmBackward object at 0x7fb4b7b8d490>\n",
      "<AccumulateGrad object at 0x7fb4b7b8d510>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([ 0.0836,  0.1201,  0.3913, -0.4708,  0.1459,  0.2671, -0.0359,  0.2574,\n",
      "         0.4952, -0.0368, -0.3959, -0.3020,  0.3118,  0.3358,  0.1539, -0.4785,\n",
      "        -0.4129,  0.0446, -0.1756,  0.4835,  0.4915, -0.3973,  0.1037,  0.1898,\n",
      "         0.4363, -0.3464,  0.0620, -0.1739,  0.3425, -0.1533,  0.1993,  0.3633,\n",
      "         0.0843,  0.1211,  0.3498,  0.1681,  0.3676,  0.2115,  0.4674,  0.2326,\n",
      "        -0.3346, -0.1176, -0.2113, -0.4495, -0.3470,  0.3860,  0.0573,  0.3063,\n",
      "        -0.4439, -0.2461, -0.2272, -0.1683, -0.1909,  0.1479, -0.0597, -0.1091,\n",
      "        -0.1934,  0.2324, -0.0354,  0.1991, -0.3690, -0.4826,  0.1946, -0.0871,\n",
      "         0.1291, -0.4522, -0.3160,  0.2630,  0.3498, -0.2587,  0.0279, -0.3408,\n",
      "         0.3226,  0.2125, -0.4604, -0.1560, -0.0347,  0.0404,  0.1804, -0.0224,\n",
      "        -0.3534,  0.2811,  0.2626, -0.3701, -0.2960, -0.3046,  0.0219, -0.1198,\n",
      "        -0.4140,  0.3908,  0.0461, -0.2172,  0.3942,  0.4233, -0.4196,  0.1689,\n",
      "         0.3650, -0.4269, -0.0369, -0.3067, -0.4417, -0.4151,  0.2616,  0.1498,\n",
      "        -0.4283, -0.1717, -0.4473,  0.1233, -0.4644, -0.4687, -0.2934, -0.4000,\n",
      "        -0.3685, -0.1792, -0.2775,  0.2510,  0.1357,  0.0069,  0.2857,  0.3933,\n",
      "         0.1654, -0.3851, -0.0755, -0.4759, -0.4915,  0.4459, -0.3034,  0.2906,\n",
      "         0.2900,  0.4937, -0.3221, -0.2144, -0.3886,  0.3191, -0.4876, -0.3652,\n",
      "         0.3490,  0.0984, -0.3171, -0.2651,  0.4516,  0.3145,  0.0015, -0.3286,\n",
      "         0.1114, -0.3040,  0.2521, -0.1331,  0.2922,  0.3933, -0.1744, -0.4901,\n",
      "        -0.3057,  0.0713, -0.4555,  0.4353,  0.3059,  0.3596,  0.0623,  0.1541,\n",
      "        -0.3760, -0.4785, -0.3927,  0.2805, -0.3923, -0.0644,  0.0854, -0.1968,\n",
      "         0.1114,  0.0407, -0.2102, -0.3769,  0.4809, -0.2539,  0.4103, -0.1010,\n",
      "         0.4072,  0.3728,  0.0574, -0.3198,  0.3949, -0.4903,  0.2637, -0.3264,\n",
      "        -0.3623, -0.0717,  0.2134,  0.1785,  0.3292,  0.2256, -0.3100, -0.1961,\n",
      "        -0.4485,  0.4104,  0.3819,  0.0430, -0.4650,  0.4618, -0.2138,  0.2528,\n",
      "        -0.4245,  0.3318, -0.4590, -0.0863, -0.3847,  0.3796,  0.3232,  0.4695,\n",
      "         0.2984, -0.0836,  0.0482,  0.4229, -0.2129,  0.2359,  0.1178,  0.1183,\n",
      "         0.2310, -0.0345, -0.2704,  0.4266, -0.2784, -0.4628,  0.3170, -0.0621,\n",
      "         0.1038,  0.4499, -0.2339, -0.0101, -0.4206,  0.4886, -0.3913, -0.1837,\n",
      "         0.0342,  0.0607,  0.1452, -0.0788,  0.4838,  0.1925,  0.0524,  0.2410,\n",
      "         0.2798,  0.4843,  0.2896,  0.4449,  0.0737, -0.1303,  0.1929, -0.0907,\n",
      "        -0.0520,  0.3719, -0.4793,  0.3978, -0.0143, -0.0567, -0.1887, -0.2277,\n",
      "         0.1942, -0.4367, -0.3477, -0.0504, -0.2108,  0.0433, -0.1850,  0.4378,\n",
      "         0.3196,  0.0840, -0.0690, -0.4070,  0.4929, -0.2717, -0.3933,  0.2984,\n",
      "        -0.0181,  0.1961, -0.1680,  0.3318, -0.3509, -0.1173,  0.3255, -0.3857,\n",
      "        -0.3023,  0.0645, -0.2970,  0.1276, -0.1310, -0.4644, -0.2819,  0.3859,\n",
      "         0.2503, -0.2184, -0.2089,  0.2583,  0.3238, -0.1235, -0.4893,  0.0641,\n",
      "         0.3571,  0.0888, -0.3076,  0.3594, -0.4102,  0.2305, -0.0202, -0.3252,\n",
      "         0.2335, -0.3692,  0.2389, -0.4916, -0.0365,  0.0402,  0.2245,  0.0583,\n",
      "         0.4531, -0.3001, -0.4821, -0.3203, -0.3823,  0.2183,  0.3845, -0.4296,\n",
      "        -0.4567, -0.1915, -0.2142,  0.0218, -0.0309,  0.4453, -0.4203, -0.2752,\n",
      "         0.2268,  0.3277, -0.2689, -0.4621, -0.2239, -0.2595, -0.3639, -0.4399,\n",
      "        -0.3379, -0.3549,  0.3601, -0.2752,  0.2462,  0.4119, -0.4212,  0.4698,\n",
      "         0.4719, -0.1430, -0.0725,  0.1860, -0.3070,  0.0802, -0.2508,  0.2254,\n",
      "         0.4213,  0.0239, -0.4746, -0.0123, -0.1043,  0.4478, -0.1762,  0.4976,\n",
      "         0.0217, -0.0891, -0.2247, -0.0114,  0.1388,  0.2436,  0.0469,  0.3821,\n",
      "        -0.3866, -0.0517, -0.3217,  0.4172,  0.2929, -0.3503,  0.4116,  0.1146,\n",
      "         0.1528,  0.0206,  0.1911,  0.1625, -0.2440,  0.1614,  0.4337,  0.3073,\n",
      "        -0.3003,  0.2351, -0.0974, -0.0076,  0.0212, -0.1825,  0.1219, -0.0410,\n",
      "        -0.1847, -0.4130, -0.1323,  0.1678,  0.0212, -0.0952, -0.3334, -0.4821,\n",
      "        -0.2706,  0.0537,  0.4572,  0.4866, -0.0385, -0.2521,  0.3174,  0.0082,\n",
      "         0.1059, -0.0188, -0.2053, -0.3943, -0.3875,  0.0307, -0.0499,  0.0293,\n",
      "        -0.3876, -0.4962,  0.1875, -0.1148,  0.2897,  0.0369,  0.4173,  0.4194,\n",
      "        -0.3274, -0.2538,  0.4459, -0.4948,  0.4447,  0.1152,  0.0992, -0.0110,\n",
      "         0.1374, -0.1414,  0.0085, -0.2994,  0.4711, -0.3588, -0.1394,  0.1996,\n",
      "         0.4288,  0.4630,  0.4356, -0.2831, -0.0762, -0.1669,  0.4988, -0.3871,\n",
      "        -0.4040, -0.3893,  0.3146,  0.3632,  0.3587,  0.3331, -0.2225,  0.0325,\n",
      "        -0.3996, -0.1156,  0.1463,  0.4661, -0.3104, -0.4605, -0.2749, -0.4925,\n",
      "        -0.1833, -0.4126, -0.2014, -0.0716, -0.1839,  0.4160, -0.2475, -0.4047,\n",
      "         0.1038,  0.2200,  0.4885,  0.3791,  0.2512,  0.0601,  0.2680,  0.4170,\n",
      "        -0.0420,  0.3225, -0.4039, -0.0183, -0.4965,  0.1191,  0.3034, -0.2433,\n",
      "         0.3667,  0.1454, -0.2418,  0.4600,  0.4698, -0.3946, -0.3335,  0.3939,\n",
      "         0.1892,  0.3304,  0.0140, -0.4216, -0.4335, -0.2401,  0.0483,  0.0603,\n",
      "        -0.4004, -0.4962, -0.4113,  0.3255, -0.4179,  0.3235,  0.0282,  0.1130],\n",
      "       requires_grad=True)\n",
      " - gradient: tensor([-5.5395e-06, -2.1103e-05, -3.3668e-05,  2.2950e-05,  2.7552e-05,\n",
      "         6.7456e-06, -3.9756e-05, -9.3146e-08,  4.3260e-05, -3.3086e-06,\n",
      "        -1.1633e-06,  0.0000e+00, -1.2658e-06, -2.2893e-07, -9.2255e-06,\n",
      "        -7.8480e-06,  0.0000e+00,  3.4819e-05, -4.1653e-05,  1.8067e-05,\n",
      "         2.9579e-06,  6.4237e-07, -3.1728e-05,  5.6861e-05,  2.6782e-05,\n",
      "         0.0000e+00, -1.0867e-05,  2.1712e-05, -1.7744e-05,  1.4234e-05,\n",
      "         2.3992e-05,  2.5051e-05, -1.3845e-05,  6.6667e-05,  2.5481e-05,\n",
      "        -3.8339e-06, -2.9678e-05, -3.2914e-05,  1.4886e-06, -1.0968e-06,\n",
      "        -1.4918e-07, -1.0137e-05, -4.9699e-06,  1.7838e-06,  1.6942e-05,\n",
      "        -3.1692e-05, -1.3105e-05, -7.0346e-05,  9.1989e-06,  0.0000e+00,\n",
      "         1.3555e-05,  6.0132e-05,  0.0000e+00,  7.1362e-06, -3.7176e-05,\n",
      "         8.6269e-06, -6.6942e-05, -2.7308e-05, -8.0061e-06,  1.5280e-05,\n",
      "         1.2221e-07, -5.6354e-06,  3.8701e-05,  4.3698e-05, -6.1100e-07,\n",
      "        -9.0578e-06, -7.1856e-06, -7.2338e-05, -6.3836e-06, -4.2891e-06,\n",
      "        -4.6756e-05, -3.0511e-06, -7.0261e-05,  6.6354e-06, -6.4156e-07,\n",
      "        -9.2307e-06, -2.9799e-05, -2.9041e-06,  1.0829e-05,  1.4640e-06,\n",
      "        -9.0344e-07, -3.0583e-06, -3.1963e-05,  2.4695e-06, -1.2296e-05,\n",
      "        -1.4429e-06,  1.8337e-05, -1.1468e-07,  0.0000e+00,  9.6786e-06,\n",
      "         6.1790e-07, -1.7311e-05,  1.4886e-05,  5.4242e-05,  0.0000e+00,\n",
      "        -1.4872e-05,  7.0680e-06,  0.0000e+00, -9.0705e-06, -6.6738e-06,\n",
      "         0.0000e+00,  2.5462e-05,  2.6095e-05, -1.5841e-06,  5.5961e-06,\n",
      "        -1.1783e-05, -2.8445e-06,  3.4865e-06,  0.0000e+00,  8.2411e-06,\n",
      "         3.3642e-06, -2.3953e-05,  9.9557e-06,  4.3321e-05, -3.7365e-06,\n",
      "        -1.7976e-05,  1.5068e-05, -1.2244e-05,  5.3373e-05, -1.0431e-05,\n",
      "         3.0783e-05,  7.5035e-06,  3.2532e-05,  6.3996e-07,  2.1201e-06,\n",
      "        -1.1730e-06, -7.5770e-06,  5.4622e-05, -8.7737e-06, -2.2859e-05,\n",
      "        -1.6449e-05,  1.8989e-05,  0.0000e+00,  1.9957e-05,  0.0000e+00,\n",
      "        -8.9059e-06,  2.2950e-05, -6.9170e-06, -4.7682e-05, -5.5867e-06,\n",
      "        -1.5707e-05,  2.5580e-05, -8.6706e-05,  2.1614e-06, -1.1215e-05,\n",
      "        -2.4201e-06, -3.9807e-05, -1.3879e-05,  7.6395e-06,  3.2419e-05,\n",
      "        -5.9092e-07,  0.0000e+00, -3.8412e-06, -6.2166e-05,  0.0000e+00,\n",
      "         2.0314e-07, -1.0679e-05,  6.3801e-06, -8.0621e-06, -2.2495e-05,\n",
      "        -2.5794e-06,  5.4744e-06,  0.0000e+00, -1.5991e-06,  1.9810e-06,\n",
      "        -1.4802e-05,  3.5290e-06, -7.0094e-07,  5.0785e-06,  4.7210e-05,\n",
      "        -6.2715e-05,  1.5353e-05, -8.0912e-05,  1.2344e-06,  2.8245e-05,\n",
      "         1.6297e-06,  9.8054e-07, -1.4500e-06,  9.1233e-06,  0.0000e+00,\n",
      "        -5.7548e-05,  7.9784e-07,  3.4731e-05, -4.5935e-05,  0.0000e+00,\n",
      "        -3.3991e-05, -3.8135e-05,  1.0840e-05,  7.6413e-06,  4.4740e-05,\n",
      "         1.1568e-06,  8.7894e-10,  0.0000e+00,  5.0240e-06,  1.5717e-05,\n",
      "         3.0172e-06,  0.0000e+00,  2.4443e-05, -1.3932e-06, -8.7192e-07,\n",
      "        -1.3301e-06,  9.8972e-08,  0.0000e+00, -2.5029e-05,  5.3791e-06,\n",
      "         1.5695e-05,  4.5583e-07, -7.6430e-07,  1.6138e-05, -1.4537e-05,\n",
      "         1.8660e-05,  1.4328e-05,  6.9026e-06,  1.1992e-06,  5.5921e-06,\n",
      "        -3.0571e-09, -4.0208e-06, -9.6747e-05,  2.1192e-06,  2.5140e-05,\n",
      "         6.4067e-06, -1.6023e-05, -1.6353e-05, -3.8285e-06, -2.0638e-06,\n",
      "        -1.3787e-05,  1.4410e-05,  1.6168e-06,  0.0000e+00,  2.4901e-06,\n",
      "         0.0000e+00, -6.1390e-05,  2.6121e-05,  2.2791e-05, -3.1632e-07,\n",
      "        -2.0822e-05, -4.1406e-05,  2.3772e-06, -5.0922e-06,  7.1827e-05,\n",
      "        -3.6756e-06,  9.9112e-06,  1.7665e-05, -1.8764e-07, -8.2462e-05,\n",
      "         2.4424e-05, -1.8893e-05, -2.4933e-06,  2.7231e-05, -1.2233e-05,\n",
      "         1.1845e-06,  9.9137e-07,  1.6564e-06,  2.0533e-06, -7.1500e-07,\n",
      "        -3.9646e-05, -4.3920e-05, -3.8459e-06,  2.6964e-05, -3.5132e-05,\n",
      "         2.2194e-07, -4.1074e-06, -1.3880e-05, -3.1970e-06,  2.4049e-05,\n",
      "        -3.6069e-06, -1.5321e-05, -1.4787e-05, -1.3686e-07, -1.1467e-05,\n",
      "         0.0000e+00,  1.3728e-07,  1.8778e-05, -2.9077e-05, -1.5609e-05,\n",
      "        -1.9530e-05, -7.1013e-07,  5.6913e-05,  1.0717e-06, -2.1381e-05,\n",
      "        -5.6700e-06,  1.0015e-04, -1.9864e-05,  5.4379e-05,  1.9938e-05,\n",
      "        -9.3073e-07,  0.0000e+00,  1.0250e-06, -7.3005e-06,  6.9748e-06,\n",
      "         5.7047e-05,  1.2496e-05, -3.2120e-05, -1.7795e-06,  0.0000e+00,\n",
      "         1.6306e-05, -3.3071e-06,  2.2218e-05,  1.7924e-06,  3.9045e-06,\n",
      "        -2.5771e-05,  5.0300e-05, -7.3440e-06, -2.3401e-05,  1.2955e-06,\n",
      "        -1.5173e-06,  1.2538e-06,  5.2046e-05,  3.9561e-05, -6.1005e-05,\n",
      "        -5.5012e-06, -1.2580e-05, -2.2842e-05,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.6366e-06, -1.7024e-05,  4.4695e-05,  1.5917e-07,\n",
      "        -5.7369e-06, -9.1032e-06, -4.9880e-06,  1.2506e-06, -3.6711e-06,\n",
      "         6.3483e-08,  8.3124e-06,  6.8066e-08, -4.8726e-07,  6.7613e-07,\n",
      "        -2.2803e-05,  4.7221e-05, -2.9878e-05,  0.0000e+00, -3.2675e-06,\n",
      "        -5.2494e-05,  1.2600e-05,  2.3590e-05, -4.5195e-08,  1.2084e-05,\n",
      "         6.5317e-05, -1.9959e-05, -9.9063e-07,  3.8354e-05,  8.9053e-06,\n",
      "         4.8397e-05,  7.0327e-06, -1.2775e-06, -2.0333e-05, -3.3122e-06,\n",
      "         4.7001e-05, -6.4541e-05,  7.4413e-05,  1.3623e-05, -2.2343e-05,\n",
      "         5.6330e-06, -5.5653e-06,  5.9903e-05, -2.4342e-05,  7.7037e-06,\n",
      "         3.2755e-05, -6.2526e-06, -3.3198e-06, -3.9569e-05, -1.8499e-05,\n",
      "        -8.1332e-06,  8.7519e-06, -2.1425e-05,  2.2847e-05, -3.3200e-05,\n",
      "         1.1753e-05,  3.0872e-05,  6.5473e-06, -1.4419e-05,  1.7467e-05,\n",
      "         5.7889e-05,  3.1520e-05, -5.6288e-06, -9.1647e-06,  1.6757e-05,\n",
      "         0.0000e+00, -2.8972e-05, -3.0664e-05,  7.9971e-06, -4.5715e-07,\n",
      "         5.1550e-06,  2.3444e-05, -3.0392e-07,  1.9583e-05,  2.9333e-05,\n",
      "        -3.8898e-06, -6.6011e-06,  2.5532e-05, -4.7621e-06, -3.2689e-06,\n",
      "        -6.2244e-06,  5.5225e-06,  5.4578e-05,  0.0000e+00,  0.0000e+00,\n",
      "        -3.6249e-06, -1.0404e-05,  1.2260e-05,  5.6485e-06,  1.8393e-05,\n",
      "         0.0000e+00, -6.2603e-06, -7.1133e-06, -5.1811e-05, -2.3774e-05,\n",
      "         2.1392e-05,  0.0000e+00,  0.0000e+00,  4.1541e-05,  1.4280e-05,\n",
      "         1.6552e-05,  0.0000e+00,  6.1999e-06,  1.1486e-06,  1.0785e-05,\n",
      "         4.6740e-05, -6.3256e-05, -6.4956e-05, -1.2181e-05,  0.0000e+00,\n",
      "        -1.6742e-06,  1.6604e-07,  5.3331e-06,  2.1492e-05,  2.2762e-05,\n",
      "        -1.3252e-06,  1.3765e-05,  1.3179e-05, -2.6177e-05,  2.9614e-06,\n",
      "        -6.3315e-06,  6.6043e-06,  1.7297e-06,  1.4531e-05, -1.4466e-06,\n",
      "        -3.1344e-05,  4.6003e-05, -5.1784e-06,  2.3043e-05, -7.0156e-06,\n",
      "         1.7836e-05, -6.3008e-06,  0.0000e+00, -5.4676e-06, -2.5680e-07,\n",
      "        -5.2855e-05,  7.2997e-05,  1.1458e-06,  1.6095e-05,  0.0000e+00,\n",
      "         3.5814e-05, -3.3796e-06,  2.0375e-05, -6.1299e-07,  7.9336e-07,\n",
      "        -1.1082e-05,  1.1538e-05, -2.9224e-05,  7.0791e-07,  7.8512e-06,\n",
      "        -2.4021e-05, -2.2157e-06, -9.0387e-06, -2.2548e-05,  4.2905e-05,\n",
      "         3.5104e-06, -1.4264e-06, -8.3500e-06, -3.2470e-08,  3.7577e-05,\n",
      "         2.3524e-06,  6.8589e-05, -1.5428e-06, -4.2747e-05,  1.7781e-06,\n",
      "         8.7205e-07,  9.2870e-06, -2.7251e-07, -1.2001e-05, -3.4366e-06,\n",
      "         4.4332e-07,  8.8611e-06, -1.6704e-06, -1.8096e-06,  6.2001e-07,\n",
      "         0.0000e+00, -7.3987e-06, -5.7508e-05, -2.0613e-05, -1.7233e-06,\n",
      "        -7.3989e-07,  2.2224e-05,  1.3277e-06,  2.3963e-05,  1.1860e-05,\n",
      "        -8.4531e-06,  1.1590e-05, -2.6392e-07, -8.8386e-05, -1.1518e-06,\n",
      "         0.0000e+00,  7.4797e-07, -1.9657e-05,  4.1970e-06, -4.5585e-05,\n",
      "         1.1099e-05,  5.6770e-05])\n",
      "\n",
      "<TBackward object at 0x7fb4b7b92850>\n",
      "<AccumulateGrad object at 0x7fb42b783690>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.1266, -0.1425,  0.2257,  0.2979],\n",
      "        [ 0.1117, -0.3096,  0.1358,  0.4590],\n",
      "        [ 0.4122,  0.1309, -0.0993, -0.4918],\n",
      "        ...,\n",
      "        [ 0.3904,  0.4327, -0.4662, -0.2754],\n",
      "        [-0.2200,  0.2023, -0.0253,  0.3765],\n",
      "        [-0.4129, -0.2818, -0.4028, -0.2308]], requires_grad=True)\n",
      " - gradient: tensor([[ 3.0934e-07,  7.8290e-06,  4.4565e-07, -9.3330e-06],\n",
      "        [ 1.1785e-06,  2.9825e-05,  1.6977e-06, -3.5555e-05],\n",
      "        [-1.8282e-06, -3.1999e-05,  2.3822e-06,  4.9341e-05],\n",
      "        ...,\n",
      "        [-2.9343e-06, -4.6731e-05,  3.7075e-06,  7.1715e-05],\n",
      "        [-5.8789e-07, -1.7307e-05, -5.7119e-07,  2.2572e-05],\n",
      "        [ 5.2735e-06,  9.4497e-05, -5.6215e-06, -1.4112e-04]])\n",
      "\n",
      "<TBackward object at 0x7fb4b7c3bb50>\n",
      "<AccumulateGrad object at 0x7fb42b783690>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.0190,  0.0440, -0.0216,  ..., -0.0135, -0.0299,  0.0425],\n",
      "        [ 0.0013, -0.0225,  0.0084,  ...,  0.0287,  0.0114, -0.0242]],\n",
      "       requires_grad=True)\n",
      " - gradient: tensor([[-0.0002, -0.0004,  0.0015,  ...,  0.0014, -0.0001,  0.0002],\n",
      "        [ 0.0002,  0.0004, -0.0015,  ..., -0.0014,  0.0001, -0.0002]])\n",
      "\n",
      "<MseLossBackward object at 0x7fb4b7b1dd90>\n",
      "<SqueezeBackward1 object at 0x7fb42b783690>\n",
      "<AddmmBackward object at 0x7fb42b783f10>\n",
      "<AccumulateGrad object at 0x7fb42b783890>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([-0.0368], requires_grad=True)\n",
      " - gradient: tensor([-0.0164])\n",
      "\n",
      "<ReluBackward0 object at 0x7fb42b783dd0>\n",
      "<AddmmBackward object at 0x7fb42b783410>\n",
      "<AccumulateGrad object at 0x7fb42b783050>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([-1.7258e-01,  3.1317e-01, -3.6147e-01,  6.0106e-02,  1.7817e-01,\n",
      "         3.5977e-01,  2.3683e-02,  4.0111e-01, -3.1846e-01, -2.2622e-01,\n",
      "        -1.1889e-01, -2.9198e-01,  3.5090e-01,  2.9462e-01, -2.7749e-01,\n",
      "        -4.9549e-01,  4.7134e-01,  4.6435e-01, -1.3984e-01, -3.3098e-02,\n",
      "        -4.9195e-01,  1.3929e-01,  3.0843e-02, -3.1897e-01,  2.7926e-02,\n",
      "         6.8812e-02,  2.8544e-01, -7.9420e-03,  2.1947e-01, -2.3720e-01,\n",
      "         3.8303e-01,  2.3182e-01,  2.6219e-01, -5.8280e-02,  1.1294e-01,\n",
      "         2.0967e-01, -8.5740e-03,  1.1582e-02, -9.8503e-02, -2.2281e-03,\n",
      "        -2.9563e-01, -7.3722e-02, -1.6605e-01, -3.1265e-01,  1.8440e-01,\n",
      "         2.5408e-01,  4.4106e-01,  3.9391e-01, -4.5660e-01,  6.0456e-02,\n",
      "        -3.5038e-01, -2.8609e-01, -2.6732e-01, -4.3374e-01,  1.5904e-01,\n",
      "        -2.1018e-01,  1.9407e-01,  1.9217e-01, -4.7874e-01,  3.3488e-01,\n",
      "        -2.0737e-01,  3.6393e-01,  3.7213e-01, -1.3094e-01,  1.3911e-01,\n",
      "        -2.8688e-02,  1.9042e-01,  6.1998e-02,  1.1475e-01, -2.8608e-01,\n",
      "        -3.7580e-02, -2.4006e-01,  4.7474e-02,  3.1186e-01, -3.9645e-01,\n",
      "         4.1236e-01, -9.3498e-02,  2.4836e-01, -3.3577e-01, -1.0054e-01,\n",
      "        -3.8700e-01,  1.5602e-01, -2.9337e-01, -1.0458e-01, -3.2496e-01,\n",
      "        -1.9448e-03, -3.7708e-01,  2.2214e-01,  4.3266e-01,  4.1844e-01,\n",
      "         4.7157e-02,  1.7708e-01,  4.5660e-01, -1.0064e-01, -4.7389e-01,\n",
      "         1.4798e-01, -1.4416e-03, -1.2553e-01, -4.3882e-01,  3.0965e-01,\n",
      "         4.3902e-01, -4.1797e-01,  3.7193e-01,  3.2796e-01, -3.2243e-01,\n",
      "         4.2213e-01, -4.2576e-01,  2.2112e-01,  3.7155e-02,  2.7272e-01,\n",
      "         3.7146e-01,  3.2612e-01,  2.8344e-01,  4.9917e-01,  2.4188e-01,\n",
      "         3.1477e-01, -3.4988e-02,  7.6714e-02, -1.9123e-01, -4.9087e-01,\n",
      "        -3.0976e-01,  1.5309e-03,  1.4217e-01,  4.2225e-01, -1.0427e-01,\n",
      "        -6.0892e-02, -4.5939e-01, -6.5729e-02,  4.7475e-01,  1.8222e-01,\n",
      "         3.1835e-01,  2.4089e-01,  1.9765e-01, -7.9627e-02,  4.2783e-01,\n",
      "        -3.6171e-01,  8.5160e-02,  4.3693e-01, -1.2587e-01, -1.4642e-01,\n",
      "        -2.3676e-01,  1.0058e-01, -3.9859e-01, -7.5806e-02,  4.9358e-01,\n",
      "         1.7875e-01, -4.5468e-01, -3.7693e-01,  5.7357e-02,  1.1249e-01,\n",
      "        -2.1343e-02,  2.1818e-02,  1.1968e-02,  3.6459e-01, -3.4428e-01,\n",
      "        -3.4172e-01,  1.0837e-01,  7.5531e-02, -4.4871e-01,  4.6357e-01,\n",
      "        -1.4988e-01, -2.4379e-01,  2.9256e-02,  3.8206e-01, -4.1999e-01,\n",
      "        -2.0893e-02, -4.9916e-01, -1.7714e-01,  4.8470e-01,  2.7786e-02,\n",
      "        -3.9544e-02,  1.4912e-01,  4.5935e-01, -4.2906e-01,  3.1437e-01,\n",
      "        -4.1533e-01, -3.0978e-01, -2.9989e-01,  4.9186e-01, -3.2567e-01,\n",
      "         1.5898e-01,  1.8070e-01,  1.3531e-01, -2.5595e-01,  1.3327e-01,\n",
      "         1.4501e-01, -4.2871e-01, -3.6666e-01,  4.6107e-01,  1.3996e-02,\n",
      "        -1.7897e-01,  4.2125e-02,  2.5730e-01,  3.3469e-01, -3.0685e-01,\n",
      "         2.2154e-01,  3.0093e-01, -3.7714e-02,  4.8250e-01, -7.1196e-02,\n",
      "        -1.9772e-01, -4.2802e-01, -1.5781e-03,  4.8850e-01,  2.6697e-02,\n",
      "        -3.8167e-01, -1.4267e-01,  3.3461e-01, -3.0559e-01,  8.4174e-02,\n",
      "         2.9031e-01, -3.2839e-01, -3.2271e-01,  8.6352e-02,  2.7615e-01,\n",
      "        -4.0215e-03,  3.8437e-01, -4.0683e-01,  2.3990e-01,  4.1137e-01,\n",
      "        -5.6875e-02, -4.4201e-01, -2.7213e-01, -2.7579e-01, -4.0107e-01,\n",
      "         4.5964e-01, -3.3930e-01,  3.8241e-01, -3.8931e-01,  4.8986e-01,\n",
      "         4.2871e-02,  1.3742e-01,  4.7240e-01,  3.8128e-01,  4.3449e-01,\n",
      "        -1.1114e-02,  1.7996e-01,  1.8085e-01, -2.8844e-01, -1.2324e-01,\n",
      "        -4.6709e-01, -2.3826e-01, -3.1781e-01, -5.4128e-02, -1.9883e-01,\n",
      "         2.9738e-01, -3.6760e-01,  3.7110e-01,  9.7562e-02, -3.2522e-01,\n",
      "         3.9965e-01,  6.5112e-02,  5.5576e-02,  4.4522e-04,  2.9769e-01,\n",
      "        -4.4161e-01, -6.3831e-03,  4.7236e-01, -3.5355e-01, -2.3606e-01,\n",
      "        -4.4206e-01,  1.6152e-01,  7.9279e-02, -3.2315e-01,  1.6230e-01,\n",
      "         1.8074e-01,  7.3438e-02, -2.7976e-01, -4.1445e-01,  5.9683e-02,\n",
      "         3.5115e-01, -3.5765e-02,  4.5202e-01, -3.0841e-01,  4.4264e-01,\n",
      "         3.4527e-01,  1.6227e-02, -4.0178e-02,  2.5361e-01, -4.9316e-01,\n",
      "         4.5350e-01,  2.5772e-01,  4.7556e-01, -3.6841e-01, -2.3446e-01,\n",
      "        -3.2026e-01,  4.8240e-01, -4.2123e-01, -1.5941e-01,  4.1258e-01,\n",
      "         9.8470e-02,  4.6070e-01, -4.8491e-01,  2.3586e-01,  1.8489e-01,\n",
      "         3.6421e-01,  3.0102e-01, -3.9864e-01, -1.0071e-01, -4.5050e-01,\n",
      "        -4.1038e-03,  1.0967e-01, -4.0598e-01, -1.1161e-02,  1.8620e-01,\n",
      "         4.2825e-01, -1.6838e-01, -3.9427e-01, -2.1116e-02, -6.4629e-03,\n",
      "         2.5245e-01,  1.2138e-01,  1.1311e-01,  3.2711e-01, -4.6454e-01,\n",
      "         1.7805e-01, -3.4509e-01, -3.1504e-01,  6.2247e-02, -4.4916e-01,\n",
      "        -2.2001e-01, -2.9125e-01, -1.5873e-01,  6.6639e-02,  3.8743e-01,\n",
      "        -2.6788e-01, -1.4956e-01,  3.9835e-02,  6.3047e-02,  2.6403e-01,\n",
      "         3.3495e-02, -1.2951e-01,  4.7620e-01, -1.3216e-01, -2.7305e-01,\n",
      "         9.2529e-02,  1.3057e-01,  7.8725e-02, -3.0282e-01, -2.4804e-01,\n",
      "        -2.5627e-03,  2.2423e-01, -4.5084e-01, -2.1646e-01,  3.8002e-01,\n",
      "        -2.1170e-01,  1.9236e-02,  5.6180e-02, -4.8531e-01,  2.0193e-01,\n",
      "         2.7138e-01, -1.7563e-01,  2.9796e-02, -3.7070e-02,  3.0743e-01,\n",
      "        -2.7574e-01,  3.6702e-02,  3.1282e-01, -2.6334e-01,  2.5735e-01,\n",
      "        -4.4645e-01, -1.2298e-01,  1.9513e-01,  1.0026e-01, -1.4244e-01,\n",
      "         3.9544e-01, -4.3579e-01,  1.4240e-01, -1.6809e-01,  2.3754e-01,\n",
      "         3.1009e-01,  4.7859e-01,  3.5252e-01, -1.5241e-01, -1.4565e-01,\n",
      "         1.4902e-01, -3.3845e-01,  2.3317e-01,  2.3684e-02, -1.8143e-02,\n",
      "        -3.4078e-01, -4.8378e-01, -1.7164e-01,  1.0475e-01,  5.3203e-02,\n",
      "         1.8436e-01, -3.6494e-01,  3.2978e-01, -4.5494e-01, -1.1316e-01,\n",
      "        -1.4219e-01, -3.2167e-01,  1.7914e-01, -1.7624e-01, -4.5788e-01,\n",
      "         5.9428e-02,  3.9384e-01, -1.6114e-01,  4.2302e-01, -2.9159e-01,\n",
      "        -1.3526e-01,  2.1192e-01,  8.8286e-03, -3.3314e-01,  2.8035e-01,\n",
      "        -3.0803e-01,  6.9566e-03, -3.7123e-01,  2.3956e-01,  4.2599e-01,\n",
      "         4.1197e-01,  4.3040e-01, -3.6341e-01, -3.2415e-01, -4.6980e-01,\n",
      "        -4.4103e-01,  7.6513e-02,  1.8845e-01,  1.2923e-02,  1.4887e-01,\n",
      "         4.1632e-01,  1.3493e-01,  3.4061e-01,  1.9352e-01, -3.6496e-02,\n",
      "        -2.9056e-01,  1.3489e-01, -3.1228e-01,  1.5023e-01, -2.5679e-01,\n",
      "         3.7713e-02, -4.1569e-01, -2.2919e-01,  2.9559e-01,  4.5267e-01,\n",
      "        -1.6092e-01, -1.5154e-01,  9.5470e-03,  2.2632e-01,  1.3045e-01,\n",
      "         2.1643e-02, -3.9859e-01,  2.9507e-02,  1.9144e-01,  1.7611e-02,\n",
      "         2.7099e-01,  3.8681e-01,  1.1455e-02,  4.8924e-01, -2.3205e-01,\n",
      "        -7.6734e-02,  1.2107e-01,  2.5240e-01, -1.3605e-01, -1.5680e-01,\n",
      "         8.7805e-02,  4.7960e-01, -3.4396e-01,  1.2111e-01, -3.2283e-01,\n",
      "        -2.1359e-01, -3.1828e-01,  8.1648e-02, -5.9442e-02, -5.8945e-02,\n",
      "         3.4734e-01, -6.7136e-02, -1.7182e-01,  2.5448e-01, -1.4441e-01,\n",
      "        -4.7413e-01,  1.9614e-01, -2.4280e-01,  9.0930e-02,  6.6545e-02,\n",
      "         1.2691e-01, -4.1699e-01, -3.4520e-01, -3.6441e-01, -4.1756e-01,\n",
      "        -3.3438e-01,  3.5522e-01, -1.4522e-01, -1.9562e-02,  3.8852e-02,\n",
      "         3.9627e-01,  3.4337e-01,  5.9154e-02,  2.2926e-01, -3.8187e-01,\n",
      "         4.9505e-01, -1.1302e-01, -2.0253e-01,  4.4525e-01,  2.5239e-01,\n",
      "         4.2764e-01,  1.2516e-01,  8.0199e-02,  3.0274e-01, -2.5924e-01,\n",
      "        -4.3176e-01, -3.6367e-01, -1.6519e-01,  4.5808e-01,  1.3583e-01,\n",
      "        -3.3026e-01, -2.0482e-01,  1.8515e-01, -3.4282e-01,  3.3720e-01,\n",
      "        -2.3179e-01,  2.3759e-02], requires_grad=True)\n",
      " - gradient: tensor([ 0.0000e+00, -2.1887e-04,  0.0000e+00, -1.7142e-04,  2.5949e-04,\n",
      "        -5.6621e-05,  6.9130e-06, -4.6410e-04,  0.0000e+00,  0.0000e+00,\n",
      "        -1.7946e-04,  1.8797e-04,  3.6629e-04,  1.7507e-04,  4.7700e-05,\n",
      "        -4.1748e-05,  5.4002e-04,  4.1625e-04, -1.1529e-04, -1.4551e-04,\n",
      "        -1.1968e-04,  1.6544e-04, -1.0685e-04,  5.9345e-06, -1.9433e-04,\n",
      "        -5.8177e-05,  2.1725e-04, -1.8483e-04, -4.3040e-04,  0.0000e+00,\n",
      "        -4.1777e-04,  5.3963e-04,  2.6036e-04, -3.4054e-04, -4.0316e-04,\n",
      "         3.3545e-04, -3.3437e-05, -1.7807e-04, -2.4860e-05,  2.8230e-04,\n",
      "        -1.3101e-04, -1.1515e-04,  9.1026e-06,  5.8969e-05, -3.9728e-05,\n",
      "        -2.7778e-04,  4.3274e-04, -2.8071e-04,  1.0199e-04,  8.4777e-06,\n",
      "         1.0320e-05,  9.7711e-05,  1.4711e-06,  0.0000e+00, -5.7785e-05,\n",
      "         1.9934e-04, -1.2314e-04,  4.4368e-05,  5.1663e-06, -4.0070e-04,\n",
      "        -8.5387e-05, -4.5900e-04,  5.1299e-04,  7.1188e-05, -6.7739e-05,\n",
      "         3.1507e-04, -1.5506e-04, -3.2090e-04,  5.5689e-04,  1.0865e-04,\n",
      "        -4.7842e-05,  6.4264e-05, -2.8035e-05,  9.5030e-05,  0.0000e+00,\n",
      "        -2.9460e-04, -2.3809e-04,  4.0579e-04,  6.5650e-05, -1.0135e-05,\n",
      "        -3.2343e-05, -1.2705e-04, -6.3317e-06,  1.9769e-05, -3.2678e-06,\n",
      "         1.5642e-04,  4.9431e-05, -8.4217e-05, -4.7051e-04,  3.5696e-04,\n",
      "         2.6721e-06,  6.0287e-04, -1.5735e-04, -1.6985e-05, -1.6961e-06,\n",
      "         2.3465e-04, -2.7549e-04, -1.1911e-04,  5.0855e-05,  6.7025e-04,\n",
      "        -1.3691e-04,  3.8212e-06, -7.5555e-05, -5.0038e-05,  0.0000e+00,\n",
      "         3.3613e-05,  4.1097e-05,  6.0591e-05,  2.4070e-04,  2.5587e-04,\n",
      "         3.6912e-04, -4.3214e-04, -2.3114e-04,  6.2035e-04, -5.6884e-05,\n",
      "         5.2146e-04, -2.4259e-04,  1.8841e-04, -1.0763e-04,  7.8558e-05,\n",
      "         1.9104e-05,  1.6689e-04, -2.6124e-04, -3.9062e-04,  5.3515e-05,\n",
      "         1.1060e-04, -2.5856e-05,  3.4881e-05, -2.4781e-04,  5.8886e-05,\n",
      "         4.4570e-04,  2.3833e-04, -5.0335e-04, -6.7366e-05, -1.4987e-04,\n",
      "         2.1621e-05, -3.1427e-04,  2.5310e-04, -3.1060e-05,  2.8710e-05,\n",
      "         0.0000e+00,  1.9465e-04,  1.4261e-04, -2.2845e-04,  6.0539e-04,\n",
      "         1.6066e-04,  5.4512e-05,  0.0000e+00,  1.2063e-05, -2.1460e-04,\n",
      "         2.5579e-04,  2.9673e-04,  3.2699e-04, -2.0643e-05, -7.8970e-06,\n",
      "         1.1396e-05, -4.1320e-04, -2.4095e-04,  7.1864e-06,  3.4499e-04,\n",
      "        -2.2011e-05, -3.9080e-05, -2.7090e-04, -1.9912e-04, -1.5254e-04,\n",
      "        -3.1333e-04,  1.1025e-04, -4.4995e-05, -3.6107e-04, -5.7297e-05,\n",
      "         6.0724e-05,  3.2629e-04, -2.8506e-04, -5.1021e-05,  1.0603e-04,\n",
      "         0.0000e+00, -6.8740e-05, -3.3634e-04,  2.7547e-04, -1.1107e-04,\n",
      "         4.0303e-04, -3.0418e-04, -8.1979e-05,  2.6708e-05,  2.1515e-04,\n",
      "        -5.1784e-04,  1.5555e-04,  0.0000e+00,  4.5819e-04,  2.3307e-04,\n",
      "        -9.8127e-06,  4.8248e-04,  1.9285e-04,  6.1431e-04,  1.0996e-04,\n",
      "         4.3843e-04, -1.9456e-04,  3.0028e-04, -6.2393e-04,  1.7666e-04,\n",
      "         4.8840e-05, -9.8943e-05, -2.6691e-04,  2.9154e-04,  4.8351e-05,\n",
      "         0.0000e+00, -3.0379e-04,  2.1196e-04, -7.2981e-05, -1.0885e-04,\n",
      "         5.6577e-04, -4.6547e-05,  0.0000e+00,  1.8737e-05,  4.5517e-04,\n",
      "         1.6970e-04,  4.8197e-04,  9.2024e-07, -2.7200e-04,  1.9019e-04,\n",
      "        -1.3587e-04,  1.8391e-04,  6.4035e-05,  0.0000e+00,  1.1708e-04,\n",
      "        -4.0193e-04,  4.3249e-05, -4.5437e-04,  0.0000e+00,  9.5445e-05,\n",
      "        -3.5104e-04, -3.1581e-04, -2.2769e-04, -2.3678e-04,  4.9289e-05,\n",
      "        -2.0360e-04,  1.4465e-04, -5.0088e-04,  1.5120e-05,  1.8047e-05,\n",
      "         0.0000e+00,  1.5755e-04,  1.1189e-04,  7.8530e-05,  1.4103e-04,\n",
      "         2.8826e-04,  2.5093e-05, -5.8255e-04, -1.3718e-04, -1.7541e-05,\n",
      "        -3.0552e-04, -2.0446e-04,  7.7120e-05, -2.6939e-04, -2.9164e-04,\n",
      "         0.0000e+00, -2.7693e-04,  3.0446e-04, -1.7628e-04, -2.8212e-04,\n",
      "         7.9924e-05,  4.4541e-04,  8.7818e-05,  9.2486e-05, -3.0860e-04,\n",
      "         2.6721e-04,  3.4918e-04,  8.5504e-05,  1.9438e-05, -3.1253e-04,\n",
      "         1.3340e-04, -3.4412e-04,  1.4216e-04, -8.8663e-06,  2.6447e-05,\n",
      "        -2.9027e-04, -6.2706e-05, -4.9462e-05, -2.3826e-04,  5.1865e-05,\n",
      "        -4.6610e-04, -3.6234e-04, -5.8573e-05,  5.7920e-06, -7.3099e-05,\n",
      "         0.0000e+00,  2.6582e-04,  6.2539e-05,  2.1513e-04,  1.4910e-04,\n",
      "        -5.2351e-04,  4.0811e-04, -4.2715e-05, -2.7826e-04,  5.6826e-08,\n",
      "         3.1157e-05,  1.0588e-05,  0.0000e+00,  1.2263e-05,  7.4957e-05,\n",
      "        -1.5111e-04, -1.2709e-04,  7.1535e-06, -5.0477e-05, -1.8888e-04,\n",
      "        -2.1365e-04, -3.0458e-04, -1.3338e-04, -3.6290e-04, -3.0415e-04,\n",
      "        -3.6149e-04,  3.1985e-04,  1.1618e-04, -2.9465e-04,  1.4037e-04,\n",
      "        -3.1855e-04,  1.6132e-04, -5.9235e-06, -1.1497e-04,  4.4715e-05,\n",
      "         1.2264e-04, -4.7417e-05,  8.9501e-06, -3.5982e-04,  6.4040e-04,\n",
      "        -5.4653e-05,  1.1567e-04, -2.9107e-04,  3.2053e-06, -3.9975e-04,\n",
      "         4.4495e-06,  3.2520e-04, -1.9224e-05,  2.7098e-04,  2.5350e-06,\n",
      "        -4.2433e-05, -4.4808e-04, -1.8247e-04, -1.2770e-04, -4.7789e-05,\n",
      "         4.4326e-05,  1.7581e-04, -4.5707e-05,  2.9109e-04,  6.1849e-04,\n",
      "        -2.3296e-04, -2.9698e-04, -4.2580e-05, -2.2353e-05,  1.1898e-04,\n",
      "         3.5661e-07,  8.3526e-06, -4.0575e-04, -2.3970e-04,  4.7595e-04,\n",
      "        -2.0384e-05, -3.5816e-05, -4.0214e-05, -1.7466e-06, -1.3344e-04,\n",
      "        -7.6644e-07,  0.0000e+00, -1.0522e-04, -2.4717e-04, -8.3097e-05,\n",
      "        -3.6658e-04, -5.1823e-05, -1.8896e-04, -1.3168e-04,  7.4608e-05,\n",
      "         1.3884e-04,  1.3617e-04, -4.0213e-04,  1.4740e-04, -1.8845e-04,\n",
      "        -2.5553e-04, -1.3392e-04, -7.8715e-05, -2.0169e-04,  2.0389e-04,\n",
      "         1.0745e-04,  1.0923e-05,  0.0000e+00, -1.5232e-04,  5.0262e-04,\n",
      "        -4.1648e-04, -7.8224e-05,  1.6197e-04,  9.5042e-07,  2.4891e-04,\n",
      "        -2.1935e-04,  1.9529e-05, -1.8443e-04, -3.0636e-04,  0.0000e+00,\n",
      "        -4.8717e-04, -4.5042e-04,  1.9813e-04,  3.3586e-04, -4.9150e-05,\n",
      "        -2.1423e-04, -3.3209e-04,  1.7453e-04,  3.7811e-06, -2.8831e-04,\n",
      "         0.0000e+00,  1.3689e-04,  3.5108e-05, -1.3074e-04,  7.5690e-05,\n",
      "        -3.9188e-04, -7.1364e-04,  1.9885e-04,  1.0029e-04, -2.2138e-06,\n",
      "        -7.5225e-05,  4.7056e-04, -5.7909e-06, -3.2262e-04, -1.9832e-04,\n",
      "         5.8146e-04, -1.7109e-04, -2.5207e-04, -1.8106e-04,  2.3855e-04,\n",
      "        -5.1974e-06,  1.1647e-04,  1.6356e-04,  2.3618e-04,  3.8354e-05,\n",
      "        -1.6677e-04,  0.0000e+00, -2.3044e-05, -6.9987e-04,  1.7843e-04,\n",
      "        -4.4207e-05,  0.0000e+00,  3.4327e-04, -2.6070e-04,  4.5598e-05,\n",
      "         3.9492e-05,  5.0179e-07,  3.4744e-04, -2.4578e-04, -8.7002e-05,\n",
      "         4.5188e-04,  2.3570e-05, -2.2727e-04, -1.9840e-04, -9.5640e-05,\n",
      "         1.1505e-04,  2.0269e-04,  4.2127e-04, -9.0199e-05, -1.7875e-04,\n",
      "         4.0791e-05, -2.5221e-04,  0.0000e+00,  1.2505e-04,  1.3328e-05,\n",
      "        -2.2104e-04, -1.2543e-04,  5.8754e-04, -1.8354e-04, -1.1762e-04,\n",
      "        -4.3023e-04, -3.0394e-04, -1.4731e-04,  3.1800e-04, -1.3794e-04,\n",
      "         3.3924e-06,  1.2109e-04, -6.0521e-06, -3.2254e-04, -3.3784e-04,\n",
      "        -5.2416e-04,  5.2965e-05, -5.1075e-05, -1.1473e-04, -7.1578e-05,\n",
      "         7.8048e-05,  6.7303e-04,  1.4076e-04,  9.6426e-05, -2.9798e-04,\n",
      "         4.3350e-04, -5.8617e-06,  1.6121e-04,  4.9377e-04,  4.9727e-05,\n",
      "         5.3278e-04,  9.9482e-05,  8.5592e-05, -4.0306e-04, -9.8857e-05,\n",
      "        -5.5174e-04, -3.1141e-04,  2.3494e-05, -2.2132e-04,  8.7378e-05,\n",
      "         2.4391e-05, -1.4620e-04, -2.9874e-05, -4.3681e-04,  2.1552e-04,\n",
      "        -8.9876e-05, -1.0870e-05, -3.7584e-04,  4.5148e-06,  6.3253e-04,\n",
      "        -8.1492e-05, -4.0161e-04])\n",
      "\n",
      "<TBackward object at 0x7fb42b7838d0>\n",
      "<AccumulateGrad object at 0x7fb42b07c6d0>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 0.0334,  0.2219, -0.2279,  0.2220],\n",
      "        [-0.2126,  0.2043, -0.4689, -0.3844],\n",
      "        [ 0.0026,  0.3490,  0.2014,  0.1585],\n",
      "        ...,\n",
      "        [ 0.4611, -0.0972, -0.4930,  0.0941],\n",
      "        [-0.3430,  0.1084,  0.1867, -0.1263],\n",
      "        [ 0.3362, -0.0040,  0.4016,  0.2253]], requires_grad=True)\n",
      " - gradient: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-4.3247e-06, -9.0059e-05,  1.4339e-05,  1.5793e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-2.3300e-05, -7.8499e-05,  3.7820e-06,  7.0772e-05],\n",
      "        [-8.5251e-06, -1.0742e-04,  1.2847e-05,  1.6789e-04],\n",
      "        [ 3.4786e-05,  2.5800e-04, -2.9930e-05, -3.7549e-04]])\n",
      "\n",
      "<TBackward object at 0x7fb42b783a10>\n",
      "<AccumulateGrad object at 0x7fb42b07cf10>\n",
      "Tensor with grad found: Parameter containing:\n",
      "tensor([[ 8.3133e-03,  2.3870e-02, -1.7410e-03,  1.6247e-02, -1.8780e-02,\n",
      "          4.3046e-03,  1.9249e-04,  2.9407e-02, -7.4609e-03,  4.0867e-02,\n",
      "          2.3793e-02, -3.9743e-02, -3.9386e-02, -1.4926e-02, -2.9600e-02,\n",
      "          2.0926e-02, -3.2042e-02, -2.8557e-02,  1.9179e-02,  2.5624e-02,\n",
      "          3.9430e-02, -1.1768e-02,  1.3788e-02, -1.0493e-02,  1.8685e-02,\n",
      "          5.9311e-03, -2.5441e-02,  2.6828e-02,  3.1856e-02, -3.9790e-02,\n",
      "          2.6335e-02, -3.6903e-02, -2.0065e-02,  4.0948e-02,  3.8803e-02,\n",
      "         -2.6629e-02,  5.5742e-03,  1.9270e-02,  3.8596e-03, -3.8876e-02,\n",
      "          3.0742e-02,  2.1875e-02, -6.8740e-03, -1.3846e-02,  4.4296e-03,\n",
      "          2.3423e-02, -2.5509e-02,  1.7987e-02, -3.6001e-02,  6.8463e-05,\n",
      "         -3.7710e-03, -2.8569e-02,  4.4640e-04, -1.6461e-02,  5.7146e-03,\n",
      "         -3.6528e-02,  1.0060e-02, -3.4358e-03, -8.6276e-03,  3.5305e-02,\n",
      "          2.2786e-02,  3.5709e-02, -3.7606e-02, -3.5867e-02,  5.7100e-03,\n",
      "         -3.8101e-02,  1.5099e-02,  2.1092e-02, -4.2749e-02, -3.4805e-02,\n",
      "          1.1902e-02, -1.0816e-02,  3.3338e-03, -7.4277e-03, -3.6791e-02,\n",
      "          2.3018e-02,  4.0040e-02, -3.0920e-02, -2.7874e-02,  2.3928e-03,\n",
      "          2.9045e-02,  1.1625e-02,  2.5804e-03, -3.0549e-03,  1.8736e-03,\n",
      "         -2.0708e-02, -1.0801e-02,  1.0392e-02,  4.3446e-02, -2.1090e-02,\n",
      "          3.3501e-04, -3.6235e-02,  1.4438e-02,  2.9105e-03,  1.1881e-02,\n",
      "         -2.7603e-02,  3.1988e-02,  1.6047e-02, -3.6172e-02, -4.1493e-02,\n",
      "          1.2261e-02, -3.6599e-02,  6.2108e-03,  3.8991e-03,  2.7832e-02,\n",
      "         -1.9115e-03, -1.7475e-02, -4.3155e-03, -2.1708e-02, -1.5241e-02,\n",
      "         -2.1648e-02,  2.7206e-02,  1.7720e-02, -4.2270e-02,  4.4842e-03,\n",
      "         -4.1948e-02,  4.1091e-02, -1.6701e-02,  2.5076e-02, -1.3948e-02,\n",
      "         -2.9271e-02, -2.2067e-02,  3.2425e-02,  3.4506e-02, -5.8573e-03,\n",
      "         -1.2109e-02,  8.0728e-03, -9.4918e-03,  1.5993e-02, -3.8654e-03,\n",
      "         -3.6183e-02, -1.6512e-02,  3.6357e-02,  1.3180e-02,  1.0020e-02,\n",
      "         -4.4107e-02,  3.9536e-02, -1.8489e-02,  6.3828e-03, -2.4071e-02,\n",
      "          3.5639e-02, -2.2124e-02, -3.0961e-02,  4.0616e-02, -3.6024e-02,\n",
      "         -9.5930e-03, -1.4926e-02,  4.3249e-02, -4.1628e-04,  1.7360e-02,\n",
      "         -2.8218e-02, -2.9005e-02, -3.6531e-02,  2.7486e-03,  1.1269e-02,\n",
      "         -2.6883e-02,  2.7242e-02,  2.4028e-02, -2.9019e-02, -2.0149e-02,\n",
      "          2.3395e-02,  4.1293e-02,  2.8375e-02,  1.9856e-02,  3.9653e-02,\n",
      "          3.8596e-02, -3.5279e-02,  1.0242e-02,  2.2905e-02,  8.1920e-03,\n",
      "         -6.8469e-03, -3.5488e-02,  2.4566e-02,  1.7816e-02, -6.7826e-03,\n",
      "          3.9529e-02,  1.7051e-02,  4.2080e-02, -1.6550e-02,  2.6052e-02,\n",
      "         -3.6500e-02,  2.4145e-02,  6.9115e-03, -5.6836e-03, -2.5239e-02,\n",
      "          3.8494e-02, -2.7637e-02,  2.4212e-02, -3.5891e-02, -2.8642e-02,\n",
      "          3.5005e-02, -3.9993e-02, -1.0954e-02, -4.2516e-02, -3.5269e-02,\n",
      "         -3.8679e-02,  1.8067e-02, -3.5454e-02,  3.9172e-02, -2.0981e-02,\n",
      "         -1.9141e-02,  4.1839e-02,  3.9513e-02, -2.3045e-02, -3.7414e-03,\n",
      "         -5.2258e-03,  3.8641e-02, -1.3616e-02,  3.6347e-02,  7.6815e-03,\n",
      "         -4.1598e-02,  1.3890e-02, -3.6947e-02, -6.5302e-04, -4.0272e-02,\n",
      "         -2.5722e-02, -2.9197e-02, -8.8112e-03,  2.4236e-02, -1.1148e-02,\n",
      "          1.7590e-02, -4.1557e-02, -2.2420e-02,  1.3490e-02, -2.6922e-02,\n",
      "          2.5545e-02, -3.4186e-02,  2.8566e-02,  3.9017e-02, -5.8828e-03,\n",
      "          4.1679e-02,  3.0937e-02,  1.4770e-02,  1.5299e-02, -2.5591e-03,\n",
      "          3.0970e-02, -8.3573e-03,  4.1402e-02, -1.8086e-03, -3.2572e-03,\n",
      "          1.3440e-02, -2.7908e-02, -2.5144e-02, -4.2564e-02, -1.7587e-02,\n",
      "         -1.7372e-02, -3.0591e-02,  3.6372e-02,  1.0458e-02,  4.1433e-02,\n",
      "          2.5751e-02,  2.5904e-02, -8.4382e-03,  3.0758e-02,  2.6317e-02,\n",
      "          2.2165e-02,  4.1030e-02, -2.0865e-02,  3.9783e-02,  3.9315e-02,\n",
      "         -2.6750e-02, -2.6401e-02, -1.0029e-02, -2.0699e-02,  2.6673e-02,\n",
      "         -2.1624e-02, -4.2269e-02, -3.2381e-02, -2.5105e-02,  3.8472e-02,\n",
      "         -9.2368e-03,  4.3636e-02, -8.6388e-03,  3.3752e-03, -7.5349e-04,\n",
      "          2.2681e-02,  8.4539e-03,  9.1779e-03,  1.8722e-02, -1.2041e-02,\n",
      "          2.9965e-02,  4.4772e-02,  5.3070e-03, -7.7763e-04,  1.5990e-02,\n",
      "         -4.3053e-02, -1.6913e-02, -2.3891e-02, -3.8466e-02, -9.7464e-03,\n",
      "          3.5426e-02, -3.4663e-02,  2.7859e-02,  1.8358e-02,  8.2676e-04,\n",
      "         -1.6187e-03, -3.9111e-04, -1.8028e-02, -1.9825e-02, -2.3833e-02,\n",
      "          1.8475e-02,  1.5856e-02, -1.5641e-02,  6.4871e-03,  1.7730e-02,\n",
      "          1.5398e-02,  3.8028e-02,  4.3942e-02,  4.4678e-02,  3.5661e-02,\n",
      "          2.5859e-02, -2.7767e-02, -9.8175e-03,  2.6214e-02, -3.6429e-02,\n",
      "          3.9443e-02, -3.6508e-02,  2.4801e-03,  1.4870e-02, -1.0788e-02,\n",
      "         -2.2897e-02,  1.0219e-02, -1.2677e-02,  3.5034e-02, -3.8174e-02,\n",
      "          1.9863e-02, -2.0381e-02,  3.5370e-02,  3.3707e-04,  2.6747e-02,\n",
      "          4.5286e-05, -3.9065e-02,  2.4543e-03, -3.9658e-02, -5.1605e-03,\n",
      "          6.0516e-03,  3.0801e-02,  1.8422e-02,  3.2991e-02,  1.4164e-02,\n",
      "         -5.1804e-03, -1.0036e-02,  1.1077e-02, -3.6946e-02, -3.7401e-02,\n",
      "          4.4452e-02,  3.9847e-02,  6.1873e-03,  2.3704e-02, -9.1122e-03,\n",
      "          8.4699e-04, -6.6594e-03,  4.1089e-02,  4.4567e-02, -3.8254e-02,\n",
      "          4.5592e-03,  3.9047e-03,  4.5061e-03,  1.0507e-03,  1.1199e-02,\n",
      "          5.3674e-03, -4.2740e-02,  9.8230e-03,  1.8338e-02,  2.0674e-02,\n",
      "          2.3602e-02,  3.5649e-02,  2.3740e-02,  4.1267e-02, -7.8737e-03,\n",
      "         -7.8889e-03, -7.6858e-03,  3.6340e-02, -2.5366e-02,  4.0916e-02,\n",
      "          2.0342e-02,  3.8949e-02,  5.6074e-03,  2.6829e-02, -3.1621e-02,\n",
      "         -1.5534e-02, -2.5751e-02,  2.8396e-02,  1.9284e-02, -3.5189e-02,\n",
      "          3.8547e-02,  2.8077e-02, -1.6932e-02,  2.2797e-04, -4.1447e-02,\n",
      "          4.1921e-02, -5.7829e-03,  1.5981e-02,  4.4189e-02,  2.3756e-02,\n",
      "          3.6850e-02,  3.5813e-02, -3.7413e-02, -2.7683e-02,  1.7452e-02,\n",
      "          4.0790e-02,  2.5735e-02, -2.0861e-02, -6.6648e-03,  2.2968e-02,\n",
      "          1.1035e-02, -1.7661e-02, -4.2830e-02,  1.6190e-02, -4.5056e-03,\n",
      "          2.5606e-02,  4.4383e-02, -3.8653e-02, -3.3756e-02,  3.5525e-03,\n",
      "          2.4029e-02, -4.1811e-02,  1.3909e-03,  4.2338e-02,  1.3639e-02,\n",
      "         -3.5117e-02,  1.4447e-02,  2.5891e-02,  1.4952e-02, -2.5808e-02,\n",
      "          3.7076e-02, -1.1334e-02, -3.7328e-02, -2.7785e-02, -1.6789e-02,\n",
      "          1.1035e-02, -1.7089e-02,  6.7129e-03,  4.3510e-02, -1.1113e-02,\n",
      "          3.9497e-02, -3.1794e-02, -3.6520e-02,  2.3316e-02, -2.8536e-03,\n",
      "         -3.8169e-03, -3.3275e-02, -3.1925e-02,  1.8281e-02,  1.2003e-02,\n",
      "         -3.1788e-02, -9.4014e-04,  2.7909e-02,  1.5653e-02,  3.2920e-02,\n",
      "         -1.9383e-02, -2.1220e-02, -3.2154e-02,  1.3629e-02,  4.1393e-02,\n",
      "         -2.9387e-03,  1.6260e-02,  3.1861e-02, -7.2271e-03, -3.8997e-03,\n",
      "          2.8821e-02,  2.4244e-02, -3.6975e-02,  3.1072e-02,  2.2211e-02,\n",
      "          2.8971e-02,  3.8096e-02,  2.8207e-02, -1.8925e-02,  2.0766e-02,\n",
      "         -1.3691e-02, -1.2924e-02,  1.7633e-03,  4.0588e-02,  3.2501e-02,\n",
      "          3.7002e-02, -1.4476e-02,  1.5218e-02,  3.3128e-02,  2.2836e-02,\n",
      "         -2.3698e-02, -4.1256e-02, -3.6583e-02, -1.5244e-02,  3.8219e-02,\n",
      "         -2.8145e-02,  1.3453e-03, -1.4888e-02, -2.9298e-02, -2.5052e-02,\n",
      "         -3.7089e-02, -1.9612e-02, -1.5487e-02,  2.5460e-02,  9.0069e-03,\n",
      "          3.5628e-02,  3.7263e-02, -2.1767e-03,  1.8609e-02, -1.9489e-02,\n",
      "         -3.5871e-02,  3.3983e-02,  1.0104e-02,  2.7516e-02, -1.3584e-02,\n",
      "          3.3788e-02,  4.0267e-03,  3.5308e-02, -3.1544e-02, -3.7718e-02,\n",
      "          3.4471e-02,  4.1138e-02]], requires_grad=True)\n",
      " - gradient: tensor([[ 0.0000e+00, -6.6689e-03,  0.0000e+00, -1.7506e-03, -2.9757e-03,\n",
      "         -5.7698e-03, -7.0924e-03, -6.1652e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -1.2951e-03, -2.2861e-03, -7.6645e-03, -1.0570e-02, -1.2936e-04,\n",
      "         -4.6559e-04, -7.6204e-03, -7.6414e-03, -1.1622e-03, -6.6841e-04,\n",
      "         -1.1205e-03, -3.5441e-03, -1.6421e-03, -3.6071e-05, -8.8398e-04,\n",
      "         -2.7498e-03, -7.1178e-03, -1.2187e-03, -4.1270e-03,  0.0000e+00,\n",
      "         -6.2660e-03, -3.7740e-03, -5.4157e-03, -1.1454e-03, -7.7354e-03,\n",
      "         -5.9159e-03, -2.5817e-03, -4.5757e-03, -4.2020e-03, -2.2797e-03,\n",
      "         -9.4526e-04, -3.1141e-03, -4.9688e-05, -8.4427e-04, -5.4857e-03,\n",
      "         -6.2787e-03, -7.0482e-03, -6.7956e-03, -8.6574e-04, -7.0932e-03,\n",
      "         -4.8813e-04, -1.1949e-03, -2.1673e-03,  0.0000e+00, -2.8517e-03,\n",
      "         -2.9115e-03, -4.5057e-03, -3.7887e-03, -2.9961e-05, -8.6586e-03,\n",
      "         -5.8935e-04, -8.6305e-03, -8.4607e-03, -1.0534e-04, -2.7131e-03,\n",
      "         -2.9618e-03, -8.0688e-03, -1.3673e-03, -1.7826e-03, -1.0526e-03,\n",
      "         -3.0422e-04, -1.5717e-03, -1.0501e-03, -5.7669e-03,  0.0000e+00,\n",
      "         -9.8531e-03, -2.1059e-03, -6.6818e-03, -3.0381e-04, -1.7461e-03,\n",
      "         -7.9574e-05, -4.3816e-03, -4.6351e-04, -1.7902e-03, -1.0761e-04,\n",
      "         -3.9123e-03, -1.1156e-03, -5.1969e-03, -8.3843e-03, -6.4714e-03,\n",
      "         -1.9788e-03, -3.1605e-03, -8.3189e-03, -4.7295e-03, -3.0217e-05,\n",
      "         -4.6317e-03, -2.5864e-03, -1.6947e-03, -1.2883e-04, -4.8048e-03,\n",
      "         -7.8813e-03, -2.6447e-06, -6.7147e-03, -5.5855e-03,  0.0000e+00,\n",
      "         -7.3945e-03, -6.0227e-04, -3.8602e-03, -5.8876e-03, -4.3242e-03,\n",
      "         -6.0470e-03, -5.9943e-03, -4.5343e-03, -7.6655e-03, -3.5121e-03,\n",
      "         -8.2441e-03, -2.9622e-03, -3.7216e-03, -7.4633e-04, -3.0351e-03,\n",
      "         -2.4465e-05, -3.2533e-03, -3.5290e-03, -7.4640e-03, -4.7865e-03,\n",
      "         -5.9427e-03, -7.5527e-04, -1.4626e-04, -7.5901e-03, -4.2903e-03,\n",
      "         -5.3924e-03, -3.8229e-03, -3.5154e-03, -4.9845e-04, -7.5021e-03,\n",
      "         -2.2795e-05, -4.2200e-03, -1.0564e-02, -2.6140e-03, -4.7700e-05,\n",
      "          0.0000e+00, -2.1525e-03, -2.3882e-03, -3.3006e-03, -7.9592e-03,\n",
      "         -3.3553e-03, -1.9920e-03,  0.0000e+00, -9.5527e-04, -2.2134e-03,\n",
      "         -4.3153e-03, -6.7439e-03, -1.2134e-03, -1.0074e-02, -5.2717e-05,\n",
      "         -2.6769e-05, -1.8084e-03, -3.0496e-03, -1.5736e-05, -7.7572e-03,\n",
      "         -2.2705e-04, -7.9112e-05, -2.4352e-03, -7.7571e-03, -2.3255e-03,\n",
      "         -3.6115e-03, -1.2588e-03, -1.2878e-03, -8.0897e-03, -2.9291e-03,\n",
      "         -2.8192e-03, -4.0512e-03, -7.7034e-03, -7.9400e-04, -4.9354e-03,\n",
      "          0.0000e+00, -6.0395e-04, -3.8006e-03, -7.3970e-03, -1.4817e-03,\n",
      "         -3.4330e-03, -4.7512e-03, -3.1484e-03, -1.7208e-03, -4.6807e-03,\n",
      "         -2.3441e-03, -2.6852e-03,  0.0000e+00, -7.9943e-03, -1.4089e-03,\n",
      "         -4.4531e-06, -1.3729e-03, -4.1139e-03, -6.1221e-03, -9.0021e-04,\n",
      "         -6.7399e-03, -8.9344e-03, -3.6329e-03, -7.7496e-03, -2.3984e-03,\n",
      "         -3.4844e-04, -2.9807e-04, -3.3417e-03, -8.5836e-03, -5.7227e-03,\n",
      "          0.0000e+00, -2.2698e-03, -6.4662e-03, -4.7327e-04, -1.7053e-03,\n",
      "         -6.3158e-03, -1.3619e-03,  0.0000e+00, -1.2849e-03, -1.0043e-02,\n",
      "         -6.0911e-04, -6.6572e-03, -3.4426e-06, -4.6056e-03, -6.2315e-03,\n",
      "         -1.6223e-03, -1.4336e-03, -3.6061e-04,  0.0000e+00, -1.1913e-03,\n",
      "         -7.4673e-03, -9.1175e-05, -6.2605e-03,  0.0000e+00, -7.8472e-03,\n",
      "         -2.5899e-03, -5.1398e-03, -7.8781e-03, -6.2459e-03, -7.8668e-03,\n",
      "         -4.1039e-03, -3.5392e-03, -4.8231e-03, -2.2562e-03, -1.0481e-03,\n",
      "          0.0000e+00, -3.9966e-03, -9.8821e-04, -2.8673e-04, -2.0154e-03,\n",
      "         -4.7417e-03, -7.5456e-05, -5.7754e-03, -1.3697e-03, -1.4355e-05,\n",
      "         -6.5957e-03, -2.4220e-03, -2.4414e-03, -3.5657e-03, -8.6397e-03,\n",
      "          0.0000e+00, -4.9885e-03, -7.6428e-03, -1.7968e-03, -2.2813e-03,\n",
      "         -8.7040e-04, -3.1051e-03, -2.9382e-03, -9.9123e-04, -2.7629e-03,\n",
      "         -5.4435e-03, -3.1740e-03, -4.4072e-04, -1.2082e-04, -3.4469e-03,\n",
      "         -7.4114e-03, -1.8791e-03, -7.2917e-03, -9.7333e-04, -6.8894e-03,\n",
      "         -5.8253e-03, -2.6570e-03, -4.1681e-03, -5.2971e-03, -1.1728e-03,\n",
      "         -7.9342e-03, -7.2201e-03, -1.0252e-02, -1.2209e-03, -1.6293e-03,\n",
      "          0.0000e+00, -7.6887e-03, -5.4790e-04, -2.7459e-03, -7.7638e-03,\n",
      "         -1.6381e-03, -8.5978e-03, -1.5977e-04, -3.7099e-03, -4.9360e-03,\n",
      "         -9.5459e-03, -8.2180e-03,  0.0000e+00, -1.1251e-05, -1.1362e-03,\n",
      "         -8.7097e-04, -3.5782e-03, -2.0145e-05, -5.5805e-03, -5.8670e-03,\n",
      "         -7.6031e-03, -1.7633e-03, -1.4173e-03, -2.2412e-03, -2.6628e-03,\n",
      "         -4.8064e-03, -4.2315e-03, -4.8356e-03, -5.7144e-03, -9.0852e-04,\n",
      "         -5.8881e-03, -8.5726e-04, -1.2455e-03, -3.2333e-03, -1.2287e-03,\n",
      "         -1.8578e-03, -1.4310e-03, -4.0025e-05, -2.8360e-03, -6.8409e-03,\n",
      "         -4.7822e-04, -2.2891e-03, -2.3129e-03, -2.7323e-03, -4.5129e-03,\n",
      "         -9.9946e-04, -5.9294e-03, -7.9775e-03, -1.0806e-03, -3.7578e-05,\n",
      "         -5.4629e-03, -1.9045e-03, -1.6532e-03, -1.8541e-03, -9.7802e-04,\n",
      "         -3.9551e-03, -3.5101e-03, -1.5163e-03, -2.7574e-03, -6.0432e-03,\n",
      "         -1.6806e-03, -2.5609e-03, -5.5737e-03, -1.4502e-04, -3.3864e-03,\n",
      "         -6.9209e-03, -1.7331e-04, -4.1039e-03, -1.5501e-03, -5.2418e-03,\n",
      "         -2.9644e-03, -1.8013e-03, -1.0527e-02, -1.2277e-03, -4.0883e-03,\n",
      "         -1.1848e-05,  0.0000e+00, -3.2651e-03, -2.1689e-03, -2.6765e-04,\n",
      "         -6.0287e-03, -1.3120e-04, -5.6286e-03, -4.8451e-04, -6.5451e-03,\n",
      "         -5.5378e-03, -8.0952e-03, -6.7076e-03, -4.6504e-03, -6.2810e-04,\n",
      "         -2.4868e-03, -1.8178e-03, -4.2086e-03, -2.8180e-03, -3.4826e-03,\n",
      "         -2.7731e-03, -4.2710e-05,  0.0000e+00, -4.9456e-03, -1.6150e-03,\n",
      "         -6.4493e-03, -5.7308e-04, -7.0542e-03, -1.1594e-03, -2.1151e-03,\n",
      "         -1.9080e-03, -1.2988e-03, -4.8909e-03, -1.3758e-03,  0.0000e+00,\n",
      "         -9.2009e-04, -9.9888e-03, -1.6976e-03, -7.5956e-03, -3.8905e-04,\n",
      "         -2.7539e-03, -4.2906e-03, -1.8311e-03, -4.5108e-05, -6.4505e-03,\n",
      "          0.0000e+00, -4.0405e-03, -7.1029e-05, -6.8490e-03, -8.2603e-03,\n",
      "         -6.2487e-03, -6.7268e-03, -1.6106e-03, -6.4881e-04, -3.9778e-05,\n",
      "         -1.7848e-03, -3.2176e-03, -6.6073e-03, -1.5173e-03, -2.7468e-03,\n",
      "         -6.8246e-03, -2.3807e-03, -6.9985e-03, -3.5321e-03, -2.8017e-03,\n",
      "         -6.0512e-06, -2.6855e-03, -8.1052e-04, -4.7484e-03, -2.2716e-04,\n",
      "         -1.0017e-03,  0.0000e+00, -7.5967e-04, -5.1551e-03, -8.5325e-03,\n",
      "         -2.0467e-04,  0.0000e+00, -9.4160e-04, -6.4759e-03, -3.1341e-03,\n",
      "         -3.5323e-04, -4.6544e-07, -3.0204e-03, -3.3211e-03, -2.9730e-03,\n",
      "         -4.5366e-03, -9.6730e-03, -6.0516e-04, -7.9566e-03, -5.4686e-04,\n",
      "         -1.2746e-03, -2.6498e-03, -5.5773e-03, -1.1216e-03, -9.2211e-04,\n",
      "         -2.8602e-03, -7.4646e-03,  0.0000e+00, -2.0053e-03, -7.6198e-04,\n",
      "         -2.5615e-03, -3.7904e-03, -1.1504e-03, -2.6578e-03, -3.5288e-03,\n",
      "         -5.7562e-03, -3.6388e-03, -1.9642e-03, -4.0344e-03, -1.6702e-03,\n",
      "         -1.5541e-05, -4.3168e-03, -1.3149e-03, -5.3020e-03, -2.6479e-03,\n",
      "         -2.4075e-03, -2.0652e-03, -1.5579e-03, -1.5974e-03, -1.3888e-03,\n",
      "         -1.3153e-03, -5.7283e-03, -6.0062e-04, -2.3063e-03, -5.8136e-03,\n",
      "         -6.6539e-03, -6.0575e-03, -1.6331e-03, -4.2216e-03, -5.5118e-04,\n",
      "         -9.3930e-03, -1.3306e-03, -1.1270e-03, -7.6293e-03, -4.4741e-03,\n",
      "         -7.2748e-03, -3.5085e-03, -2.5539e-03, -5.4046e-03, -1.1526e-03,\n",
      "         -5.5846e-05, -9.1530e-04, -6.4998e-04, -7.5347e-03, -2.4005e-03,\n",
      "         -4.0900e-04, -9.7485e-04, -6.5619e-03, -3.9067e-06, -5.5806e-03,\n",
      "         -2.5090e-04, -2.3551e-03]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=AsyncExperienceSourceDataBunch.from_env('CartPole-v1',bs=128,n_processes=4,firstlast=True,ds_kwargs={'n_envs':15,'skip_n_steps':4},num_workers=12)\n",
    "model=LinearA2C((4,),2)\n",
    "agent=ActorCriticAgent(model=model)\n",
    "learn=A3CLearner(data,model,agent=agent,callback_fns=[A3CTrainer,RewardMetric,NGamesMetric])\n",
    "learn.fit(1,lr=0.001,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 02_callbacks.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted 05_data_block.ipynb.\n",
      "Converted 06_basic_train.ipynb.\n",
      "Converted 12_a3c.a3c_data.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/12_a3c.a3c_data.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
