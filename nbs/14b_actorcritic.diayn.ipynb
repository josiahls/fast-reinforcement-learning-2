{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.diayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastrl.ptan_extension import *\n",
    "from fastrl.actorcritic.sac import *\n",
    "\n",
    "from torch.distributions import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIAYN\n",
    "\n",
    "> Diversity Is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DIAYN` extends SAC to create *Skills* that can be used for a single or multiple environments. \n",
    "[(Eysenbach et al. 2018) [DIAYN] Diversity Is All You Need](https://arxiv.org/pdf/1802.06070.pdf) covers this in detail.\n",
    "\n",
    "    The general idea is that Skills should each be as diverse as possible and should not be   tied to a reward function specific to an environment.\n",
    "\n",
    "Their [project site](https://sites.google.com/view/diayn) shows several *incredible* examples of `DIAYN` finding *Skills* without any reward. The original implementation is in tensorflow and ca be found [here](https://github.com/haarnoja/sac/blob/master/sac/algos/diayn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Discriminator(Module):\n",
    "    \"`Module` for storing skills. Receives input (`num_inputs`+`num_actions`) -> `num_skills`.\"\n",
    "    def __init__(self, num_inputs,num_actions,num_skills,hidden_dim):\n",
    "        self.linear1 = nn.Linear(num_inputs+num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim,num_skills)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(SAC)\n",
    "class DIAYN(SAC):\n",
    "    def __init__(self,num_inputs,action_space,discriminator:Module=None,num_skills:int=20,\n",
    "                 find_best_skill_interval:int=10,scale_entropy:float=1,\n",
    "                 best_skill_n_rollouts:int=10,include_actions:bool=False,\n",
    "                 learn_p_z:bool=False,add_p_z:bool=True,hidden_size=100,**kwargs):\n",
    "        store_attr()\n",
    "        self.num_inputs=num_inputs+self.num_skills\n",
    "        self.original_num_inputs=num_inputs\n",
    "        self.p_z=np.full(self.num_skills,1.0/self.num_skills)\n",
    "        self.discriminator=Discriminator(self.original_num_inputs,action_space.shape[0],\n",
    "                                         num_skills,hidden_size)\n",
    "        self.log_p_z_episode=[]\n",
    "        self.z=0\n",
    "        self.reset_z()\n",
    "        super().__init__(self.num_inputs,action_space,hidden_size=hidden_size,**kwargs)\n",
    "        \n",
    "    def sample_z(self):\n",
    "        \"\"\"Samples z from p(z), using probabilities in self._p_z.\"\"\"\n",
    "        return np.random.choice(self.num_skills,p=self.p_z)\n",
    "    \n",
    "    def reset_z(self): self.z=self.sample_z()\n",
    "    def __call__(self,s,asl):\n",
    "        aug_s=self.concat_obs_z(s,self.z)\n",
    "        return super().__call__(aug_s,asl)\n",
    "    \n",
    "    def concat_obs_z(self,obs,z):\n",
    "        \"\"\"Concatenates the observation to a one-hot encoding of Z.\"\"\"\n",
    "        assert np.isscalar(z)\n",
    "        if type(obs)==list and len(obs)==1: obs=obs[0]\n",
    "        if len(obs.shape)==2 and obs.shape[0]==1: obs=obs[0]\n",
    "            \n",
    "        z_one_hot=np.zeros(self.num_skills)\n",
    "        z_one_hot[z]=1\n",
    "        return torch.FloatTensor(np.hstack([obs,z_one_hot])).reshape(1,-1)\n",
    "    \n",
    "    def update_parameters(self, *yb, learn):\n",
    "        # Sample a batch from memory\n",
    "#         state_batch, action_batch, reward_batch, next_state_batch, mask_batch = learn.memory.sample(batch_size=batch_size)\n",
    "        batch=learn.sample_yb\n",
    "#         print(batch[0])\n",
    "#         pprint(batch)\n",
    "        state_batch=torch.stack([o.state.to(device=default_device()) for o in batch]).float()\n",
    "        next_state_batch=torch.stack([o.last_state.to(device=default_device()) for o in batch]).float()\n",
    "        action_batch=torch.stack([o.action.to(device=default_device()) for o in batch]).float()\n",
    "        reward_batch=torch.stack([o.reward.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        mask_batch=torch.stack([o.done.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        \n",
    "#         print(state_batch.shape,next_state_batch.shape,action_batch.shape,reward_batch.shape,mask_batch.shape)\n",
    "#         state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "#         next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "#         action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "#         reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "#         mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + (1-mask_batch) * self.gamma * (min_qf_next_target)\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # JœÄ = ùîºst‚àºD,Œµt‚àºN[Œ± * logœÄ(f(Œµt;st)|st) ‚àí Q(st,f(Œµt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
    "\n",
    "\n",
    "        if self.updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        self.updates+=1\n",
    "#         print(self.updates)\n",
    "#         print('complete')\n",
    "        return qf1_loss+ qf2_loss+ policy_loss+ alpha_loss+ alpha_tlogs\n",
    "    \n",
    "#     def update_parameters(self, *yb, learn):pass\n",
    "\n",
    "DIAYN.__doc__=\"\"\"\n",
    "`discriminator` is an additional `Module` to calculate z.\n",
    "`num_skills` is the number of skills/options to learn.\n",
    "`find_best_skill_interval` is how often to recompute the best skill.\n",
    "When finding the best skill, `best_skill_n_rollouts` determines how many rollouts to \n",
    "do per skill.\n",
    "`include_actions` determines whether to pass actions to the discriminator.\n",
    "`add_p_z` determines whether to include $\\log{p(z)}$ in the pseudo-reward.\n",
    "`scale_entropy` is the scaling factor for entropy.\n",
    "\n",
    "A few explainations of some of the internal fields:\n",
    "\n",
    "We now have `num_inputs` and `original_num_inputs`. `num_inputs` has the `num_skills` being\n",
    "added to it. This will then be used by the `SAC` parent in initializing the critic and actor.\n",
    "\n",
    "`original_num_inputs` will only be used by the `discriminator` now.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"DIAYN\" class=\"doc_header\"><code>class</code> <code>DIAYN</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>DIAYN</code>(**`num_inputs`**, **`action_space`**, **`discriminator`**:`Module`=*`None`*, **`num_skills`**:`int`=*`20`*, **`find_best_skill_interval`**:`int`=*`10`*, **`scale_entropy`**:`float`=*`1`*, **`best_skill_n_rollouts`**:`int`=*`10`*, **`include_actions`**:`bool`=*`False`*, **`learn_p_z`**:`bool`=*`False`*, **`add_p_z`**:`bool`=*`True`*, **`hidden_size`**=*`100`*, **`gamma`**=*`0.99`*, **`tau`**=*`0.005`*, **`alpha`**=*`0.2`*, **`policy`**=*`'gaussian'`*, **`automatic_entropy_tuning`**=*`True`*, **`target_update_interval`**=*`1`*, **`lr`**=*`0.0003`*) :: [`SAC`](/fast-reinforcement-learning-2/actorcritic.sac.html#SAC)\n",
       "\n",
       "`discriminator` is an additional `Module` to calculate z.\n",
       "`num_skills` is the number of skills/options to learn.\n",
       "`find_best_skill_interval` is how often to recompute the best skill.\n",
       "When finding the best skill, `best_skill_n_rollouts` determines how many rollouts to \n",
       "do per skill.\n",
       "`include_actions` determines whether to pass actions to the discriminator.\n",
       "`add_p_z` determines whether to include $\\log{p(z)}$ in the pseudo-reward.\n",
       "`scale_entropy` is the scaling factor for entropy.\n",
       "\n",
       "A few explainations of some of the internal fields:\n",
       "\n",
       "We now have `num_inputs` and `original_num_inputs`. `num_inputs` has the `num_skills` being\n",
       "added to it. This will then be used by the [`SAC`](/fast-reinforcement-learning-2/actorcritic.sac.html#SAC) parent in initializing the critic and actor.\n",
       "\n",
       "`original_num_inputs` will only be used by the `discriminator` now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DIAYN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DiscriminatorTrainer(ExperienceReplay):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.log_p_z_episode=[]\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.agent.warming_up=True\n",
    "        while len(self.queue)<self.starting_els:\n",
    "            for i,o in enumerate(self.dls.train):\n",
    "                z=self.learn.agent.z\n",
    "                batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(o[0][i],z)[0],\n",
    "                                           action=o[1][i],\n",
    "                                           reward=o[2][i],\n",
    "                                           last_state=self.learn.agent.concat_obs_z(o[3][i],z)[0], \n",
    "                                           done=(o[4][i] and self.max_steps!=o[6][i]),\n",
    "                                           episode_reward=o[5][i],steps=o[6][i])\n",
    "                                    for i in range(len(o[0]))]\n",
    "#                 print(self.max_steps,max([o.steps for o in batch]))\n",
    "                for _b in batch:self.queue.append(_b)\n",
    "                if any([_b.done for _b in batch]): self.learn.agent.reset_z()\n",
    "                if len(self.queue)>self.starting_els:break\n",
    "        self.learn.agent.warming_up=False\n",
    "\n",
    "# #     def after_epoch(self):\n",
    "# #         print(len(self.queue))\n",
    "    def before_batch(self):\n",
    "#         print(len(self.queue))\n",
    "        b=list(self.learn.xb)+list(self.learn.yb)\n",
    "        z=self.learn.agent.z\n",
    "        batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(b[0][i],z)[0],\n",
    "                                   action=b[1][i],\n",
    "                                   reward=b[2][i],\n",
    "                                   last_state=self.learn.agent.concat_obs_z(b[3][i],z)[0], \n",
    "                                   done=(b[4][i] and self.max_steps!=b[6][i]),\n",
    "                                   episode_reward=b[5][i],steps=b[6][i])\n",
    "              for i in range(len(b[0]))]\n",
    "        \n",
    "#         print(self.learn.xb)\n",
    "        self.learn.xb=(torch.stack([e.state for e in batch]),)\n",
    "#         print(self.learn.yb)\n",
    "        self.learn.yb=(torch.stack([o.action for o in batch]),\n",
    "                       torch.stack([o.reward for o in batch]),\n",
    "                       torch.stack([o.last_state for o in batch]),\n",
    "                       torch.stack([o.done for o in batch]),\n",
    "                       torch.stack([o.episode_reward for o in batch]),\n",
    "                       torch.stack([o.steps for o in batch]))\n",
    "#         print(self.learn.yb)\n",
    "        \n",
    "        for _b in batch: self.queue.append(_b)\n",
    "        idxs=np.random.randint(0,len(self.queue), self.bs)\n",
    "        self.learn.sample_yb=[self.queue[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "add_docs(DiscriminatorTrainer,cls_doc=\"\"\"\n",
    "Subclasses ExperienceReplay for augmenting experience, and toggling the agent's skill thats,\n",
    "being used, and also does training of the discriminator.\"\"\",\n",
    "         before_fit=\"Similar to ExperienceReplay but augments the states and toggles the skill used.\",\n",
    "         before_batch=\"Similar situation as `before_fit`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional information about `DiscrimiatorTrainer`.\n",
    "\n",
    "As noted in [(Eysenbach et al. 2018)](https://arxiv.org/pdf/1802.06070.pdf) Algorithm 1, we need $log p(z)$. We accomplish this by getting the output from the discriminator $q_{\\phi}(z|s)$, taking the softmax which will scale $z$ to $[0,1]$ which is when is needed to prepresent probability $p$. Next, we scale the distribution by $log$ which is critical for calculating entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important notes from [(Eysenbach et al. 2018)](https://arxiv.org/pdf/1802.06070.pdf):\n",
    "- Hidden nn size is changed from 128 to 300 (pg 14)\n",
    "- Alpha is changed to 0.1 (pg 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.355922</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>None</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.361305</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>None</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-2.286275</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>None</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "from pybulletgym.envs import *\n",
    "\n",
    "env='InvertedPendulumPyBulletEnv-v0'\n",
    "agent=DIAYN(5,gym.make(env).action_space,gamma=0.99,tau=0.005,alpha=0.1,hidden_size=300)\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,exclude_nones=True,\n",
    "                               dls_kwargs={'bs':1,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),splitter=FuncSplitter(lambda x:False))\n",
    "dls=blk.dataloaders([env]*1,n=50,device=default_device())\n",
    "\n",
    "learner=SACLearner(dls,agent=agent,cbs=[DiscriminatorTrainer(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),\n",
    "                                        SACCriticTrainer],\n",
    "                   metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])\n",
    "learner.fit(3,lr=0.001,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_data.ipynb.\n",
      "Converted 05c_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.v1.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/14b_actorcritic.diayn.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
