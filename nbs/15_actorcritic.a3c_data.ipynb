{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.a3c_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C Datawise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinearA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(LinearA2C, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o=self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx=x.float()\n",
    "        return self.policy(fx),self.value(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=[\n",
    " Experience(s=tensor([[-0.0285,  0.1640, -0.0033, -0.3421]]),sp=tensor([[-0.0285,  0.1640, -0.0033, -0.3421]]),\n",
    "            a=tensor([1]),r=tensor([1.]),d=tensor([0.])),\n",
    " Experience(s=tensor([[-0.0252, -0.0311, -0.0101, -0.0504]]),sp=tensor([[-0.0252, -0.0311, -0.0101, -0.0504]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([0.])),\n",
    " Experience(s=tensor([[-0.0258, -0.2261, -0.0111,  0.2391]]),sp=tensor([[-0.0258, -0.2261, -0.0111,  0.2391]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([0.])),\n",
    " Experience(s=tensor([[-0.0517, -0.2260,  0.0195,  0.2377]]),sp=tensor([[-0.0517, -0.2260,  0.0195,  0.2377]]),\n",
    "            a=tensor([1]),r=tensor([1.]),d=tensor([0.])),\n",
    " Experience(s=tensor([[-0.0562, -0.4214,  0.0242,  0.5365]]),sp=tensor([[-0.0562, -0.4214,  0.0242,  0.5365]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([0.])),\n",
    " Experience(s=tensor([[-0.0647, -0.6169,  0.0349,  0.8367]]),sp=tensor([[-0.0647, -0.6169,  0.0349,  0.8367]]),\n",
    "            a=tensor([0]),r=tensor([1.]),d=tensor([1.]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def r_estimate(s,r,d_mask,non_d_mask,model,val_gamma,device):\n",
    "    \"Returns rewards `r` estimated direction by `model` from states `s`\"\n",
    "    r_np = np.array(r, dtype=np.float32)\n",
    "    if len(d_mask) != 0:\n",
    "        s_v = torch.FloatTensor(s).to(device)\n",
    "        v = model(s_v)[1]  # Remember that models are going to return the actions and the values\n",
    "        v_np = v.data.cpu().numpy()[:, 0]\n",
    "        r_np[d_mask] += val_gamma * v_np\n",
    "    return r_np\n",
    "\n",
    "def unbatch(batch,model,last_val_gamma,device='cpu'):\n",
    "    s, a, r, d_mask, sp = [], [], [], [], []\n",
    "    non_d_mask = []\n",
    "    for i, exp in enumerate(batch):\n",
    "#         print(exp.s.shape,exp.r.shape,exp.sp.shape,exp.a.shape,exp.d.shape)\n",
    "#         raise Exception\n",
    "        s.append(exp.s.numpy()[0])\n",
    "        a.append(int(exp.a.numpy()))  # TODO can we change this to toggle between discrete and continuous actions?\n",
    "        r.append(exp.r.numpy().astype(np.float32).reshape(1,))\n",
    "        if not bool(exp.d):\n",
    "            d_mask.append(i)\n",
    "            sp.append(exp.sp.numpy()[0].reshape(1,-1))\n",
    "        else:\n",
    "            non_d_mask.append(i)\n",
    "    s_t = torch.FloatTensor(s).to(device)\n",
    "    a_t = torch.LongTensor(a).to(device)\n",
    "    r_np = r_estimate(sp, r, d_mask, non_d_mask,model, last_val_gamma, device)\n",
    "    estimated_r = torch.FloatTensor(r_np).to(device)\n",
    "    return s_t, a_t, estimated_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0285,  0.1640, -0.0033, -0.3421],\n",
       "         [-0.0252, -0.0311, -0.0101, -0.0504],\n",
       "         [-0.0258, -0.2261, -0.0111,  0.2391],\n",
       "         [-0.0517, -0.2260,  0.0195,  0.2377],\n",
       "         [-0.0562, -0.4214,  0.0242,  0.5365],\n",
       "         [-0.0647, -0.6169,  0.0349,  0.8367]]),\n",
       " tensor([1, 0, 0, 1, 0, 0]),\n",
       " tensor([[1.0210],\n",
       "         [0.9960],\n",
       "         [1.0248],\n",
       "         [1.0280],\n",
       "         [1.0872],\n",
       "         [1.0000]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearA2C((4,),2)\n",
    "unbatch(batch,model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def loss_func(pred,yb,learn):\n",
    "#     print(yb)\n",
    "    yb=[Experience(**{k:yb[k][i] for k in yb}) for i in range(learn.dls.bs)]\n",
    "    s_t,a_t,r_est=unbatch(yb,learn.model,learn.discount**learn.reward_steps)\n",
    "#     print(r_est.mean(),np.mean([o.r.numpy() for o in yb]))\n",
    "#     print(sum([o.d for o in yb]))\n",
    "#     print(s_t.shape,a_t.shape,r_est.shape)\n",
    "#     r_est=r_est.squeeze(1)\n",
    "\n",
    "    learn.opt.zero_grad()\n",
    "    logits_v,value_v=learn.model(s_t)\n",
    "\n",
    "    loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)\n",
    "\n",
    "    log_prob_v=F.log_softmax(logits_v,dim=1)\n",
    "    adv_v=r_est-value_v.detach()\n",
    "\n",
    "    log_prob_actions_v=adv_v*log_prob_v[range(learn.dls.bs),a_t]\n",
    "    loss_policy_v=-log_prob_actions_v.mean()\n",
    "\n",
    "    prob_v=F.softmax(logits_v,dim=1)\n",
    "    entropy_loss_v=learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "    loss_v=entropy_loss_v+loss_value_v+loss_policy_v\n",
    "\n",
    "    return loss_v\n",
    "\n",
    "class A3CLearner(AgentLearner):\n",
    "    def __init__(self,dls,discount=0.99,entropy_beta=0.01,clip_grad=0.1,reward_steps=1,**kwargs):\n",
    "        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)\n",
    "        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))\n",
    "        self.model.share_memory()\n",
    "        self.discount=discount\n",
    "        self.entropy_beta=entropy_beta\n",
    "        self.reward_steps=reward_steps\n",
    "        self.clip_grad=clip_grad\n",
    "        \n",
    "    def _split(self, b):\n",
    "        if len(b)==1 and type(b[0])==tuple:b=b[0]\n",
    "        super()._split(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class A2CTrainer(Callback):\n",
    "    \n",
    "    def after_backward(self):\n",
    "        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fit(queue:mp.JoinableQueue=None,items:L=None,agent:BaseAgent=None,learner_cls:Learner=None,experience_block:ExperienceBlock=None,\n",
    "             cancel:mp.Event=None):\n",
    "#     print(agent,flush=True)\n",
    "    blk=IterableDataBlock(blocks=(experience_block(agent=agent)),\n",
    "                          splitter=FuncSplitter(lambda x:False))\n",
    "    dls=blk.dataloaders(items)\n",
    "    while True:\n",
    "        for x in dls[0]:\n",
    "            queue.put(x)\n",
    "            if cancel.is_set():\n",
    "                queue.put(None)\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FirstLastTfm(Transform):\n",
    "    def __init__(self,discount=0.99):self.discount=discount\n",
    "    \n",
    "    def reset(self,items):\n",
    "        if items.extra_len!=0:items.extra_len=0\n",
    "    \n",
    "    def encodes(self,o):\n",
    "        first_o=o[0]\n",
    "        first_o.sp=o[-1].sp\n",
    "        total_reward=first_o.r\n",
    "        elms=list(o)[:-1]\n",
    "\n",
    "        for exp in elms: # reversed(elms):\n",
    "            total_reward*=self.discount\n",
    "            total_reward+=exp.r\n",
    "        first_o.r=total_reward\n",
    "#         if any([t.absolute_end for t in o]): print(first_o)\n",
    "        \n",
    "        return asdict(first_o)\n",
    "\n",
    "\n",
    "@delegates(ResetAndStepTfm)\n",
    "def FirstLastExperienceBlock(dls_kwargs=None,**kwargs):\n",
    "    return TransformBlock(type_tfms=[MakeTfm(),ResetAndStepTfm(hist2dict=False,**kwargs),FirstLastTfm],dl_type=TfmdSourceDL,dls_kwargs=dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A2CLearner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0813f29cb1cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mActorCriticAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA2CLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA2CTrainer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAvgEpisodeRewardMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A2CLearner' is not defined"
     ]
    }
   ],
   "source": [
    "env='CartPole-v1'\n",
    "model=LinearA2C((4,),2)\n",
    "\n",
    "block=AsyncExperienceBlock(\n",
    "    experience_block=partial(FirstLastExperienceBlock,a=0,seed=0,n_steps=4,dls_kwargs={'bs':1,'num_workers':0,\n",
    "                                                                             'verbose':False,'indexed':True,'shuffle_train':False,\n",
    "                                                                             'batch_tfms':lambda x:(x['s'],x)}),\n",
    "    n_processes=1,\n",
    "    n=128,\n",
    "    data_fit=data_fit,\n",
    "    agent=ActorCriticAgent(model)\n",
    ")\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "                      batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*15,bs=128)\n",
    "\n",
    "agent=ActorCriticAgent(model=model)\n",
    "learner=A3CLearner(dls,agent=agent,cbs=[A3CTrainer],reward_steps=4,metrics=[AvgEpisodeRewardMetric()])\n",
    "learner.fit(4,lr=0.001,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
