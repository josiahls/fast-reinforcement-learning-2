{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev']  # upgrade fastrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab(): \n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-greensboro",
   "metadata": {},
   "source": [
    "# Async Testing\n",
    "> Testing async environment execution in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-infrastructure",
   "metadata": {},
   "source": [
    "One of the most important things to have working with the datablock is to answer: \n",
    "\n",
    "    Can we run multiple envs, in multiple workers, using a torch Module, all loaded on cuda?\n",
    "    \n",
    "You can however there are some important things to use in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-philippines",
   "metadata": {},
   "source": [
    "> Important: This is only important if you are training with `num_workers>0` and `str(default_device())=='cuda'`. If one of these is not true, then feel free\n",
    "to train your model the *normal* way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-acquisition",
   "metadata": {},
   "source": [
    "Relevant links:\n",
    "\n",
    "  [setting the `MKL_THREADING_LAYER` variable](https://github.com/pytorch/pytorch/issues/37377)\n",
    "  \n",
    "  [handling `set_start_method('spawn')`](https://stackoverflow.com/questions/48822463/how-to-use-pytorch-multiprocessing)\n",
    "  \n",
    "  [using `if __name__ == '__main__'`](https://github.com/pytorch/pytorch/issues/3492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.load import DataLoader,_FakeLoader,multiprocessing,_MultiProcessingDataLoaderIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils import worker as z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.test_utils import synth_learner\n",
    "from fastrl.data.block import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    os.environ['MKL_THREADING_LAYER']=\"GNU\"\n",
    "    \n",
    "    import torch.multiprocessing as mp\n",
    "    try:mp.set_start_method('spawn',force=True)\n",
    "    except Exception:pass\n",
    "    \n",
    "    # Third party libs\n",
    "    from fastai.torch_basics import *\n",
    "    from fastai.data.all import *\n",
    "    # Local modules\n",
    "    from fastrl.data.block import *\n",
    "    from torch.optim import Adam\n",
    "\n",
    "    \n",
    "    dqn=DQN().share_memory()\n",
    "    opt=Adam(dqn.parameters(),lr=0.01)\n",
    "    source=ExperienceSource('CartPole-v1',None,steps_count=3,n_envs=1)\n",
    "    ds=SourceDataset(source)\n",
    "    \n",
    "    dl=DataLoader(ds,num_workers=2,batch_size=5,n=100,persistent_workers=True)\n",
    "    for _ in range(4):\n",
    "        for x in dl:\n",
    "            out=dqn.policy(x[0]['state'])+1\n",
    "            target=torch.ones(out.shape)+1\n",
    "            loss=nn.MSELoss()(out,target)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting /home/fastrl_user/fastrl/nbs/index.ipynb to README.md\n",
      "Converted 00_core.ipynb.\n",
      "Converted 00_nbdev_extension.ipynb.\n",
      "Converted 05_data.block.ipynb.\n",
      "Converted 05_data.test_async.ipynb.\n",
      "Converted 20_test_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nbdev_template.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab(): \n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbdev.cli import make_readme\n",
    "    make_readme()\n",
    "    notebook2script()\n",
    "    notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-nature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
