{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastrl.ptan_extension import *\n",
    "\n",
    "from torch.distributions import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC\n",
    "\n",
    "> Soft Actor Critic\n",
    "\n",
    "Most of the code pre-refactor is thanks to: https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state.float(), action.float()], 1)\n",
    "\n",
    "        x1 = F.relu(self.linear1(xu))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "\n",
    "        x2 = F.relu(self.linear4(xu))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = torch.tensor(1.)\n",
    "            self.action_bias = torch.tensor(0.)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(GaussianPolicy, self).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean = nn.Linear(hidden_dim, num_actions)\n",
    "        self.noise = torch.Tensor(num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = 1.\n",
    "            self.action_bias = 0.\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = torch.tanh(self.mean(x)) * self.action_scale + self.action_bias\n",
    "        return mean\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean = self.forward(state)\n",
    "        noise = self.noise.normal_(0., std=0.1)\n",
    "        noise = noise.clamp(-0.25, 0.25)\n",
    "        action = mean + noise\n",
    "        return action, torch.tensor(0.), mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        self.noise = self.noise.to(device)\n",
    "        return super(DeterministicPolicy, self).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_log_gaussian(mean, log_std, t):\n",
    "    quadratic = -((0.5 * (t - mean) / (log_std.exp())).pow(2))\n",
    "    l = mean.shape\n",
    "    log_z = log_std\n",
    "    z = l[-1] * math.log(2 * math.pi)\n",
    "    log_p = quadratic.sum(dim=-1) - log_z.sum(dim=-1) - 0.5 * z\n",
    "    return log_p\n",
    "\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "    if dim is None:\n",
    "        inputs = inputs.view(-1)\n",
    "        dim = 0\n",
    "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "    if not keepdim:\n",
    "        outputs = outputs.squeeze(dim)\n",
    "    return outputs\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pprint import pprint\n",
    "\n",
    "class SAC(BaseAgent):\n",
    "    def __init__(self, num_inputs, action_space, gamma,tau,alpha,policy='gaussian',\n",
    "                automatic_entropy_tuning=True,target_update_interval=1,hidden_size=100,lr=0.0003):\n",
    "        \n",
    "        self.action_space=action_space\n",
    "        self.warming_up=False\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.policy_type = policy\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "\n",
    "        self.device=default_device()\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(device=self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"gaussian\":\n",
    "            # Target Entropy = ‚àídim(A) (e.g. , -6 for HalfCheetah-v2) as given in the paper\n",
    "            if self.automatic_entropy_tuning is True:\n",
    "                self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
    "                self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "                self.alpha_optim = Adam([self.log_alpha], lr=lr)\n",
    "\n",
    "            self.policy = GaussianPolicy(num_inputs, action_space.shape[0], hidden_size, action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        else:\n",
    "            self.alpha = 0\n",
    "            self.automatic_entropy_tuning = False\n",
    "            self.policy = DeterministicPolicy(num_inputs, action_space.shape[0], hidden_size, action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "            \n",
    "        self.updates=0\n",
    "\n",
    "    def select_action(self, state, evaluate=False):\n",
    "        if self.warming_up: return self.action_space.sample()\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if evaluate is False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def __call__(self,s,asl):\n",
    "        return self.select_action(s),asl\n",
    "\n",
    "    def update_parameters(self, *yb, learn):\n",
    "        # Sample a batch from memory\n",
    "#         state_batch, action_batch, reward_batch, next_state_batch, mask_batch = learn.memory.sample(batch_size=batch_size)\n",
    "        batch=learn.sample_yb\n",
    "#         pprint(batch)\n",
    "        state_batch=torch.stack([o.state.to(device=default_device()) for o in batch]).float()\n",
    "        next_state_batch=torch.stack([o.last_state.to(device=default_device()) for o in batch]).float()\n",
    "        action_batch=torch.stack([o.action.to(device=default_device()) for o in batch]).float()\n",
    "        reward_batch=torch.stack([o.reward.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        mask_batch=torch.stack([o.done.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        \n",
    "#         print(state_batch.shape,next_state_batch.shape,action_batch.shape,reward_batch.shape,mask_batch.shape)\n",
    "#         state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "#         next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "#         action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "#         reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "#         mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + (1-mask_batch) * self.gamma * (min_qf_next_target)\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # JœÄ = ùîºst‚àºD,Œµt‚àºN[Œ± * logœÄ(f(Œµt;st)|st) ‚àí Q(st,f(Œµt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
    "\n",
    "\n",
    "        if self.updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        self.updates+=1\n",
    "#         print(self.updates)\n",
    "\n",
    "        return qf1_loss+ qf2_loss+ policy_loss+ alpha_loss+ alpha_tlogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExperienceReplay(Callback):\n",
    "    def __init__(self,sz=100,bs=128,starting_els=1,max_steps=1):\n",
    "        store_attr()\n",
    "        self.queue=deque(maxlen=int(sz))\n",
    "        self.max_steps=max_steps\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.agent.warming_up=True\n",
    "        while len(self.queue)<self.starting_els:\n",
    "            for i,o in enumerate(self.dls.train):\n",
    "                batch=[ExperienceFirstLast(state=o[0][i],action=o[1][i],reward=o[2][i],\n",
    "                                    last_state=o[3][i], done=(o[4][i] and self.max_steps!=o[6][i]),episode_reward=o[5][i],steps=o[6][i])\n",
    "                                    for i in range(len(o[0]))]\n",
    "#                 print(self.max_steps,max([o.steps for o in batch]))\n",
    "                for _b in batch: self.queue.append(_b)\n",
    "                if len(self.queue)>self.starting_els:break\n",
    "        self.learn.agent.warming_up=False\n",
    "\n",
    "#     def after_epoch(self):\n",
    "#         print(len(self.queue))\n",
    "    def before_batch(self):\n",
    "#         print(len(self.queue))\n",
    "        b=list(self.learn.xb)+list(self.learn.yb)\n",
    "        batch=[ExperienceFirstLast(state=b[0][i],action=b[1][i],reward=b[2][i],\n",
    "                                last_state=b[3][i], done=(b[4][i] and self.max_steps!=b[6][i]),episode_reward=b[5][i],\n",
    "                                steps=b[6][i])\n",
    "                                for i in range(len(b[0]))]\n",
    "        for _b in batch: self.queue.append(_b)\n",
    "        idxs=np.random.randint(0,len(self.queue), self.bs)\n",
    "        self.learn.sample_yb=[self.queue[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SACCriticTrainer(Callback):\n",
    "    def after_batch(self): \n",
    "        self.learn.dls.bs=1\n",
    "        for d in self.learn.dls.loaders: d.bs=1\n",
    "        \n",
    "    def after_loss(self):raise CancelBatchException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "class SACLearner(AgentLearner):\n",
    "    def __init__(self,dls,agent=None,reward_scale=2,**kwargs):\n",
    "        store_attr()\n",
    "#         print(type(self.agent))\n",
    "        super().__init__(dls,loss_func=partial(self.agent.update_parameters,learn=self),model=agent.policy,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>67.867859</td>\n",
       "      <td>-1366.547326</td>\n",
       "      <td>None</td>\n",
       "      <td>-1366.547326</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>149.835205</td>\n",
       "      <td>-1374.884861</td>\n",
       "      <td>None</td>\n",
       "      <td>-1374.884861</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>239.833511</td>\n",
       "      <td>-1371.623351</td>\n",
       "      <td>None</td>\n",
       "      <td>-1371.623351</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>230.481308</td>\n",
       "      <td>-1314.894176</td>\n",
       "      <td>None</td>\n",
       "      <td>-1314.894176</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>264.249390</td>\n",
       "      <td>-1246.377057</td>\n",
       "      <td>None</td>\n",
       "      <td>-1246.377057</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>310.760803</td>\n",
       "      <td>-1162.032088</td>\n",
       "      <td>None</td>\n",
       "      <td>-1162.032088</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>343.544617</td>\n",
       "      <td>-1130.603722</td>\n",
       "      <td>None</td>\n",
       "      <td>-1130.603722</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>362.582153</td>\n",
       "      <td>-1113.652556</td>\n",
       "      <td>None</td>\n",
       "      <td>-1113.652556</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>379.829315</td>\n",
       "      <td>-1076.529074</td>\n",
       "      <td>None</td>\n",
       "      <td>-1076.529074</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>391.366364</td>\n",
       "      <td>-1025.440770</td>\n",
       "      <td>None</td>\n",
       "      <td>-1025.440770</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>406.220245</td>\n",
       "      <td>-972.337199</td>\n",
       "      <td>None</td>\n",
       "      <td>-972.337199</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>421.276794</td>\n",
       "      <td>-927.642430</td>\n",
       "      <td>None</td>\n",
       "      <td>-927.642430</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>433.103760</td>\n",
       "      <td>-889.666813</td>\n",
       "      <td>None</td>\n",
       "      <td>-889.666813</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>438.353668</td>\n",
       "      <td>-856.775668</td>\n",
       "      <td>None</td>\n",
       "      <td>-856.775668</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>445.719269</td>\n",
       "      <td>-828.110555</td>\n",
       "      <td>None</td>\n",
       "      <td>-828.110555</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>439.653717</td>\n",
       "      <td>-802.812854</td>\n",
       "      <td>None</td>\n",
       "      <td>-802.812854</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>435.211395</td>\n",
       "      <td>-780.258549</td>\n",
       "      <td>None</td>\n",
       "      <td>-780.258549</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>420.356628</td>\n",
       "      <td>-760.156314</td>\n",
       "      <td>None</td>\n",
       "      <td>-760.156314</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>420.747498</td>\n",
       "      <td>-742.115827</td>\n",
       "      <td>None</td>\n",
       "      <td>-742.115827</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>399.372437</td>\n",
       "      <td>-725.784546</td>\n",
       "      <td>None</td>\n",
       "      <td>-725.784546</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>397.689819</td>\n",
       "      <td>-710.920380</td>\n",
       "      <td>None</td>\n",
       "      <td>-710.920380</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>369.486847</td>\n",
       "      <td>-697.310483</td>\n",
       "      <td>None</td>\n",
       "      <td>-697.310483</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>354.255035</td>\n",
       "      <td>-684.854308</td>\n",
       "      <td>None</td>\n",
       "      <td>-684.854308</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>334.669556</td>\n",
       "      <td>-673.358420</td>\n",
       "      <td>None</td>\n",
       "      <td>-673.358420</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>313.210266</td>\n",
       "      <td>-669.272344</td>\n",
       "      <td>None</td>\n",
       "      <td>-669.272344</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>298.442230</td>\n",
       "      <td>-617.109973</td>\n",
       "      <td>None</td>\n",
       "      <td>-617.109973</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>283.432068</td>\n",
       "      <td>-578.053348</td>\n",
       "      <td>None</td>\n",
       "      <td>-578.053348</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>273.536011</td>\n",
       "      <td>-539.894737</td>\n",
       "      <td>None</td>\n",
       "      <td>-539.894737</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>265.283142</td>\n",
       "      <td>-511.163623</td>\n",
       "      <td>None</td>\n",
       "      <td>-511.163623</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>250.635330</td>\n",
       "      <td>-489.507176</td>\n",
       "      <td>None</td>\n",
       "      <td>-489.507176</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "env='Pendulum-v0'\n",
    "agent=SAC(3,gym.make(env).action_space,gamma=0.99,tau=0.005,alpha=0.2)\n",
    "\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,exclude_nones=True,\n",
    "                               dls_kwargs={'bs':1,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "#                       batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*1,n=1000,device=default_device())\n",
    "\n",
    "learner=SACLearner(dls,agent=agent,cbs=[ExperienceReplay(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),SACCriticTrainer],\n",
    "                   metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])\n",
    "learner.fit(30,lr=0.001,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
