{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp qlearning.dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "from torch.optim import *\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastrl.ptan_extension import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinearDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(LinearDQN, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx=x.float()\n",
    "        return self.policy(fx)\n",
    "    \n",
    "class ExperienceReplay(Callback):\n",
    "    def __init__(self,sz=100,bs=128,starting_els=1,max_steps=1):\n",
    "        store_attr()\n",
    "        self.queue=deque(maxlen=int(sz))\n",
    "        self.max_steps=max_steps\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.agent.warming_up=True\n",
    "        while len(self.queue)<self.starting_els:\n",
    "            for i,o in enumerate(self.dls.train):\n",
    "                batch=[ExperienceFirstLast(state=o[0][i],action=o[1][i],reward=o[2][i],\n",
    "                                    last_state=o[3][i], done=o[4][i],episode_reward=o[5][i],steps=o[6][i])\n",
    "                                    for i in range(len(o[0]))]\n",
    "#                 print(self.max_steps,max([o.steps for o in batch]))\n",
    "                for _b in batch: self.queue.append(_b)\n",
    "                if len(self.queue)>self.starting_els:break\n",
    "        self.learn.agent.warming_up=False\n",
    "\n",
    "#     def after_epoch(self):\n",
    "#         print(len(self.queue))\n",
    "    def before_batch(self):\n",
    "#         print(len(self.queue))\n",
    "        b=list(self.learn.xb)+list(self.learn.yb)\n",
    "        batch=[ExperienceFirstLast(state=b[0][i],action=b[1][i],reward=b[2][i],\n",
    "                                last_state=b[3][i], done=b[4][i],episode_reward=b[5][i],\n",
    "                                steps=b[6][i])\n",
    "                                for i in range(len(b[0]))]\n",
    "        for _b in batch: self.queue.append(_b)\n",
    "        idxs=np.random.randint(0,len(self.queue), self.bs)\n",
    "        self.learn.sample_yb=[deepcopy(self.queue[i]) for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EpsilonTracker(Callback):\n",
    "    def __init__(self,e_stop=0.2,e_start=1.0,e_steps=5000,current_step=0):\n",
    "        store_attr()\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.agent.a_selector.epsilon=self.e_start\n",
    "    \n",
    "    def after_step(self):\n",
    "        self.learn.agent.a_selector.epsilon=max(self.e_stop,self.e_start-self.current_step/self.e_steps)\n",
    "        self.current_step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_target(net, local_reward,next_state,done,discount):\n",
    "    if done: return local_reward\n",
    "    next_q_v = net(next_state.float().unsqueeze(0))\n",
    "    best_q = next_q_v.max(dim=1)[0].item()\n",
    "    return local_reward + discount * best_q\n",
    "\n",
    "class DQNTrainer(Callback):\n",
    "    def after_pred(self):\n",
    "        s,a,r,sp,d,er,steps=(self.learn.xb+self.learn.yb)\n",
    "        exps=[ExperienceFirstLast(*o) for o in zip(*(self.learn.xb+self.learn.yb))]\n",
    "        batch_targets=[calc_target(self.learn.model, exp.reward, exp.last_state,exp.done,self.learn.discount)\n",
    "                         for exp in exps]\n",
    "        \n",
    "        s_v = s.float()\n",
    "        q_v = self.learn.model(s_v)\n",
    "        t_q=q_v.data.numpy().copy()\n",
    "        t_q[range(len(exps)), a] = batch_targets\n",
    "        target_q_v = torch.tensor(t_q)\n",
    "        self.learn._yb=self.learn.yb\n",
    "        self.learn.yb=(target_q_v,)\n",
    "        self.learn.pred=q_v\n",
    "#         print(*self.learn.yb,self.learn.pred)\n",
    "#         print(self.learn.pred,self.learn.yb)\n",
    "#         print(self.learn._yb,self.learn.yb[0])\n",
    "    \n",
    "    def after_loss(self):self.learn.yb=self.learn._yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DQNLearner(AgentLearner):\n",
    "    def __init__(self,dls,discount=0.99,**kwargs):\n",
    "        store_attr()\n",
    "        self.target_q_v=[]\n",
    "        super().__init__(dls,loss_func=nn.MSELoss(),**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>24.613981</td>\n",
       "      <td>29.540000</td>\n",
       "      <td>None</td>\n",
       "      <td>29.540000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>32.930523</td>\n",
       "      <td>51.570000</td>\n",
       "      <td>None</td>\n",
       "      <td>51.570000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21.004852</td>\n",
       "      <td>82.090000</td>\n",
       "      <td>None</td>\n",
       "      <td>82.090000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.406000</td>\n",
       "      <td>121.940000</td>\n",
       "      <td>None</td>\n",
       "      <td>121.940000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.700549</td>\n",
       "      <td>157.430000</td>\n",
       "      <td>None</td>\n",
       "      <td>157.430000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>25.562292</td>\n",
       "      <td>185.620000</td>\n",
       "      <td>None</td>\n",
       "      <td>185.620000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>72.588402</td>\n",
       "      <td>192.840000</td>\n",
       "      <td>None</td>\n",
       "      <td>192.840000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.033764</td>\n",
       "      <td>207.890000</td>\n",
       "      <td>None</td>\n",
       "      <td>207.890000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>39.098164</td>\n",
       "      <td>186.120000</td>\n",
       "      <td>None</td>\n",
       "      <td>186.120000</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>18.933603</td>\n",
       "      <td>222.090000</td>\n",
       "      <td>None</td>\n",
       "      <td>222.090000</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16.825060</td>\n",
       "      <td>218.860000</td>\n",
       "      <td>None</td>\n",
       "      <td>218.860000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>37.401455</td>\n",
       "      <td>240.070000</td>\n",
       "      <td>None</td>\n",
       "      <td>240.070000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>47.032150</td>\n",
       "      <td>256.590000</td>\n",
       "      <td>None</td>\n",
       "      <td>256.590000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>40.052586</td>\n",
       "      <td>288.350000</td>\n",
       "      <td>None</td>\n",
       "      <td>288.350000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>45.070744</td>\n",
       "      <td>323.100000</td>\n",
       "      <td>None</td>\n",
       "      <td>323.100000</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env='CartPole-v1'\n",
    "model=LinearDQN((4,),2)\n",
    "agent=DiscreteAgent(model=model.to(default_device()),device=default_device(),\n",
    "                    a_selector=EpsilonGreedyActionSelector())\n",
    "\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,dls_kwargs={'bs':8,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "#                       batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*1,n=8*1000,device=default_device())\n",
    "\n",
    "learner=DQNLearner(dls,agent=agent,cbs=[EpsilonTracker,\n",
    "                                        ExperienceReplay(sz=50000,bs=8,starting_els=8,max_steps=gym.make(env)._max_episode_steps),\n",
    "                                        DQNTrainer],metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])\n",
    "learner.fit(15,lr=0.01,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 05c_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.v1.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/20a_qlearning.dqn.ipynb\n",
      "converting: /opt/project/fastrl/nbs/20c_qlearning.dqn_target.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
