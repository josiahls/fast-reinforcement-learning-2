{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from fastai.metrics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "from torch.optim import *\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastai.callback.progress import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Metrics\n",
    "\n",
    "> Metrics for tracking the progress of reinforcement learning agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AvgEpisodeRewardMetric(Metric):\n",
    "    def __init__(self):self.rolling_rewards=deque([0],maxlen=100)\n",
    "        \n",
    "    def accumulate(self,learn):\n",
    "        yb=learn.yb[0]\n",
    "#         print(yb)\n",
    "        yb=[Experience(**{k:yb[k][i] for k in yb}) for i in range(yb['absolute_end'].shape[0])]\n",
    "#         yb=[for yb.items()]\n",
    "        rewards=[y.episode_r for y in yb if y.absolute_end]\n",
    "#         print([y for y in yb if y.absolute_end])\n",
    "        for r in rewards:\n",
    "            if len(r.numpy().shape)!=0:self.rolling_rewards.extend(r.numpy())\n",
    "            else:                      self.rolling_rewards.append(r.numpy())\n",
    "#         print(len(rewards))\n",
    "#         if len(rewards)!=0:self.r=sum(rewards)/len(rewards)\n",
    "        \n",
    "    @property\n",
    "    def value(self):return np.mean(self.rolling_rewards) if len(self.rolling_rewards)!=1 else self.rolling_rewards[0]\n",
    "    @property\n",
    "    def name(self):return 'avg_episode_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.actorcritic.a3c_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.674018</td>\n",
       "      <td>8.307692</td>\n",
       "      <td>None</td>\n",
       "      <td>8.307692</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.630438</td>\n",
       "      <td>8.653846</td>\n",
       "      <td>None</td>\n",
       "      <td>8.653846</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.587409</td>\n",
       "      <td>8.769231</td>\n",
       "      <td>None</td>\n",
       "      <td>8.769231</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/project/fastrl/fastrl/actorcritic/a3c_data.py:97: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)\n",
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "env='CartPole-v1'\n",
    "\n",
    "block=AsyncExperienceBlock(\n",
    "    experience_block=partial(FirstLastExperienceBlock,a=0,seed=0,dls_kwargs={'bs':1,'n_steps':4,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False}),\n",
    "    n_processes=4,\n",
    "    n=128\n",
    ")\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "                      batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*15,bs=128)\n",
    "\n",
    "model=LinearA2C((4,),2)\n",
    "agent=ActorCriticAgent(model=model)\n",
    "learner=A3CLearner(dls,agent=agent,cbs=[A3CTrainer],reward_steps=4,\n",
    "                   metrics=[AvgEpisodeRewardMetric()])\n",
    "learner.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_data.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 06_basic_train.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14_actorcritic.sac.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/16_actorcritic.a2c.ipynb\n",
      "converting: /opt/project/fastrl/nbs/13_metrics.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export2html import *\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
