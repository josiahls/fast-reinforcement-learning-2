{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExperienceSourceDatasets\n",
    "\n",
    "> Iterable datasets for returning environment outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `TfmdSourceDL` to trigger some cleanup before doing an iteration. \n",
    "\n",
    "TODO: (Josiah): Is there a way to override the `before_iter` in the DataBlock instead? The main issue is that we need to be able to reference `self` which isn't possible when passing methods through the `DataLoader` params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TfmdSourceDL(TfmdDL):\n",
    "    def before_iter(self):\n",
    "        super().before_iter()\n",
    "        self.dataset.reset_src()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TfmdSource` has an adjustable `__len__`. Unlike the `TfmdLists`, `TfmdSource` iters on a single item until the item raises a `SourceExhausted` exception. This means that the soruces `items` are being tracked by a separate index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SourceExhausted(Exception):pass\n",
    "\n",
    "@delegates(TfmdLists)\n",
    "class TfmdSource(TfmdLists):\n",
    "    \"A `Pipeline` of `tfms` applied to a collection of sources called `items`. Only swtches between them if they get exhausted.\"\n",
    "    def __init__(self,items, tfms,n:int=None,cycle_srcs=True,verbose=False,**kwargs):\n",
    "        self.n=n;self.cycle_srcs=cycle_srcs;self.source_idx=0;self.verbose=verbose;self.res_buffer=deque([]);self.extra_len=0\n",
    "        super().__init__(items,tfms,**kwargs)\n",
    "#         store_attr('n,cycle_srcs', self) TODO (Josiah): Does not seem to work?\n",
    "        \n",
    "    def __repr__(self): return f\"{self.__class__.__name__}: Cycling sources: {self.cycle_srcs}\\n{self.items}\\ntfms - {self.tfms.fs}\"\n",
    "    def reset_src(self): \n",
    "        [t.reset(self) for t in self.tfms if hasattr(t,'reset')]\n",
    "        self.res_buffer.clear()\n",
    "        \n",
    "    def setup(self,train_setup=True):super().setup(train_setup);self.reset_src()\n",
    "     \n",
    "    def __len__(self):\n",
    "#         return ifnone(self.n,super().__len__()) TODO (Josiah): self.n is not settable in DataBlock, and since TfmdLists gets reinit, this will not persist\n",
    "        if len(self.items)!=0 and isinstance(self.items[0],gym.Env) and self.cycle_srcs:\n",
    "            self.reset_src()\n",
    "            return self.items[0].spec.max_episode_steps+self.extra_len # TODO(Josiah): This is the only OpenAI dependent code. How do we have htis set in setup?\n",
    "        return super().__len__()\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if len(self.res_buffer)!=0:\n",
    "#             print('\\nBuffer Not Empty:',self.res_buffer)\n",
    "            return self.res_buffer.popleft()\n",
    "        \n",
    "        try:res=super().__getitem__(self.source_idx if self.cycle_srcs else idx)\n",
    "        except (IndexError,SourceExhausted) as e:\n",
    "            if not self.cycle_srcs:raise\n",
    "            if type(e)==SourceExhausted: \n",
    "                self.source_idx+=1;  pv(f'SourceExhausted, incrementing to idx {self.source_idx}',verbose=self.verbose) \n",
    "                if len(self.items)<=self.source_idx:e=IndexError(f'Index {self.source_idx} from SourceExhausted except is out of bounds.')\n",
    "            if type(e)==IndexError:      \n",
    "                self.source_idx=0;   pv(f'IndexError, setting idx to {self.source_idx}',verbose=self.verbose)\n",
    "                self.reset_src()\n",
    "            res=self.__getitem__(self.source_idx)\n",
    "        \n",
    "        if is_listy(res):\n",
    "            self.res_buffer=deque(res)\n",
    "#             print('\\nBuffer Empty:',self.res_buffer)\n",
    "            return self.res_buffer.popleft()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class IterableDataBlock(DataBlock):\n",
    "    def datasets(self, source, verbose=True):\n",
    "        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n",
    "        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n",
    "        splits = (self.splitter or RandomSplitter())(items)\n",
    "        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n",
    "        tls=L([TfmdSource(items, t,verbose=verbose) for t in L(ifnone(self._combine_type_tfms(),[None]))])\n",
    "        return Datasets(items,tls=tls,splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MakeTfm(Transform):\n",
    "    def setup(self,items:TfmdSource,train_setup=False):\n",
    "        items.cycle_srcs=False\n",
    "        for i in range(len(items)):items[i]=gym.make(items[i])\n",
    "        return super().setup(items,train_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export    \n",
    "def env_display(env:gym.Env):\n",
    "    img=env.render('rgb_array')\n",
    "    try:display.clear_output(wait=True)\n",
    "    except AttributeError:pass\n",
    "    new_im=PIL.Image.fromarray(img)\n",
    "    display.display(new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class Experience:d:bool;s:np.ndarray;sp:np.ndarray;r:float;a:Any;eid:int=0\n",
    "def envlen(o:gym.Env):return o.spec.max_episode_steps\n",
    "\n",
    "@dataclass\n",
    "class ResetAndStepTfm(Transform):\n",
    "    def __init__(self,seed:int=None,agent:object=None,n_steps:int=1,steps_delta:int=1,a:Any=None,history:deque=None,\n",
    "                 s:dict=None,steps:dict=None,maxsteps:int=None,display:bool=False,hist2dict:bool=True):\n",
    "        self.seed=seed;self.agent=agent;self.n_steps=n_steps;self.steps_delta=steps_delta;self.a=a;self.history=history;self.hist2dict=hist2dict\n",
    "        self.maxsteps=maxsteps;self.display=display\n",
    "        self.s=ifnone(s,{})\n",
    "        self.steps=ifnone(steps,{})\n",
    "        # store_attr('n,cycle_srcs', self) TODO (Josiah): Does not seem to work?\n",
    "            \n",
    "    def setup(self,items:TfmdSource,train_setup=False):\n",
    "        self.reset(items)\n",
    "        self.history=deque(maxlen=self.n_steps)\n",
    "        return super().setup(items,train_setup)\n",
    "    \n",
    "    def reset(self,items):\n",
    "        if items.extra_len==0:\n",
    "            items.extra_len=items.items[0].spec.max_episode_steps*(self.n_steps-1) # Extra steps to unwrap done\n",
    "        items.cycle_srcs=False\n",
    "        self.s={id(o):o.reset() for o in items.items if o.seed(self.seed) or True}\n",
    "        self.steps={id(o):0 for o in items.items}\n",
    "        self.maxsteps=ifnone(self.maxsteps,envlen(items.items[0]))\n",
    "        if self.history is not None:self.history.clear()\n",
    "        items.cycle_srcs=True\n",
    "        \n",
    "    def queue2dict(self,q:deque):return [asdict(hist) for hist in tuple(copy(q))]\n",
    "    def encodes(self,o:gym.Env):\n",
    "        # If history has done, then instead we try emptying the environment\n",
    "        if self.history and self.history[-1].d:\n",
    "            self.history.popleft()\n",
    "            if len(self.history)==0:raise SourceExhausted\n",
    "            return self.queue2dict(self.history) if self.hist2dict else copy(self.history)\n",
    "        \n",
    "        while True:\n",
    "            a=ifnone(self.a,o.action_space.sample()) if self.agent is None else self.agent(self.s[id(o)])\n",
    "            sp,r,d,_=o.env.step(a)\n",
    "            if self.display:env_display(o)\n",
    "                \n",
    "            self.steps[id(o)]+=1\n",
    "            d=self.steps[id(o)]>=self.maxsteps if not d else d\n",
    "    \n",
    "            self.history.append(Experience(d=d,s=self.s[id(o)].copy(),sp=sp.copy(),r=r,a=a,eid=id(o)))\n",
    "            self.s[id(o)]=sp.copy()\n",
    "            \n",
    "            if self.steps[id(o)]%self.steps_delta!=0: continue # TODO(Josiah): if `steps_delta`!=1, it may skip the first state. Is this ok?\n",
    "            if len(self.history)!=self.n_steps:       continue\n",
    "            break\n",
    "        \n",
    "        return self.queue2dict(self.history) if self.hist2dict else copy(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(ResetAndStepTfm)\n",
    "def ExperienceBlock(**kwargs):\n",
    "    return TransformBlock(type_tfms=[MakeTfm(),ResetAndStepTfm(**kwargs)],dl_type=TfmdSourceDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "@delegates(ResetAndStepTfm)\n",
    "def test_block(n_steps=1,steps_delta=1,block=ExperienceBlock,**kwargs):\n",
    "    for env in ['MountainCar-v0','CartPole-v1']:\n",
    "        blk=IterableDataBlock(blocks=(block(n_steps=n_steps,steps_delta=steps_delta,**kwargs)),\n",
    "                    splitter=FuncSplitter(lambda x:False))\n",
    "\n",
    "        dls=blk.dataloaders([env]*5,bs=1,num_workers=0,verbose=False,\n",
    "                              indexed=True,shuffle_train=False)\n",
    "\n",
    "\n",
    "        states={\n",
    "            'MountainCar-v0':['tensor([[-0.5891,  0.0000]], dtype=torch.float64)','tensor([[-0.7105,  0.0043]], dtype=torch.float64)'],\n",
    "            'CartPole-v1':['tensor([[-0.0446,  0.0465,  0.0133, -0.0210]], dtype=torch.float64)',\n",
    "                           'tensor([[-0.1770, -1.7150,  0.2274,  2.7892]], dtype=torch.float64)']\n",
    "        }\n",
    "\n",
    "        print('Starting Iteration')\n",
    "        counter=0\n",
    "        counters=[]\n",
    "        for epoch in range(3):\n",
    "            dones=0\n",
    "            for x in dls[0]:\n",
    "#                 print('\\nResult',x)\n",
    "                if counter==0: test_eq(str(x[0]['s']),states[env][0])\n",
    "                if x[0]['d']:\n",
    "#                     print(len(dls[0]),dones)\n",
    "                    dones+=1\n",
    "                    if dones==n_steps:\n",
    "                        test_eq(str(x[0]['sp']),states[env][1])\n",
    "                        print(counter)\n",
    "                        # TODO(Josiah): Figure out why this differs between runs.\n",
    "                        if env=='MountainCar-v0':test_eq(counter,pytest.approx(200*n_steps-n_steps*(0+1),10))\n",
    "                        counters.append(counter)\n",
    "                        counter=0\n",
    "                else:\n",
    "                    counter+=1\n",
    "            test_ne(counters[-1],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all defaults work as expected. That an episode completes, and that the expected final state gets reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting items from ['MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n",
      "199\n",
      "199\n",
      "199\n",
      "Collecting items from ['CartPole-v1', 'CartPole-v1', 'CartPole-v1', 'CartPole-v1', 'CartPole-v1']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n",
      "8\n",
      "445\n",
      "445\n"
     ]
    }
   ],
   "source": [
    "test_block(a=0,seed=0,n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting items from ['MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n",
      "594\n",
      "597\n",
      "597\n",
      "Collecting items from ['CartPole-v1', 'CartPole-v1', 'CartPole-v1', 'CartPole-v1', 'CartPole-v1']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n",
      "21\n",
      "1314\n",
      "1314\n"
     ]
    }
   ],
   "source": [
    "test_block(a=0,seed=0,n_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting items from ['MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n",
      "199\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "==:\ntensor([[-0.7105,  0.0043]], dtype=torch.float64)\ntensor([[-0.5891,  0.0000]], dtype=torch.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-565-b86bfbd2212d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-562-d4a9513a59b9>\u001b[0m in \u001b[0;36mtest_block\u001b[0;34m(n_steps, steps_delta, block, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#                 print('\\nResult',x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#                     print(len(dls[0]),dones)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest_eq\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"`test` that `a==b`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'=='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m\"`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ==:\ntensor([[-0.7105,  0.0043]], dtype=torch.float64)\ntensor([[-0.5891,  0.0000]], dtype=torch.float64)"
     ]
    }
   ],
   "source": [
    "test_block(a=0,seed=0,n_steps=3,steps_delta=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FirstLastTfm(Transform):\n",
    "    def __init__(self,discount=0.99):self.discount=discount\n",
    "    \n",
    "    def reset(self,items):\n",
    "        if items.extra_len!=0:items.extra_len=0\n",
    "    \n",
    "    def encodes(self,o):\n",
    "        total_reward=0.0\n",
    "        first_o=o[0]\n",
    "        first_o.sp=o[-1].sp\n",
    "        for exp in reversed(list(o)):\n",
    "            total_reward*=self.discount\n",
    "            total_reward+=exp.r\n",
    "        first_o.r=total_reward\n",
    "        return asdict(first_o)\n",
    "\n",
    "\n",
    "@delegates(ResetAndStepTfm)\n",
    "def FirstLastExperienceBlock(**kwargs):\n",
    "    return TransformBlock(type_tfms=[MakeTfm(),ResetAndStepTfm(hist2dict=False,**kwargs),FirstLastTfm],dl_type=TfmdSourceDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting items from ['MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0', 'MountainCar-v0']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n",
      "Collecting items from ['CartPole-v1', 'CartPole-v1', 'CartPole-v1', 'CartPole-v1', 'CartPole-v1']\n",
      "Found 5 items\n",
      "2 datasets of sizes 5,0\n",
      "Starting Iteration\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "@delegates(ResetAndStepTfm)\n",
    "def fl_test_block(n_steps,block=FirstLastExperienceBlock,**kwargs):\n",
    "    for env in ['MountainCar-v0','CartPole-v1']:\n",
    "        blk=IterableDataBlock(blocks=(block(n_steps=n_steps,**kwargs)),\n",
    "                    splitter=FuncSplitter(lambda x:False))\n",
    "\n",
    "        dls=blk.dataloaders([env]*5,bs=1,num_workers=0,verbose=False,\n",
    "                              indexed=True,shuffle_train=False)\n",
    "\n",
    "\n",
    "        states={\n",
    "            'MountainCar-v0':['tensor([[-0.5891,  0.0000]], dtype=torch.float64)','tensor([[-0.7105,  0.0043]], dtype=torch.float64)'],\n",
    "            'CartPole-v1':['tensor([[-0.0446,  0.0465,  0.0133, -0.0210]], dtype=torch.float64)',\n",
    "                           'tensor([[-0.1770, -1.7150,  0.2274,  2.7892]], dtype=torch.float64)']\n",
    "        }\n",
    "\n",
    "        print('Starting Iteration')\n",
    "        counter=0\n",
    "        counters=[]\n",
    "        for epoch in range(3):\n",
    "            dones=0\n",
    "            for x in dls[0]:\n",
    "#                 print('\\nResult',x)\n",
    "                if counter==0: test_eq(str(x[0]['s']),states[env][0])\n",
    "                if x[0]['d']:\n",
    "#                     print(len(dls[0]),dones)\n",
    "                    dones+=1\n",
    "                    test_eq(str(x[0]['sp']),states[env][1])\n",
    "#                     print(counter)\n",
    "                    # TODO(Josiah): Figure out why this differs between runs.\n",
    "                    if env=='MountainCar-v0':test_eq(counter,pytest.approx(200*n_steps-n_steps*(0+1),1))\n",
    "                    counters.append(counter)\n",
    "                    counter=0\n",
    "                else:\n",
    "                    counter+=1\n",
    "            test_ne(counters[-1],0)\n",
    "fl_test_block(a=0,seed=0,n_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# ExperienceSourceDatasets OLD\n",
    "\n",
    "> Iterable datasets for returning environment outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnvLists\n",
    "\n",
    "> Iterable lists for returning environment outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(TfmdLists.__init__)\n",
    "class EnvLists(TfmdLists):\n",
    "    def __init__(self,items,tfms,**kwargs):\n",
    "        self.is_set=False\n",
    "        super().__init__(items,tfms,**kwargs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if (len(self.items)!=0 and not issubclass(type(self.items[0]),gym.Env)) or not self.is_set: return len(self.items) \n",
    "        else:                                                                                       return self.items[0].spec.max_episode_steps\n",
    "\n",
    "    def _get(self,i): return i if self.is_set else super()._get(i) \n",
    "        \n",
    "    def setup(self,train_setup=True):\n",
    "        super().setup(train_setup)\n",
    "        for f in self.fs:\n",
    "            if hasattr(f,'reset'):f.reset()\n",
    "        self.is_set=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnvMakeTfm\n",
    "\n",
    "> Make environments into their openai gym versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EnvMakeTfm(Transform):    \n",
    "    def setup(self,items=None,train_setup=False):\n",
    "        for i,o in enumerate(items):\n",
    "            print(o)\n",
    "            items[i]=gym.make(o)\n",
    "        return super().setup(items,train_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the correct environment was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl=EnvLists(['CartPole-v1' for _ in range(5)],tfms=[EnvMakeTfm])\n",
    "test_eq(len(tl),500)\n",
    "for o in tl:test_stdout(lambda:print(tl.items[o%5]),'<TimeLimit<CartPoleEnv<CartPole-v1>>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnvResetTfms\n",
    "\n",
    "> Handles environment resetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class DoneStateEnv:d:bool;s:np.array;env:object\n",
    "DoneStateEnv.__repr__=lambda self:str((self.d,self.s,self.env))\n",
    "\n",
    "@dataclass\n",
    "class EnvResetTfm(Transform):\n",
    "    env_idx:int=0;seed:Optional[int]=None;s:Optional[np.ndarray]=None;d:bool=False;was_setup:bool=False\n",
    "    callback:DoneStateEnv=field(default_factory=lambda:DoneStateEnv(True,None,None))\n",
    "    items:List[gym.Env]=field(default_factory=list)\n",
    "\n",
    "    def setup(self,items=None,train_setup=False):\n",
    "        for o in items:self.items.append(o)\n",
    "        self._env_idx=len(items)\n",
    "        return self \n",
    "    \n",
    "    def reset(self):\n",
    "        self._env_idx=0\n",
    "        if self.seed is not None:[o.seed(self.seed) for o in self.items]\n",
    "        self.s=[_o.reset() for _o in self.items]\n",
    "        self.d=[False for _ in self.items]\n",
    "        self.callback=DoneStateEnv(d=self.d[self._env_idx],s=self.s[self._env_idx],env=self.items[self._env_idx])\n",
    "    \n",
    "    def encodes(self,o:int):        \n",
    "        if self.callback.d:\n",
    "#             print('was done lol')\n",
    "            self._env_idx+=1\n",
    "            if self._env_idx>=len(self.items):self.reset()\n",
    "            self.callback=DoneStateEnv(d=self.d[self._env_idx],s=self.s[self._env_idx],env=self.items[self._env_idx])\n",
    "#             print(self.callback.s)\n",
    "        return self.callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `_env_idx` is incremented per iter. The intention is that a generating `Transform` uses the value to generate many elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs=5\n",
    "tl=EnvLists(['CartPole-v1' for _ in range(n_envs)],tfms=[EnvMakeTfm,EnvResetTfm(seed=0)])\n",
    "\n",
    "next_inc=0\n",
    "for ii in range(50):\n",
    "    for i,o in enumerate(tl):\n",
    "        if ii==0:test_eq(str(o.s),str(np.array([-0.04456399,  0.04653909,  0.01326909, -0.02099827])))\n",
    "        test_eq(tl.tfms[1]._env_idx,next_inc)\n",
    "        test_eq(tl.tfms[1].items.index(o.env),next_inc)\n",
    "        test_eq(str(o),str((False,np.array([-0.04456399,  0.04653909,  0.01326909, -0.02099827]),tl.items[0])))\n",
    "    o.d=True\n",
    "    next_inc=next_inc+1 if next_inc<n_envs-1 else 0\n",
    "# for o in tl:test_eq(str(o),str((False,np.array([-0.04456399,  0.04653909,  0.01326909, -0.02099827]),tl.items[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnvGenUnwrapTfm\n",
    "> Unwraps generators into 1d lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class EnvGenUnwrapTfm(Transform):\n",
    "    step:int=0\n",
    "    \n",
    "    def encodes(self,o:Generator): \n",
    "        result= tuple([xx for x in o for xx in x]) \n",
    "        self.step+=1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnvStepTfm\n",
    "\n",
    "> Handles stepping through environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export    \n",
    "def env_display(env:gym.Env):\n",
    "    img=env.render('rgb_array')\n",
    "    try:display.clear_output(wait=True)\n",
    "    except AttributeError:pass\n",
    "    new_im=PIL.Image.fromarray(img)\n",
    "    display.display(new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class Experience:d:bool;s:np.ndarray;sp:np.ndarray;r:float;a:Any\n",
    "\n",
    "@dataclass\n",
    "class EnvStepTfm(Transform):\n",
    "    agent:Optional[object]=None;constant_action:Optional[int]=None;n_steps:int=1;steps_delta:int=1;step:int=-1\n",
    "    _reset_step:bool=False;history:deque=None;display:bool=False;enc_set:bool=False\n",
    "    \n",
    "    def __post_init__(self):     self.history=deque(maxlen=self.n_steps)\n",
    "    def bump_step_count(self,d): self.step=0 if d else self.step+1\n",
    "    def reset(self):             self.step=0; self.history.clear()\n",
    "        \n",
    "    def encodes(self,o:DoneStateEnv):\n",
    "        while True:\n",
    "            if self.agent is None: a=ifnone(self.constant_action,o.env.action_space.sample()) \n",
    "            else:                  a=self.agent(o.s)\n",
    "                \n",
    "            sp,r,o.d,_=o.env.step(a)\n",
    "            if self.display:env_display(o.env)\n",
    "            self.history.append(Experience(d=o.d,s=o.s.copy(),sp=sp.copy(),r=r,a=a))\n",
    "            o.s=sp.copy()\n",
    "            self.bump_step_count(o.d)\n",
    "            if o.d:\n",
    "                while len(self.history)>1: # We allow the a single element left to be yielded by the broken while\n",
    "                    yield tuple(self.history)\n",
    "                    self.history.popleft()\n",
    "                break\n",
    "#             print(len(self.history),((self.step-1)%self.steps_delta),self.step)\n",
    "            if len(self.history)==self.n_steps and ((self.step-1)%self.steps_delta)==0 and not o.d or \\\n",
    "                 (len(self.history)==self.step and len(self.history)==self.n_steps and not o.d):break\n",
    "            \n",
    "        history=tuple(copy(self.history))\n",
    "        if o.d:self.history.clear();self.reset()\n",
    "        yield tuple(history)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_env(env_name,a,n_steps,steps_delta,n_envs,env_steps,dones,max_iter=800,n_episodes_break=-1,\n",
    "                 initial_s=str(np.array([-0.58912799 , 0.        ])),\n",
    "                 final_s=str(np.array([-0.71048047,  0.00427297]))):\n",
    "    tl=EnvLists([env_name for _ in range(n_envs)],tfms=[EnvMakeTfm,EnvResetTfm(seed=0),\n",
    "                                                        EnvStepTfm(constant_action=a,n_steps=n_steps,steps_delta=steps_delta,display=False),EnvGenUnwrapTfm])\n",
    "    print('Starting loop')\n",
    "    count=0\n",
    "    for k in range(n_envs):\n",
    "        print('\\n\\n')\n",
    "        for i,o in enumerate(tl):\n",
    "#             print(count,o)\n",
    "            count+=1\n",
    "#             print(o,i)\n",
    "            if i==0:test_eq(str(o[0].s),initial_s)\n",
    "            for exp in o:test_eq(exp.a,a)\n",
    "            \n",
    "            if any(_.d for _ in o):break\n",
    "        if any(_.d for _ in o):\n",
    "            test_eq(str(o[-1].sp),final_s)\n",
    "            dones+=1\n",
    "            if count>round((env_steps*n_envs)/steps_delta)-10:break\n",
    "                \n",
    "    \n",
    "#     test_eq(count-(steps_delta!=1)*n_envs+((n_steps-1)*n_envs),round((env_steps*n_envs)/steps_delta))\n",
    "    # If both are changed, the env will loop extra times at the start\n",
    "    extra_steps_on_start=0 if n_steps==1 or steps_delta==1 else min((steps_delta,n_steps))-1\n",
    "    \n",
    "    \n",
    "    test_eq(count-(steps_delta!=1)*n_envs+((n_steps-1)*n_envs),round((env_steps*n_envs)/steps_delta)+extra_steps_on_start)\n",
    "    test_eq(n_envs,dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `MountainCar-v0` with `steps_delta=2` has single episode lengths cut in half. Verify that each episode keeps its `done` signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_env(\n",
    "    env_name='MountainCar-v0',\n",
    "    a=0,\n",
    "    n_steps=1,\n",
    "    steps_delta=2,\n",
    "    n_envs=1,\n",
    "    env_steps=200,\n",
    "    dones=0,\n",
    "    n_episodes_break=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `MountainCar-v0` at defaults, runs the full 200 iterations.\n",
    "Check that all actions are `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_env(\n",
    "    env_name='MountainCar-v0',\n",
    "    a=0,\n",
    "    n_steps=1,\n",
    "    steps_delta=1,\n",
    "    n_envs=1,\n",
    "    env_steps=200,\n",
    "    dones=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `MountainCar-v0` at defaults, with 2 envs, fully resets between episodes, and that the starting and ending states are always output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_env(\n",
    "    env_name='MountainCar-v0',\n",
    "    a=0,\n",
    "    n_steps=1,\n",
    "    steps_delta=1,\n",
    "    n_envs=2,\n",
    "    env_steps=200,\n",
    "    dones=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `MountainCar-v0` with `n_steps=2` has single episode iterations is cut in half. Verify that each episode keeps its `done` signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_env(\n",
    "    env_name='MountainCar-v0',\n",
    "    a=0,\n",
    "    n_steps=2,\n",
    "    steps_delta=1,\n",
    "    n_envs=1,\n",
    "    env_steps=200,\n",
    "    dones=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `CartPole-v1` iterates correctly also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "dones=0\n",
    "env_steps=10\n",
    "steps_delta=1\n",
    "n_steps=2\n",
    "n_envs=1\n",
    "\n",
    "tl=EnvLists(['CartPole-v0' for _ in range(n_envs)],tfms=[EnvMakeTfm,EnvResetTfm(seed=0),\n",
    "                                                    EnvStepTfm(constant_action=0,n_steps=n_steps,steps_delta=steps_delta,display=True),EnvGenUnwrapTfm])\n",
    "print('Starting loop')\n",
    "for k in range(n_envs):\n",
    "    print('\\n\\n')\n",
    "    for i,o in enumerate(tl):\n",
    "#             print(count,o)\n",
    "        count+=1\n",
    "#             print(o,i)\n",
    "        if i==0:test_eq(str(o[0].s),'[-0.04456399  0.04653909  0.01326909 -0.02099827]')\n",
    "        for exp in o:test_eq(exp.a,0)\n",
    "\n",
    "        if any(_.d for _ in o):break\n",
    "    if any(_.d for _ in o):\n",
    "        test_eq(str(o[-1].sp),'[-0.17695373 -1.71499924  0.22743892  2.78917835]')\n",
    "        dones+=1\n",
    "        if count>round((env_steps*n_envs)/steps_delta)-10:break\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperienceToDictTfm(Transform):\n",
    "    def encodes(self,o:Experience):\n",
    "        print('out pooting ',o)\n",
    "        return asdict(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# DataBlock \n",
    "\n",
    "> Generates DataBlock appropriate for running OpenAI envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doer(o):print('Itemm dorer',o)\n",
    "def NoopSplitter(o):return [o]\n",
    "    \n",
    "class TestDataBlock(DataBlock):\n",
    "\n",
    "    def datasets(self, source, verbose=False):\n",
    "        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n",
    "        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n",
    "        splits = (self.splitter or RandomSplitter())(items)\n",
    "        print('splits',splits,items)\n",
    "        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n",
    "        return Datasets(None, tls=[EnvLists(items,tfms=self.item_tfms,splits=[[0]])], \n",
    "                        splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n",
    "    \n",
    "    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
    "        pv(source,verbose)\n",
    "        dsets = self.datasets(source)\n",
    "        for dset in dests:\n",
    "        print('Splits: ',dsets.splits[0])\n",
    "        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "        return dsets.dataloaders(path=path,after_batch=self.batch_tfms,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=block.datasets(['MountainCar-v0' for _ in range(1)]);ds.tls[0].splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block=TestDataBlock(splitter=NoopSplitter,item_tfms=[EnvMakeTfm,EnvResetTfm(seed=0),\n",
    "                                                 EnvStepTfm(constant_action=0,n_steps=n_steps,steps_delta=steps_delta,display=False),\n",
    "                                                 EnvGenUnwrapTfm,\n",
    "                                                 ExperienceToDictTfm])\n",
    "dls=DataLoaders.from_dblock(block,['MountainCar-v0' for _ in range(1)],bs=20,verbose=True,shuffle_train=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls[0].do_batch([dls[0].do_item(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls[0].dataset.tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls[0].dataset.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls[0].dataset.tls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls[0].dataset.tls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dls[0]:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
