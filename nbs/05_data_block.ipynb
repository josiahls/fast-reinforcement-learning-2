{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp data_block\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Block\n",
    "\n",
    "> Primary handlers for interfacing the openai gym envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.callbacks import *\n",
    "from fastrl.wrappers import *\n",
    "from fastrl.basic_agents import *\n",
    "from dataclasses import asdict\n",
    "from functools import partial\n",
    "from fastprogress.fastprogress import IN_NOTEBOOK\n",
    "from fastcore.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "from queue import Empty\n",
    "import textwrap\n",
    "import logging\n",
    "import gym\n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s line:%(lineno)d %(levelname)s - %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "_logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "_logger.setLevel('INFO')\n",
    "from fastcore.foundation import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "`Dataset` instances are going to be a little different from the typically classification dataset that you might use in pytorch. Commonly, datasets have:\n",
    "- A known size to iter through\n",
    "- Maintain their state during the training sequence\n",
    "- Randomly sample their dataset\n",
    "- Have a common `x`/`y` or `input`/`target` data format\n",
    "\n",
    "For our `ExperienceSourceDataset`, most of this is going to be different. \n",
    "- We can have multiple sources (envs)\n",
    "\n",
    "You could think of a traditional dataset approach as being a mix of a `ExperienceSourceDataset` and a form of `ExperienceReplay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fix_s(x):\n",
    "    \"Flatten `x` to `(1,-1)` where `1` is the batch dim (B) unless **seems** to be an image e.g. has 3 dims (W,H,C), then it will attempt (B,W,H,C).\"\n",
    "    return (x if x.shape[0]==1 else \n",
    "            x.reshape(1,-1) if len(x.shape)==2 else\n",
    "            np.expand_dims(x,0))\n",
    "\n",
    "@dataclass\n",
    "class Experience(object):\n",
    "    s:np.array\n",
    "    sp:np.array\n",
    "    a:np.array\n",
    "    r:np.array\n",
    "    d:np.array\n",
    "    agent_s:np.array\n",
    "        \n",
    "    # TODO possibly have x,y for more generic experience integration with datasets.\n",
    "        \n",
    "#     def __post_init__(self):\n",
    "#         for k,v in asdict(self).items():\n",
    "#             setattr(self,k,fix_s(v) if k in ['s','sp'] else np.array(v,dtype=float).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': array([[2, 2, 2, 2, 1],\n",
       "        [1, 1, 2, 1, 1],\n",
       "        [2, 1, 2, 1, 1],\n",
       "        [1, 2, 1, 2, 1],\n",
       "        [2, 2, 1, 1, 1]]),\n",
       " 'sp': array([[1, 2, 2, 1, 2],\n",
       "        [2, 2, 2, 2, 1],\n",
       "        [2, 2, 2, 1, 1],\n",
       "        [2, 1, 2, 2, 1],\n",
       "        [1, 1, 2, 2, 2]]),\n",
       " 'a': array([[1, 2]]),\n",
       " 'r': array([[1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2]]),\n",
       " 'd': array([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]),\n",
       " 'agent_s': array([[4, 1, 2, 3, 1]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp=Experience(s=np.random.randint(1,3,(5,5)),\n",
    "               sp=np.random.randint(1,3,(5,5)),\n",
    "               a=np.random.randint(1,3,(1,2)),\n",
    "               r=np.random.randint(1,3,(1,20)),\n",
    "               d=np.random.randint(0,1,(5,5)),\n",
    "               agent_s=np.random.randint(0,6,(1,5)))\n",
    "asdict(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExperienceSourceCallback(LearnerCallback):\n",
    "    def on_train_begin(self,*args,**kwargs):\n",
    "        self.learn.data.train_dl.dataset.learn=self.learn\n",
    "        if not self.learn.data.empty_val:\n",
    "            self.learn.data.valid_dl.dataset.learn=self.learn\n",
    "\n",
    "class ExperienceSourceDataset(Dataset):\n",
    "    \"Similar to fastai's `LabelList`, iters in-order samples from `1->len(envs)` `envs`.\"\n",
    "    def __init__(self,env:str,n_envs=1,steps=1,max_episode_step=None,pixels=False):\n",
    "        def make_env():\n",
    "            env=gym.make(\"CartPole-v1\")\n",
    "            if pixels:env.reset()\n",
    "            return env\n",
    "            \n",
    "        self.envs=[make_env() for _ in range(n_envs)]\n",
    "        if pixels:self.envs=[PixelObservationWrapper(e) for e in self.envs]\n",
    "        self.steps=steps\n",
    "        self.max_episode_step=max_episode_step\n",
    "        self.d=np.zeros((len(self.envs),))+1\n",
    "        self.s=np.zeros((len(self.envs),*self.envs[0].observation_space.sample().shape))\n",
    "        self.iterations=np.zeros((len(self.envs)))\n",
    "        self.total_r=[]\n",
    "        self.total_steps=[]\n",
    "        self.learn=None\n",
    "        self._warned=False\n",
    "        self.callback_fns=[ExperienceSourceCallback]\n",
    "        self.inc=0\n",
    "        \n",
    "    def __len__(self): return ifnone(self.max_episode_step,self.envs[0].spec.max_episode_steps)*len(self.envs)\n",
    "    \n",
    "    def get_action(self,idx):\n",
    "        if self.learn is None:\n",
    "            if not self._warned:_logger.warning('`self.learn` is None. will use random actions instead.')\n",
    "            self._warned=True\n",
    "            return self.envs[0].action_space.sample(),np.zeros((1,1))\n",
    "        return self.learn.predict(self.s[idx])\n",
    "    \n",
    "    @log_args\n",
    "    def __getitem__(self,_):\n",
    "        idx=self.inc\n",
    "        _logger.debug('Idx:%s,Iter:%s',idx,self.iterations[idx])\n",
    "        if idx==0 and self.iterations[idx]==0:\n",
    "            for i,e in enumerate(self.envs): self.s[i]=e.reset() # There is the possiblity this will equal None (and crash)?\n",
    "            self.iterations=np.zeros((len(self.envs)),dtype=int)\n",
    "        \n",
    "        exps:List[Experience]=[]\n",
    "        while True:\n",
    "            a,agent_s=self.get_action(idx)\n",
    "            sp,r,self.d[idx],_=self.envs[idx].step(a)\n",
    "            self.total_r.append(r)\n",
    "            self.total_steps.append(self.iterations[idx])\n",
    "            exps.append(Experience(self.s[idx],sp,a,r,self.d[idx],agent_s=ifnone(agent_s,[])))\n",
    "            self.s[idx]=sp\n",
    "            self.iterations[idx]+=1\n",
    "            if self.d[idx] or self.iterations[idx]>=len(self)-1:\n",
    "                self.iterations[idx]=0\n",
    "                if self.inc>=len(self.envs)-1:self.inc=0\n",
    "                else:                       self.inc+=1\n",
    "                if idx>=len(self.envs)-1:raise StopIteration()\n",
    "                break\n",
    "            if len(exps)%self.steps==0:\n",
    "                break\n",
    "        return [e.s for e in exps],[asdict(e) for e in exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 18:41:38] p324 line:34 WARNING - `self.learn` is None. will use random actions instead.\n",
      "[07-16 18:41:38] p324 line:6 CRITICAL - xb: torch.Size([15, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:6 CRITICAL - xb: torch.Size([15, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:6 CRITICAL - xb: torch.Size([12, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:6 CRITICAL - xb: torch.Size([15, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:34 WARNING - `self.learn` is None. will use random actions instead.\n",
      "[07-16 18:41:38] p324 line:16 CRITICAL - xb: torch.Size([25, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:16 CRITICAL - xb: torch.Size([25, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:16 CRITICAL - xb: torch.Size([20, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:16 CRITICAL - xb: torch.Size([5, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:16 CRITICAL - xb: torch.Size([25, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:38] p324 line:16 CRITICAL - xb: torch.Size([10, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# make_env = lambda: gym.make(\"Acrobot-v1\")\n",
    "ds=ExperienceSourceDataset(\"CartPole-v1\",n_envs=3,max_episode_step=50,steps=5)\n",
    "dl=DataLoader(ds,batch_size=3,num_workers=0)\n",
    "for xb,yb in dl:\n",
    "    _logger.critical('xb: %s, yb: %s',torch.cat(xb).shape,yb[0].keys())\n",
    "    assert torch.cat(xb).shape[0]<=5*3\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# make_env = lambda: gym.make(\"Acrobot-v1\")\n",
    "make_env = lambda: gym.make(\"CartPole-v1\")\n",
    "envs=[make_env() for _ in range(5)]\n",
    "ds=ExperienceSourceDataset(\"CartPole-v1\",n_envs=5,max_episode_step=50,steps=5)\n",
    "dl=DataLoader(ds,batch_size=5,num_workers=0)\n",
    "for xb,yb in dl:\n",
    "    _logger.critical('xb: %s, yb: %s',torch.cat(xb).shape,yb[0].keys())\n",
    "    assert torch.cat(xb).shape[0]<=5*5\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 18:41:39] p324 line:34 WARNING - `self.learn` is None. will use random actions instead.\n",
      "[07-16 18:41:39] p324 line:4 CRITICAL - xb: torch.Size([15, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:39] p324 line:4 CRITICAL - xb: torch.Size([15, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:40] p324 line:4 CRITICAL - xb: torch.Size([15, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:40] p324 line:4 CRITICAL - xb: torch.Size([15, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:40] p324 line:4 CRITICAL - xb: torch.Size([15, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:40] p324 line:4 CRITICAL - xb: torch.Size([3, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:40] p324 line:4 CRITICAL - xb: torch.Size([15, 400, 600, 3]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n"
     ]
    }
   ],
   "source": [
    "ds=ExperienceSourceDataset('CartPole-v1',n_envs=3,max_episode_step=50,steps=5,pixels=True)\n",
    "dl=DataLoader(ds,batch_size=3,num_workers=0)\n",
    "for xb,yb in dl:\n",
    "    _logger.critical('xb: %s, yb: %s',torch.cat(xb).shape,yb[0].keys())\n",
    "    assert torch.cat(xb).shape[0]<=5*3\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FirstLastExperienceSourceDataset(ExperienceSourceDataset):\n",
    "    \"Similar to `ExperienceSourceDataset` but only keeps the first and last parts of a step. Can be seen as frame skipping.\"\n",
    "    @log_args\n",
    "    def __getitem__(self,idx):\n",
    "        s,exps=super(FirstLastExperienceSourceDataset,self).__getitem__(idx)\n",
    "        exp=exps[-1]\n",
    "        exp['s']=exps[0]['s']\n",
    "        return [exps[0]['s']],[exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 18:41:41] p324 line:34 WARNING - `self.learn` is None. will use random actions instead.\n",
      "[07-16 18:41:41] p324 line:5 CRITICAL - xb: torch.Size([3, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:41] p324 line:5 CRITICAL - xb: torch.Size([3, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:41] p324 line:5 CRITICAL - xb: torch.Size([3, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:41] p324 line:5 CRITICAL - xb: torch.Size([3, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n",
      "[07-16 18:41:41] p324 line:5 CRITICAL - xb: torch.Size([3, 4]), yb: dict_keys(['s', 'sp', 'a', 'r', 'd', 'agent_s'])\n"
     ]
    }
   ],
   "source": [
    "ds=FirstLastExperienceSourceDataset(\"CartPole-v1\",n_envs=3,max_episode_step=50,steps=5)\n",
    "dl=DataLoader(ds,batch_size=3,num_workers=0)\n",
    "index=0\n",
    "for xb,yb in dl:\n",
    "    _logger.critical('xb: %s, yb: %s',torch.cat(xb).shape,yb[0].keys())\n",
    "    assert torch.cat(xb).shape[0]<=3\n",
    "    sys.stdout.flush()\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AsyncExperienceSourceCallback(LearnerCallback):\n",
    "    _order = -11\n",
    "    \n",
    "    def on_epoch_begin(self,**kwargs):\n",
    "        ds=(self.learn.data.train_ds if self.learn.model.training or self.learn.data.empty_val else\n",
    "            self.learn.data.valid_ds)\n",
    "        if not self.learn.data.empty_val:ds.pause_event.clear()\n",
    "\n",
    "        if len(ds.data_proc_list)==0:\n",
    "            if not hasattr(self.learn,'fitter'):\n",
    "                _logger.warning('Using the default fitter function which will likely not work. Make sure your `AgentLearner` has a `fitter` attribute to actually run/train.')\n",
    "            \n",
    "            for proc_idx in range(ds.n_processes):\n",
    "                _logger.info('Starting Process')\n",
    "                data_proc=self.load_process()\n",
    "                data_proc.start()\n",
    "                ds.data_proc_list.append(data_proc)\n",
    "                \n",
    "    def load_process(self):raise NotImplementedError()\n",
    "    def empty_queues(self):raise NotImplementedError()\n",
    "\n",
    "    def on_batch_end(self,**kwargs):\n",
    "        # If not training, pause train ds, otherwise pause valid ds\n",
    "        ds=(self.learn.data.train_ds if not self.learn.model.training or self.learn.data.empty_val else\n",
    "            self.learn.data.valid_ds)\n",
    "        if not self.learn.data.empty_val:ds.pause_event.set()\n",
    "    \n",
    "    def on_train_end(self,**kwargs):\n",
    "        for ds in [self.learn.data.train_ds,None if self.learn.data.empty_val else self.learn.data.valid_ds]:\n",
    "            if ds is None: continue\n",
    "            ds.cancel_event.set()\n",
    "            self.empty_queues()\n",
    "            for proc in ds.data_proc_list: proc.join()\n",
    "                \n",
    "class AsyncGradExperienceSourceCallback(AsyncExperienceSourceCallback):\n",
    "    def load_process(self):\n",
    "        ds=(self.learn.data.train_ds if self.learn.model.training or self.learn.data.empty_val else\n",
    "            self.learn.data.valid_ds)\n",
    "        return mp.Process(target=getattr(self.learn,'fitter',grad_fitter), \n",
    "                          args=(self.learn.model,self.learn.agent,ds.ds_cls),\n",
    "                          kwargs={'grad_queue':ds.grad_queue,'loss_queue':ds.loss_queue,'pause_event':ds.pause_event,'cancel_event':ds.cancel_event})\n",
    "    def empty_queues(self):\n",
    "        ds=(self.learn.data.train_ds if self.learn.model.training or self.learn.data.empty_val else\n",
    "            self.learn.data.valid_ds)\n",
    "        while not ds.grad_queue.empty(): ds.grad_queue.get()\n",
    "        while not ds.loss_queue.empty(): ds.loss_queue.get()\n",
    "\n",
    "class AsyncDataExperienceSourceCallback(AsyncExperienceSourceCallback):\n",
    "    def load_process(self):\n",
    "        ds=(self.learn.data.train_ds if self.learn.model.training or self.learn.data.empty_val else\n",
    "            self.learn.data.valid_ds)\n",
    "        return mp.Process(target=getattr(self.learn,'fitter',data_fitter), \n",
    "                          args=(self.learn.model,self.learn.agent,ds.ds_cls),\n",
    "                          kwargs={'data_queue':ds.data_queue,'pause_event':ds.pause_event,'cancel_event':ds.cancel_event})\n",
    "    def empty_queues(self):\n",
    "        ds=(self.learn.data.train_ds if self.learn.model.training or self.learn.data.empty_val else\n",
    "            self.learn.data.valid_ds)\n",
    "        while not ds.data_queue.empty():ds.data_queue.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def safe_fit(f):\n",
    "    def wrap(*args,cancel_event,**kwargs):\n",
    "        try:\n",
    "            return f(*args,cancel_event=cancel_event,**kwargs)\n",
    "        finally:\n",
    "            cancel_event.set()\n",
    "            for k,v in kwargs.items():\n",
    "                if k.__contains__('queue'):v.put(None)\n",
    "            return None\n",
    "    return wrap\n",
    "\n",
    "def grad_fitter(model:nn.Module,agent:BaseAgent,ds:ExperienceSourceDataset,grad_queue:mp.JoinableQueue,\n",
    "                loss_queue:mp.JoinableQueue,pause_event:mp.Event,cancel_event:mp.Event):\n",
    "    \"Updates a `train_queue` with `model.parameters()` and `loss_queue` with the loss. Note that this is only an example grad_fitter.\"\n",
    "    while not cancel_event.is_set(): # We are expecting the  grad_fitter to loop unless cancel_event is set\n",
    "        grad_queue.put(None)         # Adding `None` to `train_queue` will trigger an eventual ending of training\n",
    "        loss_queue.put(None)\n",
    "        if pause_event.is_set():     # There needs to be the ability for the grad_fitter to pause e.g. if waiting for validation to end.\n",
    "            cancel_event.wait(0.1)   # Using cancel_event to wait allows the main process to end this Process.\n",
    "\n",
    "def data_fitter(model:nn.Module,agent:BaseAgent,ds:ExperienceSourceDataset,data_queue:mp.JoinableQueue,\n",
    "                pause_event:mp.Event,cancel_event:mp.Event):\n",
    "    _logger.warning('Using the `test_fitter` function. Make sure your `AgentLearner` has a `grad_fitter` to actually run/train.')\n",
    "    while not cancel_event.is_set(): # We are expecting the  grad_fitter to loop unless cancel_event is set\n",
    "        data_queue.put(None)         # Adding `None` to `train_queue` will trigger an eventual ending of training\n",
    "        if pause_event.is_set():     # There needs to be the ability for the grad_fitter to pause e.g. if waiting for validation to end.\n",
    "            cancel_event.wait(0.1)   # Using cancel_event to wait allows the main process to end this Process.     \n",
    "            \n",
    "def _soft_queue_get(q:mp.Queue,e:mp.Event):\n",
    "    entry=None\n",
    "    while entry is None and not e.is_set():\n",
    "        try:\n",
    "            entry=q.get_nowait()\n",
    "        except Empty:pass\n",
    "    return entry\n",
    "            \n",
    "class AsyncGradExperienceSourceDataset(ExperienceSourceDataset):\n",
    "    \"Contains dataloaders of multiple sub-datasets and executes them using `n_processes`. `xb` is the gradients from the agents, `yb` is the loss.\"\n",
    "    def __init__(self,env_name:str,n_envs=1,ds_cls=ExperienceSourceDataset,max_episode_step=None,n_processes=1,*args,**kwargs):\n",
    "        self.n_processes=n_processes\n",
    "        self.n_envs=n_envs\n",
    "        self.env_name=env_name\n",
    "        self.ds_cls=ds_cls\n",
    "        self.pause_event=mp.Event()                               # If the event is set, then the Process will freeze.\n",
    "        self.cancel_event=mp.Event()                              # If the event is set, then the Process will freeze.\n",
    "        self.max_episode_step=max_episode_step\n",
    "        self.grad_queue=mp.JoinableQueue(maxsize=self.n_processes)\n",
    "        self.loss_queue=mp.JoinableQueue(maxsize=self.n_processes)\n",
    "        self.data_proc_list=[]\n",
    "        self.callback_fns=[AsyncGradExperienceSourceCallback]\n",
    "        self._env=gym.make(self.env_name)\n",
    "    \n",
    "    def __len__(self): return ifnone(self.max_episode_step,self._env.spec.max_episode_steps)*self.n_envs\n",
    "    \n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if len(self.data_proc_list)==0: raise StopIteration()\n",
    "        train_entry=_soft_queue_get(self.grad_queue,self.cancel_event)\n",
    "\n",
    "        if train_entry is None:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        train_loss_entry=_soft_queue_get(self.loss_queue,self.cancel_event)\n",
    "        return train_entry,[train_loss_entry]     \n",
    "    \n",
    "class AsyncDataExperienceSourceDataset(ExperienceSourceDataset):\n",
    "    \"Contains dataloaders of multiple sub-datasets and executes them using `n_processes`. `xb` is the gradients from the agents, `yb` is the loss.\"\n",
    "    def __init__(self,env_name:str,n_envs=1,ds_cls=ExperienceSourceDataset,max_episode_step=None,n_processes=1,*args,**kwargs):\n",
    "        self.n_processes=n_processes\n",
    "        self.n_envs=n_envs\n",
    "        self.ds_cls=ds_cls\n",
    "        self.env_name=env_name\n",
    "        self.pause_event=mp.Event()                               # If the event is set, then the Process will freeze.\n",
    "        self.cancel_event=mp.Event()                              # If the event is set, then the Process will freeze.\n",
    "        self.max_episode_step=max_episode_step\n",
    "        self.data_queue=mp.JoinableQueue(maxsize=self.n_processes)\n",
    "        self.data_proc_list=[]\n",
    "        self.callback_fns=[AsyncDataExperienceSourceCallback]\n",
    "        self._env=gym.make(self.env_name)\n",
    "        \n",
    "    def __len__(self): return ifnone(self.max_episode_step,self._env.spec.max_episode_steps)*self.n_envs\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if len(self.data_proc_list)==0: raise StopIteration()\n",
    "        train_entry=_soft_queue_get(self.data_queue,self.cancel_event)\n",
    "\n",
    "        if train_entry is None:\n",
    "            raise StopIteration()\n",
    "\n",
    "        return [e['s'] for e in train_entry],train_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(AsyncGradExperienceSourceDataset,\n",
    "         __init__=textwrap.fill(\"\"\"The `AsyncGradExperienceSourceDataset` class is instantiated via passing \n",
    "         the `env_name` that we want to train on, and a `partial` class of a `ExperienceSourceDataset` called `ds_cls`.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Shower\n",
    "We can define a wrapper around a dataset which will show up to `rows * cols` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DatasetDisplayWrapper(object):\n",
    "    def __init__(self,ds,rows=2,cols=2,max_w=800):\n",
    "        \"Wraps a ExperienceSourceDataset instance showing multiple envs in a `rows` by `cols` grid in a Jupyter notebook.\"\n",
    "        # Ref: https://stackoverflow.com/questions/1443129/completely-wrap-an-object-in-python\n",
    "        # We are basically Wrapping any instance of ExperienceSourceDataset (kind of cool right?)\n",
    "        assert issubclass(ds.__class__,(ExperienceSourceDataset,FirstLastExperienceSourceDataset)),'Currently this only works with the ExperienceSourceDataset class only.'\n",
    "        self.__class__ = type(ds.__class__.__name__,(self.__class__, ds.__class__),{})\n",
    "        self.__dict__=ds.__dict__\n",
    "        self.rows,self.cols,self.max_w=rows,cols,max_w\n",
    "        self.current_display=None\n",
    "        if not IN_NOTEBOOK: \n",
    "            _logger.warning('It seems you are not running in a notebook. Nothing is going to be displayed.')\n",
    "            return\n",
    "        \n",
    "        if self.envs[0].render('rgb_array') is None: self.envs[0].reset()\n",
    "        rdr=self.envs[0].render('rgb_array')\n",
    "        if rdr.shape[1]*self.cols>max_w:\n",
    "            _logger.warning('Max Width is %s but %s*%s is greater than. Decreasing the number of cols to %s, rows increase by %s',\n",
    "                            max_w,rdr.shape[1],self.cols,max_w%rdr.shape[1],max_w%rdr.shape[1])\n",
    "            self.cols=max_w%rdr.shape[1]\n",
    "            self.rows+=max_w%rdr.shape[1]\n",
    "\n",
    "        self.current_display=np.zeros(shape=(self.rows*rdr.shape[0],self.cols*rdr.shape[1],rdr.shape[2])).astype('uint8')\n",
    "        _logger.info('%s, %s, %s, %s, %s',0,0//self.cols,0%self.cols,rdr.shape,self.current_display.shape)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        o=super(DatasetDisplayWrapper,self).__getitem__(idx)\n",
    "        idx=idx%len(self.envs)\n",
    "        if self.current_display is not None and idx<self.rows*self.cols:\n",
    "            display.clear_output(wait=True)\n",
    "            im=self.envs[idx].render(mode='rgb_array')\n",
    "            self.current_display[(idx//self.cols)*im.shape[0]:(idx//self.cols)*im.shape[0]+im.shape[0],\n",
    "                                 (idx%self.cols)*im.shape[1]:(idx%self.cols)*im.shape[1]+im.shape[1],:]=im\n",
    "            new_im=PIL.Image.fromarray(self.current_display)\n",
    "            display.display(new_im)\n",
    "        else:\n",
    "            display.display(PIL.Image.fromarray(self.current_display))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExperienceSourceDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def from_env(cls,env:str,n_envs=1,firstlast=False,display=False,max_steps=None,skip_step=1,path:PathOrStr='.',add_valid=True,\n",
    "                 cols=1,rows=1,max_w=800):\n",
    "        def create_ds():\n",
    "            _ds_cls=FirstLastExperienceSourceDataset if firstlast else ExperienceSourceDataset\n",
    "            make_env = lambda: gym.make(env)\n",
    "            envs=[make_env() for _ in range(n_envs)]\n",
    "            _ds=_ds_cls(envs,max_episode_step=max_steps,steps=skip_step)\n",
    "            if display:_ds=DatasetDisplayWrapper(_ds,cols=cols,rows=rows,max_w=max_w)\n",
    "            return _ds\n",
    "            \n",
    "        dss=(create_ds(),create_ds() if add_valid else None)\n",
    "        return cls.create(*dss,bs=n_envs,num_workers=0)\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, train_ds:Dataset, valid_ds:Dataset, test_ds:Optional[Dataset]=None, path:PathOrStr='.', bs:int=64,\n",
    "               val_bs:int=None, num_workers:int=defaults.cpus, dl_tfms:Optional[Collection[Callable]]=None,\n",
    "               device:torch.device=None, collate_fn:Callable=data_collate, no_check:bool=False, **dl_kwargs)->'DataBunch':\n",
    "        \"Create a `DataBunch` from `train_ds`, `valid_ds` and maybe `test_ds` with a batch size of `bs`. Passes `**dl_kwargs` to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        dls = [DataLoader(d, b, shuffle=s, drop_last=s, num_workers=num_workers, **dl_kwargs) for d,b,s in\n",
    "               zip(datasets, (bs,val_bs,val_bs,val_bs), (False,False,False,False)) if d is not None]\n",
    "        return cls(*dls, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 19:49:21] p324 line:34 WARNING - `self.learn` is None. will use random actions instead.\n"
     ]
    }
   ],
   "source": [
    "data=ExperienceSourceDataBunch.from_env('CartPole-v1',n_envs=5,display=False,firstlast=False,add_valid=False)\n",
    "for xb,yb in data.train_dl:\n",
    "    test_eq(len(xb),1)\n",
    "    test_eq(tuple(xb[0].shape),(5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 19:49:22] p324 line:34 WARNING - `self.learn` is None. will use random actions instead.\n"
     ]
    }
   ],
   "source": [
    "data=ExperienceSourceDataBunch.from_env('CartPole-v1',n_envs=5,display=False,firstlast=True,add_valid=False)\n",
    "for xb,yb in data.train_dl:\n",
    "    test_eq(len(xb),1)\n",
    "    test_eq(tuple(xb[0].shape),(5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.basic_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AsyncExperienceSourceDataBunch(ExperienceSourceDataBunch):\n",
    "    @classmethod\n",
    "    def from_env(cls,env:str,n_envs=1,data_exp=True,firstlast=False,display=False,max_steps=None,skip_step=1,path:PathOrStr='.',add_valid=True,\n",
    "                 cols=1,rows=1,max_w=800,n_processes=1):\n",
    "        def create_ds():\n",
    "            _sub_ds_cls=FirstLastExperienceSourceDataset if firstlast else ExperienceSourceDataset\n",
    "            _sub_ds_cls=partial(_sub_ds_cls,env=env,n_envs=1,max_episode_step=max_steps,steps=skip_step)\n",
    "            _ds_cls=AsyncDataExperienceSourceDataset if data_exp else AsyncGradExperienceSourceDataset\n",
    "            _ds=_ds_cls(env,max_episode_step=max_steps,steps=skip_step,ds_cls=_sub_ds_cls,n_processes=n_processes)\n",
    "            if display:_ds=DatasetDisplayWrapper(_ds,cols=cols,rows=rows,max_w=max_w)\n",
    "            return _ds\n",
    "            \n",
    "        dss=(create_ds(),create_ds() if add_valid else None)\n",
    "        return cls.create(*dss,bs=n_envs,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@safe_fit\n",
    "def dqn_fitter(model:nn.Module,agent:BaseAgent,ds:ExperienceSourceDataset,data_queue:mp.JoinableQueue,\n",
    "                pause_event:mp.Event,cancel_event:mp.Event):\n",
    "    dataset=ds()\n",
    "    while not cancel_event.is_set(): \n",
    "        for xb,yb in dataset:\n",
    "            data_queue.put(yb)\n",
    "            if pause_event.is_set():cancel_event.wait(0.1) \n",
    "            if cancel_event.is_set():break\n",
    "                \n",
    "@safe_fit\n",
    "def dqn_grad_fitter(model:nn.Module,agent:BaseAgent,ds:ExperienceSourceDataset,grad_queue:mp.JoinableQueue,loss_queue:mp.JoinableQueue,\n",
    "                    pause_event:mp.Event,cancel_event:mp.Event):\n",
    "    dataset=ds()\n",
    "    while not cancel_event.is_set(): \n",
    "        for xb,yb in dataset:\n",
    "            sys.stdout.flush()\n",
    "            grad_queue.put(xb)\n",
    "            loss_queue.put(0.5)\n",
    "            print(xb)\n",
    "            if pause_event.is_set():cancel_event.wait(0.1) \n",
    "            if cancel_event.is_set():break\n",
    "\n",
    "@safe_fit\n",
    "def buggy_dqn_fitter(model:nn.Module,agent:BaseAgent,ds:ExperienceSourceDataset,data_queue:mp.JoinableQueue,\n",
    "                pause_event:mp.Event,cancel_event:mp.Event):\n",
    "    dataset=ds()\n",
    "    while not cancel_event.is_set(): \n",
    "        for xb,yb in dataset:\n",
    "            data_queue.put(yb)\n",
    "            if pause_event.is_set():cancel_event.wait(0.1) \n",
    "            if cancel_event.is_set():break\n",
    "            raise Exception('Crashing on purpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='33' class='' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      13.20% [33/250 00:03<00:22 0.5000]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 20:40:16] p324 line:15 INFO - Starting Process\n",
      "[07-16 20:40:16] p324 line:15 INFO - Starting Process\n",
      "[07-16 20:40:16] p4201 line:34 WARNING - `self.learn` is None. will use random actions instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.032091, -0.157941, -0.014705,  0.263347])]\n",
      "[array([-0.03525 , -0.35285 , -0.009438,  0.551356])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 20:40:16] p4204 line:34 WARNING - `self.learn` is None. will use random actions instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.043079,  0.150063,  0.043078, -0.324534])]\n",
      "[array([-0.042307, -0.547838,  0.00159 ,  0.841051])]\n",
      "[array([ 0.046081,  0.344546,  0.036587, -0.603327])]\n",
      "[array([-0.053263, -0.352738,  0.018411,  0.548868])]\n",
      "\n",
      "[array([ 0.052972,  0.539138,  0.024521, -0.884265])][array([-0.060318, -0.548114,  0.029388,  0.847295])]\n",
      "[array([ 0.063754,  0.733918,  0.006835, -1.16914 ])]\n",
      "[array([-0.07128 , -0.353405,  0.046334,  0.563996])]\n",
      "[array([ 0.078433,  0.928951, -0.016548, -1.459672])]\n",
      "[array([-0.078349, -0.158962,  0.057614,  0.286263])]\n",
      "[array([ 0.097012,  1.124271, -0.045741, -1.757478])]\n",
      "[array([-0.081528,  0.035293,  0.063339,  0.012293])]\n",
      "\n",
      "[array([ 0.119497,  0.929697, -0.080891, -1.479364])][array([-0.080822,  0.229452,  0.063585, -0.259753])]\n",
      "[array([ 0.138091,  0.73565 , -0.110478, -1.213002])]\n",
      "[array([-0.076233,  0.423611,  0.05839 , -0.531722])]\n",
      "[array([ 0.152804,  0.542113, -0.134738, -0.956881])]\n",
      "[array([-0.067761,  0.227719,  0.047755, -0.221227])]\n",
      "[array([ 0.163646,  0.349035, -0.153876, -0.709383])]\n",
      "[array([-0.063206,  0.422127,  0.043331, -0.498472])]\n",
      "[array([ 0.170627,  0.545916, -0.168063, -1.046275])]\n",
      "[array([-0.054764,  0.616612,  0.033361, -0.77719 ])]\n",
      "[array([ 0.181545,  0.742821, -0.188989, -1.38665 ])]\n",
      "[array([-0.042432,  0.421047,  0.017818, -0.4742  ])]\n",
      "[array([-0.049999,  0.171746, -0.000585, -0.294113])]\n",
      "[array([-0.034011,  0.225678,  0.008334, -0.175955])]\n",
      "[array([-0.046564,  0.366876, -0.006468, -0.586981])]\n",
      "[array([-0.029497,  0.42068 ,  0.004814, -0.465998])]\n",
      "[array([-0.039226,  0.171845, -0.018207, -0.296342])]\n",
      "[array([-0.021083,  0.22549 , -0.004505, -0.171801])]\n",
      "[array([-0.03579 , -0.023012, -0.024134, -0.009457])]\n",
      "[array([-0.016574,  0.030433, -0.007942,  0.119457])]\n",
      "[array([-0.03625 ,  0.172447, -0.024323, -0.309655])]\n",
      "[array([-0.015965, -0.164574, -0.005552,  0.409624])]\n",
      "[array([-0.032801,  0.367907, -0.030516, -0.609909])]\n",
      "[array([-0.019256,  0.030626,  0.00264 ,  0.115196])]\n",
      "[array([-0.025443,  0.563442, -0.042715, -0.912045])]\n",
      "[array([-0.018644,  0.22571 ,  0.004944, -0.176653])]\n",
      "[array([-0.014174,  0.759115, -0.060955, -1.217841])][array([-0.01413 ,  0.030518,  0.001411,  0.117586])]\n",
      "\n",
      "[array([ 1.008466e-03,  9.549679e-01, -8.531225e-02, -1.528984e+00])]\n",
      "[array([-0.013519, -0.164624,  0.003763,  0.410713])]\n",
      "[array([ 0.020108,  1.151009, -0.115892, -1.847028])]\n",
      "[array([-0.016812,  0.030444,  0.011977,  0.119219])]\n",
      "[array([ 0.043128,  1.347201, -0.152832, -2.173338])]\n",
      "[array([-0.016203, -0.164848,  0.014361,  0.415656])]\n",
      "[array([ 0.070072,  1.543447, -0.196299, -2.509033])]\n",
      "[array([-0.0195  , -0.36017 ,  0.022674,  0.712832])]\n",
      "[array([ 0.027266, -0.243392,  0.003005,  0.268869])]\n",
      "[array([-0.026703, -0.165369,  0.036931,  0.427372])]\n",
      "[array([ 0.022398, -0.048313,  0.008382, -0.022865])]\n",
      "[array([-0.030011, -0.360994,  0.045479,  0.731464])]\n",
      "[array([ 0.021432, -0.243554,  0.007925,  0.272451])]\n",
      "[array([-0.037231, -0.556714,  0.060108,  1.038107])]\n",
      "[array([ 0.016561, -0.438788,  0.013374,  0.567623])]\n",
      "[array([-0.048365, -0.752581,  0.08087 ,  1.349038])]\n",
      "[array([ 0.007785, -0.243857,  0.024727,  0.279183])]\n",
      "[array([-0.063417, -0.948621,  0.107851,  1.665886])]\n",
      "[array([ 0.002908, -0.049096,  0.03031 , -0.0056  ])]\n",
      "[array([-0.082389, -0.754907,  0.141168,  1.408649])]\n",
      "[array([ 0.001926,  0.145578,  0.030198, -0.288567])]\n",
      "[array([-0.097487, -0.951469,  0.169341,  1.741926])]\n",
      "[array([ 0.004837, -0.049961,  0.024427,  0.013485])]\n",
      "[array([-0.116516, -1.148066,  0.20418 ,  2.082152])]\n",
      "[array([ 0.003838,  0.144802,  0.024697, -0.271392])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-87726c636e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAgentLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFakeRunCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfake_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fitter'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdqn_grad_fitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-157-cf48650f3c5f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_proc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtrain_entry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_soft_queue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_queue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_entry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-157-cf48650f3c5f>\u001b[0m in \u001b[0;36m_soft_queue_get\u001b[0;34m(q, e)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mentry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nowait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget_nowait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_nowait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput_nowait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastrl/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data=AsyncExperienceSourceDataBunch.from_env('CartPole-v1',data_exp=False,display=False,firstlast=True,add_valid=False,n_processes=2,n_envs=2)\n",
    "model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2))\n",
    "agent=DQNAgent(model=model)\n",
    "learn=AgentLearner(data,model,agent=agent,callback_fns=[FakeRunCallback],loss_func=fake_loss)\n",
    "setattr(learn,'fitter',dqn_grad_fitter)\n",
    "learn.fit(1,lr=0.01,wd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 20:10:33] p324 line:15 INFO - Starting Process\n",
      "[07-16 20:10:34] p324 line:15 INFO - Starting Process\n",
      "[07-16 20:10:34] p3659 line:34 WARNING - `self.learn` is None. will use random actions instead.\n",
      "[07-16 20:10:34] p3662 line:34 WARNING - `self.learn` is None. will use random actions instead.\n"
     ]
    }
   ],
   "source": [
    "data=AsyncExperienceSourceDataBunch.from_env('CartPole-v1',display=False,firstlast=True,add_valid=False,n_processes=2,n_envs=2)\n",
    "model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2))\n",
    "agent=DQNAgent(model=model)\n",
    "learn=AgentLearner(data,model,agent=agent,callback_fns=[FakeRunCallback],loss_func=fake_loss)\n",
    "setattr(learn,'fitter',dqn_fitter)\n",
    "learn.fit(1,lr=0.01,wd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/250 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07-16 20:11:53] p324 line:15 INFO - Starting Process\n",
      "[07-16 20:11:53] p324 line:15 INFO - Starting Process\n",
      "[07-16 20:11:53] p3673 line:34 WARNING - `self.learn` is None. will use random actions instead.\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "data=AsyncExperienceSourceDataBunch.from_env('CartPole-v1',display=False,firstlast=True,add_valid=False,n_processes=2,n_envs=2)\n",
    "model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2))\n",
    "agent=DQNAgent(model=model)\n",
    "learn=AgentLearner(data,model,agent=agent,callback_fns=[FakeRunCallback],loss_func=fake_loss)\n",
    "setattr(learn,'fitter',buggy_dqn_fitter)\n",
    "with pytest.raises(RuntimeError):\n",
    "    learn.fit(1,lr=0.01,wd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=AsyncExperienceSourceDataBunch.from_env('CartPole-v1',display=False,firstlast=True,add_valid=False,n_processes=2,n_envs=2)\n",
    "model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2))\n",
    "agent=DQNAgent(model=model)\n",
    "learn=AgentLearner(data,model,agent=agent,callback_fns=[FakeRunCallback],loss_func=fake_loss)\n",
    "setattr(learn,'fitter',dqn_fitter)\n",
    "learn.fit(1,lr=0.01,wd=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=AsyncExperienceSourceDataBunch.from_env('CartPole-v1',n_envs=5,display=False,firstlast=False,add_valid=False)\n",
    "for xb,yb in data.train_dl:\n",
    "    test_eq(len(xb),1)\n",
    "    test_eq(tuple(xb[0].shape),(5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 02_callbacks.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 05_data_block.ipynb.\n",
      "Converted 06_basic_train.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n",
      "converting: /opt/project/fastrl/nbs/06_basic_train.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/jupyter_client/manager.py:358: FutureWarning: Method cleanup(connection_file=True) is deprecated, use cleanup_resources(restart=False).\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/05_data_block.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html(n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
