{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.dads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastrl.ptan_extension import *\n",
    "from fastrl.actorcritic.sac import *\n",
    "\n",
    "from torch.distributions import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes: Temporarily here. goal is convertion into pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2e3e83c0ea91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-572acb1c60fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO(architsh): Implement the dynamics with last K step input\n",
    "class SkillDynamics:\n",
    "\n",
    "    def __init__(self,\n",
    "      observation_size,\n",
    "      action_size,\n",
    "      restrict_observation=0,\n",
    "      normalize_observations=False,\n",
    "      # network properties\n",
    "      fc_layer_params=(256, 256),\n",
    "      network_type='default',\n",
    "      num_components=1,\n",
    "      fix_variance=False,\n",
    "      reweigh_batches=False,\n",
    "      graph=None,\n",
    "      scope_name='skill_dynamics'):\n",
    "\n",
    "        self._observation_size = observation_size\n",
    "        self._action_size = action_size\n",
    "        self._normalize_observations = normalize_observations\n",
    "        self._restrict_observation = restrict_observation\n",
    "        self._reweigh_batches = reweigh_batches\n",
    "\n",
    "        # tensorflow requirements\n",
    "        if graph is not None:\n",
    "            self._graph = graph\n",
    "        else:\n",
    "            self._graph = tf.compat.v1.get_default_graph()\n",
    "        self._scope_name = scope_name\n",
    "\n",
    "        # dynamics network properties\n",
    "        self._fc_layer_params = fc_layer_params\n",
    "        self._network_type = network_type\n",
    "        self._num_components = num_components\n",
    "        self._fix_variance = fix_variance\n",
    "        if not self._fix_variance:\n",
    "            self._std_lower_clip = 0.3\n",
    "            self._std_upper_clip = 10.0\n",
    "\n",
    "        self._use_placeholders = False\n",
    "        self.log_probability = None\n",
    "        self.dyn_max_op = None\n",
    "        self.dyn_min_op = None\n",
    "        self._session = None\n",
    "        self._use_modal_mean = False\n",
    "\n",
    "        # saving/restoring variables\n",
    "        self._saver = None\n",
    "\n",
    "    def _get_distribution(self, out):\n",
    "        if self._num_components > 1:\n",
    "            self.logits = tf.compat.v1.layers.dense(\n",
    "              out, self._num_components, name='logits', reuse=tf.compat.v1.AUTO_REUSE)\n",
    "            means, scale_diags = [], []\n",
    "            for component_id in range(self._num_components):\n",
    "                means.append(\n",
    "                tf.compat.v1.layers.dense(\n",
    "                    out,\n",
    "                    self._observation_size,\n",
    "                    name='mean_' + str(component_id),\n",
    "                    reuse=tf.compat.v1.AUTO_REUSE))\n",
    "            if not self._fix_variance:\n",
    "                scale_diags.append(\n",
    "                  tf.clip_by_value(\n",
    "                      tf.compat.v1.layers.dense(\n",
    "                          out,\n",
    "                          self._observation_size,\n",
    "                          activation=tf.nn.softplus,\n",
    "                          name='stddev_' + str(component_id),\n",
    "                          reuse=tf.compat.v1.AUTO_REUSE), self._std_lower_clip,\n",
    "                      self._std_upper_clip))\n",
    "            else:\n",
    "                scale_diags.append(\n",
    "                  tf.fill([tf.shape(out)[0], self._observation_size], 1.0))\n",
    "\n",
    "            self.means = tf.stack(means, axis=1)\n",
    "            self.scale_diags = tf.stack(scale_diags, axis=1)\n",
    "            return tfp.distributions.MixtureSameFamily(\n",
    "              mixture_distribution=tfp.distributions.Categorical(\n",
    "                  logits=self.logits),\n",
    "              components_distribution=tfp.distributions.MultivariateNormalDiag(\n",
    "                  loc=self.means, scale_diag=self.scale_diags))\n",
    "\n",
    "        else:\n",
    "            mean = tf.compat.v1.layers.dense(\n",
    "              out, self._observation_size, name='mean', reuse=tf.compat.v1.AUTO_REUSE)\n",
    "            if not self._fix_variance:\n",
    "                stddev = tf.clip_by_value(\n",
    "                tf.compat.v1.layers.dense(\n",
    "                    out,\n",
    "                    self._observation_size,\n",
    "                    activation=tf.nn.softplus,\n",
    "                    name='stddev',\n",
    "                    reuse=tf.compat.v1.AUTO_REUSE), self._std_lower_clip,\n",
    "                self._std_upper_clip)\n",
    "            else:\n",
    "                stddev = tf.fill([tf.shape(out)[0], self._observation_size], 1.0)\n",
    "            return tfp.distributions.MultivariateNormalDiag(\n",
    "              loc=mean, scale_diag=stddev)\n",
    "\n",
    "    # dynamics graph with separate pipeline for skills and timesteps\n",
    "    def _graph_with_separate_skill_pipe(self, timesteps, actions):\n",
    "        skill_out = actions\n",
    "        with tf.compat.v1.variable_scope('action_pipe'):\n",
    "            for idx, layer_size in enumerate((self._fc_layer_params[0] // 2,)):\n",
    "                skill_out = tf.compat.v1.layers.dense(\n",
    "                  skill_out,\n",
    "                  layer_size,\n",
    "                  activation=tf.nn.relu,\n",
    "                  name='hid_' + str(idx),\n",
    "                  reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        ts_out = timesteps\n",
    "        with tf.compat.v1.variable_scope('ts_pipe'):\n",
    "            for idx, layer_size in enumerate((self._fc_layer_params[0] // 2,)):\n",
    "                ts_out = tf.compat.v1.layers.dense(\n",
    "                  ts_out,\n",
    "                  layer_size,\n",
    "                  activation=tf.nn.relu,\n",
    "                  name='hid_' + str(idx),\n",
    "                  reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        # out = tf.compat.v1.layers.flatten(tf.einsum('ai,aj->aij', ts_out, skill_out))\n",
    "        out = tf.concat([ts_out, skill_out], axis=1)\n",
    "        with tf.compat.v1.variable_scope('joint'):\n",
    "            for idx, layer_size in enumerate(self._fc_layer_param[1:]):\n",
    "                out = tf.compat.v1.layers.dense(\n",
    "                  out,\n",
    "                  layer_size,\n",
    "                  activation=tf.nn.relu,\n",
    "                  name='hid_' + str(idx),\n",
    "                  reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        return self._get_distribution(out)\n",
    "\n",
    "    # simple dynamics graph\n",
    "    def _default_graph(self, timesteps, actions):\n",
    "        out = tf.concat([timesteps, actions], axis=1)\n",
    "        for idx, layer_size in enumerate(self._fc_layer_params):\n",
    "            out = tf.compat.v1.layers.dense(\n",
    "                out,\n",
    "                layer_size,\n",
    "                activation=tf.nn.relu,\n",
    "                name='hid_' + str(idx),\n",
    "                reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        return self._get_distribution(out)\n",
    "\n",
    "    def _get_dict(self,\n",
    "                input_data,\n",
    "                input_actions,\n",
    "                target_data,\n",
    "                batch_size=-1,\n",
    "                batch_weights=None,\n",
    "                batch_norm=False,\n",
    "                noise_targets=False,\n",
    "                noise_std=0.5):\n",
    "        if batch_size > 0:\n",
    "            shuffled_batch = np.random.permutation(len(input_data))[:batch_size]\n",
    "        else:\n",
    "            shuffled_batch = np.arange(len(input_data))\n",
    "\n",
    "        # if we are noising the input, it is better to create a new copy of the numpy arrays\n",
    "        batched_input = input_data[shuffled_batch, :]\n",
    "        batched_skills = input_actions[shuffled_batch, :]\n",
    "        batched_targets = target_data[shuffled_batch, :]\n",
    "\n",
    "        if self._reweigh_batches and batch_weights is not None:\n",
    "            example_weights = batch_weights[shuffled_batch]\n",
    "\n",
    "        if noise_targets:\n",
    "            batched_targets += np.random.randn(*batched_targets.shape) * noise_std\n",
    "\n",
    "        return_dict = {\n",
    "            self.timesteps_pl: batched_input,\n",
    "            self.actions_pl: batched_skills,\n",
    "            self.next_timesteps_pl: batched_targets\n",
    "        }\n",
    "        if self._normalize_observations:\n",
    "            return_dict[self.is_training_pl] = batch_norm\n",
    "        if self._reweigh_batches and batch_weights is not None:\n",
    "            return_dict[self.batch_weights] = example_weights\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def _get_run_dict(self, input_data, input_actions):\n",
    "        return_dict = {\n",
    "            self.timesteps_pl: input_data,\n",
    "            self.actions_pl: input_actions\n",
    "        }\n",
    "        if self._normalize_observations:\n",
    "            return_dict[self.is_training_pl] = False\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def make_placeholders(self):\n",
    "        self._use_placeholders = True\n",
    "        with self._graph.as_default(), tf.compat.v1.variable_scope(self._scope_name):\n",
    "            self.timesteps_pl = tf.compat.v1.placeholder(\n",
    "              tf.float32, shape=(None, self._observation_size), name='timesteps_pl')\n",
    "            self.actions_pl = tf.compat.v1.placeholder(\n",
    "              tf.float32, shape=(None, self._action_size), name='actions_pl')\n",
    "            self.next_timesteps_pl = tf.compat.v1.placeholder(\n",
    "              tf.float32,\n",
    "              shape=(None, self._observation_size),\n",
    "              name='next_timesteps_pl')\n",
    "            if self._normalize_observations:\n",
    "                self.is_training_pl = tf.compat.v1.placeholder(tf.bool, name='batch_norm_pl')\n",
    "            if self._reweigh_batches:\n",
    "                self.batch_weights = tf.compat.v1.placeholder(\n",
    "                    tf.float32, shape=(None,), name='importance_sampled_weights')\n",
    "\n",
    "    def set_session(self, session=None, initialize_or_restore_variables=False):\n",
    "        if session is None:\n",
    "            self._session = tf.Session(graph=self._graph)\n",
    "        else:\n",
    "            self._session = session\n",
    "\n",
    "        # only initialize uninitialized variables\n",
    "        if initialize_or_restore_variables:\n",
    "            if tf.io.gfile.exists(self._save_prefix):\n",
    "                self.restore_variables()\n",
    "            with self._graph.as_default():\n",
    "                var_list = tf.compat.v1.global_variables(\n",
    "                ) + tf.compat.v1.local_variables()\n",
    "                is_initialized = self._session.run(\n",
    "                    [tf.compat.v1.is_variable_initialized(v) for v in var_list])\n",
    "                uninitialized_vars = []\n",
    "                for flag, v in zip(is_initialized, var_list):\n",
    "                    if not flag:\n",
    "                        uninitialized_vars.append(v)\n",
    "\n",
    "                if uninitialized_vars:\n",
    "                    self._session.run(\n",
    "                      tf.compat.v1.variables_initializer(uninitialized_vars))\n",
    "\n",
    "    def build_graph(self,\n",
    "                  timesteps=None,\n",
    "                  actions=None,\n",
    "                  next_timesteps=None,\n",
    "                  is_training=None):\n",
    "        with self._graph.as_default(), tf.compat.v1.variable_scope(\n",
    "            self._scope_name, reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            if self._use_placeholders:\n",
    "                timesteps = self.timesteps_pl\n",
    "                actions = self.actions_pl\n",
    "                next_timesteps = self.next_timesteps_pl\n",
    "            if self._normalize_observations:\n",
    "                is_training = self.is_training_pl\n",
    "\n",
    "            # predict deltas instead of observations\n",
    "            next_timesteps -= timesteps\n",
    "\n",
    "            if self._restrict_observation > 0:\n",
    "                timesteps = timesteps[:, self._restrict_observation:]\n",
    "\n",
    "            if self._normalize_observations:\n",
    "                timesteps = tf.compat.v1.layers.batch_normalization(\n",
    "                timesteps,\n",
    "                training=is_training,\n",
    "                name='input_normalization',\n",
    "                reuse=tf.compat.v1.AUTO_REUSE)\n",
    "                self.output_norm_layer = tf.compat.v1.layers.BatchNormalization(\n",
    "                scale=False, center=False, name='output_normalization')\n",
    "                next_timesteps = self.output_norm_layer(\n",
    "                next_timesteps, training=is_training)\n",
    "\n",
    "            if self._network_type == 'default':\n",
    "                self.base_distribution = self._default_graph(timesteps, actions)\n",
    "            elif self._network_type == 'separate':\n",
    "                self.base_distribution = self._graph_with_separate_skill_pipe(\n",
    "                timesteps, actions)\n",
    "\n",
    "            # if building multiple times, be careful about which log_prob you are optimizing\n",
    "            self.log_probability = self.base_distribution.log_prob(next_timesteps)\n",
    "            self.mean = self.base_distribution.mean()\n",
    "\n",
    "            return self.log_probability\n",
    "\n",
    "    def increase_prob_op(self, learning_rate=3e-4, weights=None):\n",
    "        with self._graph.as_default():\n",
    "            update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                if self._reweigh_batches:\n",
    "                    self.dyn_max_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                      learning_rate=learning_rate,\n",
    "                      name='adam_max').minimize(-tf.reduce_mean(self.log_probability *\n",
    "                                                                self.batch_weights))\n",
    "                elif weights is not None:\n",
    "                    self.dyn_max_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                      learning_rate=learning_rate,\n",
    "                      name='adam_max').minimize(-tf.reduce_mean(self.log_probability *\n",
    "                                                                weights))\n",
    "                else:\n",
    "                    self.dyn_max_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                      learning_rate=learning_rate,\n",
    "                      name='adam_max').minimize(-tf.reduce_mean(self.log_probability))\n",
    "\n",
    "                return self.dyn_max_op\n",
    "\n",
    "    def decrease_prob_op(self, learning_rate=3e-4, weights=None):\n",
    "        with self._graph.as_default():\n",
    "            update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                if self._reweigh_batches:\n",
    "                    self.dyn_min_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                      learning_rate=learning_rate, name='adam_min').minimize(\n",
    "                          tf.reduce_mean(self.log_probability * self.batch_weights))\n",
    "                elif weights is not None:\n",
    "                    self.dyn_min_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                      learning_rate=learning_rate, name='adam_min').minimize(\n",
    "                          tf.reduce_mean(self.log_probability * weights))\n",
    "                else:\n",
    "                    self.dyn_min_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                      learning_rate=learning_rate,\n",
    "                      name='adam_min').minimize(tf.reduce_mean(self.log_probability))\n",
    "                return self.dyn_min_op\n",
    "\n",
    "    def create_saver(self, save_prefix):\n",
    "        if self._saver is not None:\n",
    "            return self._saver\n",
    "        else:\n",
    "            with self._graph.as_default():\n",
    "                self._variable_list = {}\n",
    "                for var in tf.compat.v1.get_collection(\n",
    "                    tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self._scope_name):\n",
    "                    self._variable_list[var.name] = var\n",
    "                self._saver = tf.compat.v1.train.Saver(\n",
    "                    self._variable_list, save_relative_paths=True)\n",
    "                self._save_prefix = save_prefix\n",
    "\n",
    "    def save_variables(self, global_step):\n",
    "        if not tf.io.gfile.exists(self._save_prefix):\n",
    "            tf.io.gfile.makedirs(self._save_prefix)\n",
    "\n",
    "        self._saver.save(\n",
    "            self._session,\n",
    "            os.path.join(self._save_prefix, 'ckpt'),\n",
    "            global_step=global_step)\n",
    "\n",
    "    def restore_variables(self):\n",
    "        self._saver.restore(self._session,\n",
    "                            tf.compat.v1.train.latest_checkpoint(self._save_prefix))\n",
    "\n",
    "    # all functions here-on require placeholders----------------------------------\n",
    "    def train(self,\n",
    "            timesteps,\n",
    "            actions,\n",
    "            next_timesteps,\n",
    "            batch_weights=None,\n",
    "            batch_size=512,\n",
    "            num_steps=1,\n",
    "            increase_probs=True):\n",
    "        if not self._use_placeholders:return\n",
    "\n",
    "        if increase_probs: run_op = self.dyn_max_op\n",
    "        else: run_op = self.dyn_min_op\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            self._session.run(\n",
    "              run_op,\n",
    "              feed_dict=self._get_dict(\n",
    "                  timesteps,\n",
    "                  actions,\n",
    "                  next_timesteps,\n",
    "                  batch_weights=batch_weights,\n",
    "                  batch_size=batch_size,\n",
    "                  batch_norm=True))\n",
    "\n",
    "    def get_log_prob(self, timesteps, actions, next_timesteps):\n",
    "        if not self._use_placeholders:return\n",
    "\n",
    "        return self._session.run(\n",
    "            self.log_probability,\n",
    "            feed_dict=self._get_dict(\n",
    "                timesteps, actions, next_timesteps, batch_norm=False))\n",
    "\n",
    "    def predict_state(self, timesteps, actions):\n",
    "        if not self._use_placeholders:\n",
    "            return\n",
    "\n",
    "        if self._use_modal_mean:\n",
    "            all_means, modal_mean_indices = self._session.run(\n",
    "              [self.means, tf.argmax(self.logits, axis=1)],\n",
    "              feed_dict=self._get_run_dict(timesteps, actions))\n",
    "            pred_state = all_means[[\n",
    "              np.arange(all_means.shape[0]), modal_mean_indices\n",
    "          ]]\n",
    "        else:\n",
    "            pred_state = self._session.run(\n",
    "              self.mean, feed_dict=self._get_run_dict(timesteps, actions))\n",
    "\n",
    "        if self._normalize_observations:\n",
    "            with self._session.as_default(), self._graph.as_default():\n",
    "                mean_correction, variance_correction = self.output_norm_layer.get_weights(\n",
    "                )\n",
    "\n",
    "            pred_state = pred_state * np.sqrt(variance_correction +\n",
    "                                                1e-3) + mean_correction\n",
    "\n",
    "        pred_state += timesteps\n",
    "        return pred_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1cd16b332a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDADSAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self,\n\u001b[1;32m      4\u001b[0m                \u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mskill_dynamics_observation_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAC' is not defined"
     ]
    }
   ],
   "source": [
    "class DADSAgent(SAC):\n",
    "\n",
    "    def __init__(self,\n",
    "               save_directory,\n",
    "               skill_dynamics_observation_size,\n",
    "               observation_modify_fn=None,\n",
    "               restrict_input_size=0,\n",
    "               latent_size=2,\n",
    "               latent_prior='cont_uniform',\n",
    "               prior_samples=100,\n",
    "               fc_layer_params=(256, 256),\n",
    "               normalize_observations=True,\n",
    "               network_type='default',\n",
    "               num_mixture_components=4,\n",
    "               fix_variance=True,\n",
    "               skill_dynamics_learning_rate=3e-4,\n",
    "               reweigh_batches=False,\n",
    "               agent_graph=None,\n",
    "               skill_dynamics_graph=None,\n",
    "               *sac_args,\n",
    "               **sac_kwargs):\n",
    "        self._skill_dynamics_learning_rate = skill_dynamics_learning_rate\n",
    "        self._latent_size = latent_size\n",
    "        self._latent_prior = latent_prior\n",
    "        self._prior_samples = prior_samples\n",
    "        self._save_directory = save_directory\n",
    "        self._restrict_input_size = restrict_input_size\n",
    "        self._process_observation = observation_modify_fn\n",
    "\n",
    "        if agent_graph is None:\n",
    "            self._graph = tf.compat.v1.get_default_graph()\n",
    "        else:\n",
    "            self._graph = agent_graph\n",
    "\n",
    "        if skill_dynamics_graph is None:\n",
    "            skill_dynamics_graph = self._graph\n",
    "\n",
    "        # instantiate the skill dynamics\n",
    "        self._skill_dynamics = skill_dynamics.SkillDynamics(\n",
    "            observation_size=skill_dynamics_observation_size,\n",
    "            action_size=self._latent_size,\n",
    "            restrict_observation=self._restrict_input_size,\n",
    "            normalize_observations=normalize_observations,\n",
    "            fc_layer_params=fc_layer_params,\n",
    "            network_type=network_type,\n",
    "            num_components=num_mixture_components,\n",
    "            fix_variance=fix_variance,\n",
    "            reweigh_batches=reweigh_batches,\n",
    "            graph=skill_dynamics_graph)\n",
    "\n",
    "        super(DADSAgent, self).__init__(*sac_args, **sac_kwargs)\n",
    "        self._placeholders_in_place = False\n",
    "\n",
    "    def compute_dads_reward(self, input_obs, cur_skill, target_obs):\n",
    "        if self._process_observation is not None:\n",
    "            input_obs, target_obs = self._process_observation(\n",
    "              input_obs), self._process_observation(target_obs)\n",
    "\n",
    "        num_reps = self._prior_samples if self._prior_samples > 0 else self._latent_size - 1\n",
    "        input_obs_altz = np.concatenate([input_obs] * num_reps, axis=0)\n",
    "        target_obs_altz = np.concatenate([target_obs] * num_reps, axis=0)\n",
    "\n",
    "        # for marginalization of the denominator\n",
    "        if self._latent_prior == 'discrete_uniform' and not self._prior_samples:\n",
    "            alt_skill = np.concatenate(\n",
    "              [np.roll(cur_skill, i, axis=1) for i in range(1, num_reps + 1)],\n",
    "              axis=0)\n",
    "        elif self._latent_prior == 'discrete_uniform':\n",
    "            alt_skill = np.random.multinomial(\n",
    "              1, [1. / self._latent_size] * self._latent_size,\n",
    "              size=input_obs_altz.shape[0])\n",
    "        elif self._latent_prior == 'gaussian':\n",
    "            alt_skill = np.random.multivariate_normal(\n",
    "              np.zeros(self._latent_size),\n",
    "              np.eye(self._latent_size),\n",
    "              size=input_obs_altz.shape[0])\n",
    "        elif self._latent_prior == 'cont_uniform':\n",
    "            alt_skill = np.random.uniform(\n",
    "              low=-1.0, high=1.0, size=(input_obs_altz.shape[0], self._latent_size))\n",
    "\n",
    "        logp = self._skill_dynamics.get_log_prob(input_obs, cur_skill, target_obs)\n",
    "\n",
    "        # denominator may require more memory than that of a GPU, break computation\n",
    "        split_group = 20 * 4000\n",
    "        if input_obs_altz.shape[0] <= split_group:\n",
    "            logp_altz = self._skill_dynamics.get_log_prob(input_obs_altz, alt_skill,\n",
    "                                                        target_obs_altz)\n",
    "        else:\n",
    "            logp_altz = []\n",
    "            for split_idx in range(input_obs_altz.shape[0] // split_group):\n",
    "                start_split = split_idx * split_group\n",
    "                end_split = (split_idx + 1) * split_group\n",
    "                logp_altz.append(\n",
    "                  self._skill_dynamics.get_log_prob(\n",
    "                    input_obs_altz[start_split:end_split],\n",
    "                    alt_skill[start_split:end_split],\n",
    "                    target_obs_altz[start_split:end_split]))\n",
    "            if input_obs_altz.shape[0] % split_group:\n",
    "                start_split = input_obs_altz.shape[0] % split_group\n",
    "                logp_altz.append(\n",
    "                    self._skill_dynamics.get_log_prob(input_obs_altz[-start_split:],\n",
    "                                                      alt_skill[-start_split:],\n",
    "                                                  target_obs_altz[-start_split:]))\n",
    "            logp_altz = np.concatenate(logp_altz)\n",
    "        logp_altz = np.array(np.array_split(logp_altz, num_reps))\n",
    "\n",
    "        # final DADS reward\n",
    "        intrinsic_reward = np.log(num_reps + 1) - np.log(1 + np.exp(\n",
    "            np.clip(logp_altz - logp.reshape(1, -1), -50, 50)).sum(axis=0))\n",
    "\n",
    "        return intrinsic_reward, {'logp': logp, 'logp_altz': logp_altz.flatten()}\n",
    "\n",
    "    def get_experience_placeholder(self):\n",
    "        self._placeholders_in_place = True\n",
    "        self._placeholders = []\n",
    "        for item in nest.flatten(self.collect_data_spec):\n",
    "            self._placeholders += [\n",
    "              tf.compat.v1.placeholder(\n",
    "                  item.dtype,\n",
    "                  shape=(None, 2) if len(item.shape) == 0 else\n",
    "                  (None, 2, item.shape[-1]),\n",
    "                  name=item.name)\n",
    "          ]\n",
    "        self._policy_experience_ph = nest.pack_sequence_as(self.collect_data_spec,\n",
    "                                                           self._placeholders)\n",
    "        return self._policy_experience_ph\n",
    "\n",
    "    def build_agent_graph(self):\n",
    "        with self._graph.as_default():\n",
    "            self.get_experience_placeholder()\n",
    "            self.agent_train_op = self.train(self._policy_experience_ph)\n",
    "            self.summary_ops = tf.compat.v1.summary.all_v2_summary_ops()\n",
    "            return self.agent_train_op\n",
    "\n",
    "    def build_skill_dynamics_graph(self):\n",
    "        self._skill_dynamics.make_placeholders()\n",
    "        self._skill_dynamics.build_graph()\n",
    "        self._skill_dynamics.increase_prob_op(\n",
    "        learning_rate=self._skill_dynamics_learning_rate)\n",
    "\n",
    "    def create_savers(self):\n",
    "        self._skill_dynamics.create_saver(\n",
    "            save_prefix=os.path.join(self._save_directory, 'dynamics'))\n",
    "\n",
    "    def set_sessions(self, initialize_or_restore_skill_dynamics, session=None):\n",
    "        if session is not None:\n",
    "            self._session = session\n",
    "        else:\n",
    "            self._session = tf.compat.v1.Session(graph=self._graph)\n",
    "        self._skill_dynamics.set_session(\n",
    "            initialize_or_restore_variables=initialize_or_restore_skill_dynamics,\n",
    "            session=session)\n",
    "\n",
    "    def save_variables(self, global_step):\n",
    "        self._skill_dynamics.save_variables(global_step=global_step)\n",
    "\n",
    "    def _get_dict(self, trajectories, batch_size=-1):\n",
    "        tf.nest.assert_same_structure(self.collect_data_spec, trajectories)\n",
    "        if batch_size > 0:\n",
    "              shuffled_batch = np.random.permutation(\n",
    "              trajectories.observation.shape[0])[:batch_size]\n",
    "        else:\n",
    "              shuffled_batch = np.arange(trajectories.observation.shape[0])\n",
    "\n",
    "        return_dict = {}\n",
    "\n",
    "        for placeholder, val in zip(self._placeholders, nest.flatten(trajectories)):\n",
    "              return_dict[placeholder] = val[shuffled_batch]\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def train_loop(self,\n",
    "                 trajectories,\n",
    "                 recompute_reward=False,\n",
    "                 batch_size=-1,\n",
    "                 num_steps=1):\n",
    "        if not self._placeholders_in_place:\n",
    "            return\n",
    "\n",
    "        if recompute_reward:\n",
    "            input_obs = trajectories.observation[:, 0, :-self._latent_size]\n",
    "            cur_skill = trajectories.observation[:, 0, -self._latent_size:]\n",
    "            target_obs = trajectories.observation[:, 1, :-self._latent_size]\n",
    "            new_reward, info = self.compute_dads_reward(input_obs, cur_skill,\n",
    "                                                      target_obs)\n",
    "            trajectories = trajectories._replace(\n",
    "              reward=np.concatenate(\n",
    "                  [np.expand_dims(new_reward, axis=1), trajectories.reward[:, 1:]],\n",
    "                  axis=1))\n",
    "\n",
    "          # TODO(architsh):all agent specs should be the same as env specs, shift preprocessing to actor/critic networks\n",
    "        if self._restrict_input_size > 0:\n",
    "            trajectories = trajectories._replace(\n",
    "              observation=trajectories.observation[:, :,\n",
    "                                                   self._restrict_input_size:])\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            self._session.run([self.agent_train_op, self.summary_ops],\n",
    "                            feed_dict=self._get_dict(\n",
    "                                trajectories, batch_size=batch_size))\n",
    "\n",
    "        if recompute_reward:\n",
    "            return new_reward, info\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    @property\n",
    "    def skill_dynamics(self):\n",
    "        return self._skill_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DADSLearner(SACLearner):pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env='Pendulum-v0'\n",
    "agent=DADSAgent(3,gym.make(env).action_space,gamma=0.99,tau=0.005,alpha=0.2)\n",
    "\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,exclude_nones=True,\n",
    "                               dls_kwargs={'bs':1,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "#                       batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*1,n=1000,device=default_device())\n",
    "\n",
    "learner=DADSLearner(dls,agent=agent,cbs=[ExperienceReplay(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),SACCriticTrainer],\n",
    "                   metrics=[AvgEpisodeRewardMetric()])\n",
    "learner.fit(30,lr=0.001,wd=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
