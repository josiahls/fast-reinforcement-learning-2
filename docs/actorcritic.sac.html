---

title: SAC


keywords: fastai
sidebar: home_sidebar

summary: "Soft Actor Critic"
description: "Soft Actor Critic"
nb_path: "nbs/14a_actorcritic.sac.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/14a_actorcritic.sac.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_LAUNCH_BLOCKING&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="weights_init_" class="doc_header"><code>weights_init_</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L43" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>weights_init_</code>(<strong><code>m</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ValueNetwork" class="doc_header"><code>class</code> <code>ValueNetwork</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L49" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ValueNetwork</code>(<strong><code>num_inputs</code></strong>, <strong><code>hidden_dim</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="QNetwork" class="doc_header"><code>class</code> <code>QNetwork</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L67" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>QNetwork</code>(<strong><code>num_inputs</code></strong>, <strong><code>num_actions</code></strong>, <strong><code>hidden_dim</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GaussianPolicy" class="doc_header"><code>class</code> <code>GaussianPolicy</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L99" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GaussianPolicy</code>(<strong><code>num_inputs</code></strong>, <strong><code>num_actions</code></strong>, <strong><code>hidden_dim</code></strong>, <strong><code>action_space</code></strong>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DeterministicPolicy" class="doc_header"><code>class</code> <code>DeterministicPolicy</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L150" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DeterministicPolicy</code>(<strong><code>num_inputs</code></strong>, <strong><code>num_actions</code></strong>, <strong><code>hidden_dim</code></strong>, <strong><code>action_space</code></strong>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_log_gaussian" class="doc_header"><code>create_log_gaussian</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L193" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_log_gaussian</code>(<strong><code>mean</code></strong>, <strong><code>log_std</code></strong>, <strong><code>t</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logsumexp" class="doc_header"><code>logsumexp</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L201" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>logsumexp</code>(<strong><code>inputs</code></strong>, <strong><code>dim</code></strong>=<em><code>None</code></em>, <strong><code>keepdim</code></strong>=<em><code>False</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="soft_update" class="doc_header"><code>soft_update</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L211" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>soft_update</code>(<strong><code>target</code></strong>, <strong><code>source</code></strong>, <strong><code>tau</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="hard_update" class="doc_header"><code>hard_update</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L215" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>hard_update</code>(<strong><code>target</code></strong>, <strong><code>source</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SAC" class="doc_header"><code>class</code> <code>SAC</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L223" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SAC</code>(<strong><code>num_inputs</code></strong>, <strong><code>action_space</code></strong>, <strong><code>gamma</code></strong>=<em><code>0.99</code></em>, <strong><code>tau</code></strong>=<em><code>0.005</code></em>, <strong><code>alpha</code></strong>=<em><code>0.2</code></em>, <strong><code>policy</code></strong>=<em><code>'gaussian'</code></em>, <strong><code>automatic_entropy_tuning</code></strong>=<em><code>True</code></em>, <strong><code>target_update_interval</code></strong>=<em><code>1</code></em>, <strong><code>hidden_size</code></strong>=<em><code>100</code></em>, <strong><code>lr</code></strong>=<em><code>0.0003</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#BaseAgent"><code>BaseAgent</code></a></p>
</blockquote>
<p>BaseAgent(model: torch.nn.modules.module.Module = None)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ExperienceReplay" class="doc_header"><code>class</code> <code>ExperienceReplay</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L344" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ExperienceReplay</code>(<strong><code>sz</code></strong>=<em><code>100</code></em>, <strong><code>bs</code></strong>=<em><code>128</code></em>, <strong><code>starting_els</code></strong>=<em><code>1</code></em>, <strong><code>max_steps</code></strong>=<em><code>1</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SACCriticTrainer" class="doc_header"><code>class</code> <code>SACCriticTrainer</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L376" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SACCriticTrainer</code>(<strong><code>after_create</code></strong>=<em><code>None</code></em>, <strong><code>before_fit</code></strong>=<em><code>None</code></em>, <strong><code>before_epoch</code></strong>=<em><code>None</code></em>, <strong><code>before_train</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_pred</code></strong>=<em><code>None</code></em>, <strong><code>after_loss</code></strong>=<em><code>None</code></em>, <strong><code>before_backward</code></strong>=<em><code>None</code></em>, <strong><code>after_backward</code></strong>=<em><code>None</code></em>, <strong><code>after_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_train</code></strong>=<em><code>None</code></em>, <strong><code>after_train</code></strong>=<em><code>None</code></em>, <strong><code>before_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_fit</code></strong>=<em><code>None</code></em>, <strong><code>after_fit</code></strong>=<em><code>None</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SACLearner" class="doc_header"><code>class</code> <code>SACLearner</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L388" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SACLearner</code>(<strong><code>dls</code></strong>, <strong><code>agent</code></strong>=<em><code>None</code></em>, <strong><code>model</code></strong>=<em><code>None</code></em>, <strong><code>use_train_mets</code></strong>=<em><code>True</code></em>, <strong><code>loss_func</code></strong>=<em><code>None</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>splitter</code></strong>=<em><code>trainable_params</code></em>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>metrics</code></strong>=<em><code>None</code></em>, <strong><code>path</code></strong>=<em><code>None</code></em>, <strong><code>model_dir</code></strong>=<em><code>'models'</code></em>, <strong><code>wd</code></strong>=<em><code>None</code></em>, <strong><code>wd_bn_bias</code></strong>=<em><code>False</code></em>, <strong><code>train_bn</code></strong>=<em><code>True</code></em>, <strong><code>moms</code></strong>=<em><code>(0.95, 0.85, 0.95)</code></em>) :: <a href="/fast-reinforcement-learning-2/learner.html#AgentLearner"><code>AgentLearner</code></a></p>
</blockquote>
<p>Base Learner for all reinforcement learning agents</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Modules">Modules<a class="anchor-link" href="#Modules"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pybulletgym.envs</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">env</span><span class="o">=</span><span class="s1">&#39;InvertedPendulumPyBulletEnv-v0&#39;</span>
<span class="n">agent</span><span class="o">=</span><span class="n">SAC</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env</span><span class="p">)</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">tau</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">block</span><span class="o">=</span><span class="n">FirstLastExperienceBlock</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">exclude_nones</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">dls_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;bs&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;num_workers&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s1">&#39;verbose&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">,</span><span class="s1">&#39;indexed&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span><span class="s1">&#39;shuffle_train&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">})</span>
<span class="n">blk</span><span class="o">=</span><span class="n">IterableDataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">block</span><span class="p">),</span>
                      <span class="n">splitter</span><span class="o">=</span><span class="n">FuncSplitter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="kc">False</span><span class="p">),</span>
<span class="c1">#                       batch_tfms=lambda x:(x[&#39;s&#39;],x),</span>
                     <span class="p">)</span>
<span class="n">dls</span><span class="o">=</span><span class="n">blk</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">([</span><span class="n">env</span><span class="p">]</span><span class="o">*</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>

<span class="n">learner</span><span class="o">=</span><span class="n">SACLearner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">ExperienceReplay</span><span class="p">(</span><span class="n">sz</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span><span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="n">starting_els</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">max_steps</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env</span><span class="p">)</span><span class="o">.</span><span class="n">_max_episode_steps</span><span class="p">),</span><span class="n">SACCriticTrainer</span><span class="p">],</span>
                   <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AvgEpisodeRewardMetric</span><span class="p">(</span><span class="n">experience_cls</span><span class="o">=</span><span class="n">ExperienceFirstLast</span><span class="p">)])</span>
<span class="n">learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">wd</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>train_avg_episode_r</th>
      <th>valid_loss</th>
      <th>valid_avg_episode_r</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>120.603119</td>
      <td>0</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(tensor([[-0.9422, -0.3350,  0.9308]], dtype=torch.float64),) (tensor([[-1.5522]]), tensor([-15.5913], dtype=torch.float64), tensor([[-0.9259, -0.3779,  0.4724]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([3]))
(tensor([[-0.9345, -0.3559,  0.4467]], dtype=torch.float64),) (tensor([[1.9507]]), tensor([-15.2719], dtype=torch.float64), tensor([[-0.9256, -0.3786,  0.0150]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([4]))
(tensor([[-0.9259, -0.3779,  0.4724]], dtype=torch.float64),) (tensor([[-1.1597]]), tensor([-15.1137], dtype=torch.float64), tensor([[-0.9310, -0.3649, -0.2945]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([5]))
(tensor([[-0.9256, -0.3786,  0.0150]], dtype=torch.float64),) (tensor([[-0.1708]]), tensor([-15.1768], dtype=torch.float64), tensor([[-0.9444, -0.3288, -0.7705]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([6]))
(tensor([[-0.9310, -0.3649, -0.2945]], dtype=torch.float64),) (tensor([[-1.3488]]), tensor([-15.5322], dtype=torch.float64), tensor([[-0.9633, -0.2685, -1.2644]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([7]))
(tensor([[-0.9444, -0.3288, -0.7705]], dtype=torch.float64),) (tensor([[-1.6486]]), tensor([-16.2521], dtype=torch.float64), tensor([[-0.9821, -0.1883, -1.6463]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([8]))
(tensor([[-0.9633, -0.2685, -1.2644]], dtype=torch.float64),) (tensor([[-1.2039]]), tensor([-17.2942], dtype=torch.float64), tensor([[-0.9957, -0.0925, -1.9369]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([9]))
(tensor([[-0.9821, -0.1883, -1.6463]], dtype=torch.float64),) (tensor([[-0.9955]]), tensor([-18.5643], dtype=torch.float64), tensor([[-0.9998,  0.0201, -2.2540]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([10]))
(tensor([[-0.9957, -0.0925, -1.9369]], dtype=torch.float64),) (tensor([[-1.6517]]), tensor([-19.8269], dtype=torch.float64), tensor([[-0.9930,  0.1178, -1.9604]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([11]))
(tensor([[-0.9998,  0.0201, -2.2540]], dtype=torch.float64),) (tensor([[1.8570]]), tensor([-19.6866], dtype=torch.float64), tensor([[-0.9765,  0.2156, -1.9852]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([12]))
(tensor([[-0.9930,  0.1178, -1.9604]], dtype=torch.float64),) (tensor([[-0.7541]]), tensor([-18.3855], dtype=torch.float64), tensor([[-0.9486,  0.3165, -2.0929]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([13]))
(tensor([[-0.9765,  0.2156, -1.9852]], dtype=torch.float64),) (tensor([[-1.7964]]), tensor([-17.2561], dtype=torch.float64), tensor([[-0.9096,  0.4156, -2.1316]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([14]))
(tensor([[-0.9486,  0.3165, -2.0929]], dtype=torch.float64),) (tensor([[-1.8404]]), tensor([-16.1304], dtype=torch.float64), tensor([[-0.8624,  0.5062, -2.0428]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([15]))
(tensor([[-0.9096,  0.4156, -2.1316]], dtype=torch.float64),) (tensor([[-1.4858]]), tensor([-14.9817], dtype=torch.float64), tensor([[-0.8098,  0.5867, -1.9251]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([16]))
(tensor([[-0.8624,  0.5062, -2.0428]], dtype=torch.float64),) (tensor([[-1.7458]]), tensor([-13.8664], dtype=torch.float64), tensor([[-0.7564,  0.6541, -1.7206]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([17]))
(tensor([[-0.8098,  0.5867, -1.9251]], dtype=torch.float64),) (tensor([[-1.5703]]), tensor([-12.8315], dtype=torch.float64), tensor([[-0.7056,  0.7086, -1.4897]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([18]))
(tensor([[-0.7564,  0.6541, -1.7206]], dtype=torch.float64),) (tensor([[-1.7316]]), tensor([-11.9052], dtype=torch.float64), tensor([[-0.6787,  0.7344, -0.7470]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([19]))
(tensor([[-0.7056,  0.7086, -1.4897]], dtype=torch.float64),) (tensor([[1.4084]]), tensor([-11.1355], dtype=torch.float64), tensor([[-0.6766,  0.7363, -0.0563]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([20]))
(tensor([[-0.6787,  0.7344, -0.7470]], dtype=torch.float64),) (tensor([[0.9329]]), tensor([-10.7255], dtype=torch.float64), tensor([[-0.6912,  0.7226,  0.4006]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([21]))
(tensor([[-0.6766,  0.7363, -0.0563]], dtype=torch.float64),) (tensor([[-0.6359]]), tensor([-10.7667], dtype=torch.float64), tensor([[-0.7332,  0.6800,  1.1960]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([22]))
(tensor([[-0.6912,  0.7226,  0.4006]], dtype=torch.float64),) (tensor([[1.6893]]), tensor([-11.2814], dtype=torch.float64), tensor([[-0.7922,  0.6103,  1.8283]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([23]))
(tensor([[-0.7332,  0.6800,  1.1960]], dtype=torch.float64),) (tensor([[0.8154]]), tensor([-12.3193], dtype=torch.float64), tensor([[-0.8582,  0.5134,  2.3463]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([24]))
(tensor([[-0.7922,  0.6103,  1.8283]], dtype=torch.float64),) (tensor([[0.4021]]), tensor([-13.7615], dtype=torch.float64), tensor([[-0.9224,  0.3862,  2.8509]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([25]))
(tensor([[-0.8582,  0.5134,  2.3463]], dtype=torch.float64),) (tensor([[0.7970]]), tensor([-15.5888], dtype=torch.float64), tensor([[-0.9708,  0.2397,  3.0892]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([26]))
(tensor([[-0.9224,  0.3862,  2.8509]], dtype=torch.float64),) (tensor([[-0.3422]]), tensor([-17.6165], dtype=torch.float64), tensor([[-0.9973,  0.0734,  3.3734]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([27]))
(tensor([[-0.9708,  0.2397,  3.0892]], dtype=torch.float64),) (tensor([[0.6960]]), tensor([-19.8087], dtype=torch.float64), tensor([[-0.9957, -0.0921,  3.3136]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([28]))
(tensor([[-0.9973,  0.0734,  3.3734]], dtype=torch.float64),) (tensor([[-0.7660]]), tensor([-20.8456], dtype=torch.float64), tensor([[-0.9695, -0.2451,  3.1064]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([29]))
(tensor([[-0.9957, -0.0921,  3.3136]], dtype=torch.float64),) (tensor([[-0.9206]]), tensor([-19.6442], dtype=torch.float64), tensor([[-0.9226, -0.3857,  2.9685]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([30]))
(tensor([[-0.9695, -0.2451,  3.1064]], dtype=torch.float64),) (tensor([[0.3060]]), tensor([-17.6756], dtype=torch.float64), tensor([[-0.8631, -0.5050,  2.6672]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([31]))
(tensor([[-0.9226, -0.3857,  2.9685]], dtype=torch.float64),) (tensor([[-0.0802]]), tensor([-15.8802], dtype=torch.float64), tensor([[-0.8043, -0.5942,  2.1382]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([32]))
(tensor([[-0.8631, -0.5050,  2.6672]], dtype=torch.float64),) (tensor([[-1.0015]]), tensor([-14.2046], dtype=torch.float64), tensor([[-0.7581, -0.6521,  1.4814]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([33]))
(tensor([[-0.8043, -0.5942,  2.1382]], dtype=torch.float64),) (tensor([[-1.4073]]), tensor([-12.8064], dtype=torch.float64), tensor([[-0.7186, -0.6954,  1.1726]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([34]))
(tensor([[-0.7581, -0.6521,  1.4814]], dtype=torch.float64),) (tensor([[1.2015]]), tensor([-11.8411], dtype=torch.float64), tensor([[-0.6942, -0.7198,  0.6907]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([35]))
(tensor([[-0.7186, -0.6954,  1.1726]], dtype=torch.float64),) (tensor([[0.2643]]), tensor([-11.2261], dtype=torch.float64), tensor([[-0.6878, -0.7259,  0.1774]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([36]))
(tensor([[-0.6942, -0.7198,  0.6907]], dtype=torch.float64),) (tensor([[0.1769]]), tensor([-10.8912], dtype=torch.float64), tensor([[-0.6920, -0.7219, -0.1163]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([37]))
(tensor([[-0.6878, -0.7259,  0.1774]], dtype=torch.float64),) (tensor([[1.6718]]), tensor([-10.8329], dtype=torch.float64), tensor([[-0.7069, -0.7073, -0.4170]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([38]))
(tensor([[-0.6920, -0.7219, -0.1163]], dtype=torch.float64),) (tensor([[1.6050]]), tensor([-10.9691], dtype=torch.float64), tensor([[-0.7347, -0.6784, -0.8033]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([39]))
(tensor([[-0.7069, -0.7073, -0.4170]], dtype=torch.float64),) (tensor([[0.9613]]), tensor([-11.3161], dtype=torch.float64), tensor([[-0.7793, -0.6266, -1.3673]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([40]))
(tensor([[-0.7347, -0.6784, -0.8033]], dtype=torch.float64),) (tensor([[-0.3682]]), tensor([-12.0049], dtype=torch.float64), tensor([[-0.8283, -0.5603, -1.6490]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([41]))
(tensor([[-0.7793, -0.6266, -1.3673]], dtype=torch.float64),) (tensor([[1.2549]]), tensor([-12.9531], dtype=torch.float64), tensor([[-0.8793, -0.4764, -1.9646]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([42]))
(tensor([[-0.8283, -0.5603, -1.6490]], dtype=torch.float64),) (tensor([[0.6978]]), tensor([-14.0682], dtype=torch.float64), tensor([[-0.9308, -0.3656, -2.4454]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([43]))
(tensor([[-0.8793, -0.4764, -1.9646]], dtype=torch.float64),) (tensor([[-0.8238]]), tensor([-15.5576], dtype=torch.float64), tensor([[-0.9734, -0.2292, -2.8598]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([44]))
(tensor([[-0.9308, -0.3656, -2.4454]], dtype=torch.float64),) (tensor([[-0.9351]]), tensor([-17.4539], dtype=torch.float64), tensor([[-0.9960, -0.0894, -2.8350]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([45]))
(tensor([[-0.9734, -0.2292, -2.8598]], dtype=torch.float64),) (tensor([[1.3115]]), tensor([-19.3076], dtype=torch.float64), tensor([[-0.9983,  0.0579, -2.9484]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([46]))
(tensor([[-0.9960, -0.0894, -2.8350]], dtype=torch.float64),) (tensor([[-0.3091]]), tensor([-20.3960], dtype=torch.float64), tensor([[-0.9817,  0.1905, -2.6747]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([47]))
(tensor([[-0.9983,  0.0579, -2.9484]], dtype=torch.float64),) (tensor([[1.5351]]), tensor([-19.7043], dtype=torch.float64), tensor([[-0.9511,  0.3088, -2.4463]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([48]))
(tensor([[-0.9817,  0.1905, -2.6747]], dtype=torch.float64),) (tensor([[0.5701]]), tensor([-17.9265], dtype=torch.float64), tensor([[-0.9138,  0.4062, -2.0856]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([49]))
(tensor([[-0.9511,  0.3088, -2.4463]], dtype=torch.float64),) (tensor([[0.8607]]), tensor([-16.3681], dtype=torch.float64), tensor([[-0.8715,  0.4904, -1.8868]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([50]))
(tensor([[-0.9138,  0.4062, -2.0856]], dtype=torch.float64),) (tensor([[-0.7054]]), tensor([-15.0471], dtype=torch.float64), tensor([[-0.8329,  0.5534, -1.4774]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([51]))
(tensor([[-0.8715,  0.4904, -1.8868]], dtype=torch.float64),) (tensor([[0.2769]]), tensor([-13.9504], dtype=torch.float64), tensor([[-0.8104,  0.5859, -0.7903]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([52]))
(tensor([[-0.8329,  0.5534, -1.4774]], dtype=torch.float64),) (tensor([[1.8138]]), tensor([-13.0771], dtype=torch.float64), tensor([[-0.8003,  0.5996, -0.3397]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([53]))
(tensor([[-0.8104,  0.5859, -0.7903]], dtype=torch.float64),) (tensor([[0.0743]]), tensor([-12.5861], dtype=torch.float64), tensor([[-0.8115,  0.5844,  0.3777]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([54]))
(tensor([[-0.8003,  0.5996, -0.3397]], dtype=torch.float64),) (tensor([[1.7848]]), tensor([-12.5465], dtype=torch.float64), tensor([[-0.8339,  0.5519,  0.7889]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([55]))
(tensor([[-0.8115,  0.5844,  0.3777]], dtype=torch.float64),) (tensor([[-0.1799]]), tensor([-12.8896], dtype=torch.float64), tensor([[-0.8723,  0.4890,  1.4730]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([56]))
(tensor([[-0.8339,  0.5519,  0.7889]], dtype=torch.float64),) (tensor([[1.8007]]), tensor([-13.6706], dtype=torch.float64), tensor([[-0.9098,  0.4150,  1.6611]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([57]))
(tensor([[-0.8723,  0.4890,  1.4730]], dtype=torch.float64),) (tensor([[-1.1911]]), tensor([-14.7026], dtype=torch.float64), tensor([[-0.9482,  0.3178,  2.0904]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([58]))
(tensor([[-0.9098,  0.4150,  1.6611]], dtype=torch.float64),) (tensor([[0.7868]]), tensor([-15.9358], dtype=torch.float64), tensor([[-0.9789,  0.2046,  2.3479]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([59]))
(tensor([[-0.9482,  0.3178,  2.0904]], dtype=torch.float64),) (tensor([[0.1276]]), tensor([-17.4600], dtype=torch.float64), tensor([[-0.9978,  0.0663,  2.7940]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([60]))
(tensor([[-0.9789,  0.2046,  2.3479]], dtype=torch.float64),) (tensor([[1.9516]]), tensor([-19.3094], dtype=torch.float64), tensor([[-0.9977, -0.0675,  2.6771]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([61]))
(tensor([[-0.9978,  0.0663,  2.7940]], dtype=torch.float64),) (tensor([[-1.1110]]), tensor([-20.3058], dtype=torch.float64), tensor([[-0.9823, -0.1876,  2.4232]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([62]))
(tensor([[-0.9977, -0.0675,  2.6771]], dtype=torch.float64),) (tensor([[-1.3549]]), tensor([-19.3842], dtype=torch.float64), tensor([[-0.9577, -0.2877,  2.0621]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([63]))
(tensor([[-0.9823, -0.1876,  2.4232]], dtype=torch.float64),) (tensor([[-1.4697]]), tensor([-17.7736], dtype=torch.float64), tensor([[-0.9324, -0.3615,  1.5620]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([64]))
(tensor([[-0.9577, -0.2877,  2.0621]], dtype=torch.float64),) (tensor([[-1.8957]]), tensor([-16.3977], dtype=torch.float64), tensor([[-0.9047, -0.4260,  1.4044]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([65]))
(tensor([[-0.9324, -0.3615,  1.5620]], dtype=torch.float64),) (tensor([[0.7573]]), tensor([-15.3471], dtype=torch.float64), tensor([[-0.8806, -0.4739,  1.0730]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([66]))
(tensor([[-0.9047, -0.4260,  1.4044]], dtype=torch.float64),) (tensor([[-0.0793]]), tensor([-14.5519], dtype=torch.float64), tensor([[-0.8578, -0.5140,  0.9221]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([67]))
(tensor([[-0.8806, -0.4739,  1.0730]], dtype=torch.float64),) (tensor([[1.3639]]), tensor([-13.9168], dtype=torch.float64), tensor([[-0.8362, -0.5485,  0.8130]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([68]))
(tensor([[-0.8578, -0.5140,  0.9221]], dtype=torch.float64),) (tensor([[1.8426]]), tensor([-13.4173], dtype=torch.float64), tensor([[-0.8291, -0.5591,  0.2558]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([69]))
(tensor([[-0.8362, -0.5485,  0.8130]], dtype=torch.float64),) (tensor([[-0.9721]]), tensor([-13.0616], dtype=torch.float64), tensor([[-0.8321, -0.5546, -0.1077]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([70]))
(tensor([[-0.8291, -0.5591,  0.2558]], dtype=torch.float64),) (tensor([[0.3717]]), tensor([-12.9587], dtype=torch.float64), tensor([[-0.8422, -0.5392, -0.3680]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([71]))
(tensor([[-0.8321, -0.5546, -0.1077]], dtype=torch.float64),) (tensor([[1.0385]]), tensor([-13.0886], dtype=torch.float64), tensor([[-0.8682, -0.4963, -1.0045]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([72]))
(tensor([[-0.8422, -0.5392, -0.3680]], dtype=torch.float64),) (tensor([[-1.5477]]), tensor([-13.5406], dtype=torch.float64), tensor([[-0.9044, -0.4268, -1.5677]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([73]))
(tensor([[-0.8682, -0.4963, -1.0045]], dtype=torch.float64),) (tensor([[-1.2734]]), tensor([-14.4445], dtype=torch.float64), tensor([[-0.9436, -0.3311, -2.0686]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([74]))
(tensor([[-0.9044, -0.4268, -1.5677]], dtype=torch.float64),) (tensor([[-1.2053]]), tensor([-15.7506], dtype=torch.float64), tensor([[-0.9776, -0.2103, -2.5114]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([75]))
(tensor([[-0.9436, -0.3311, -2.0686]], dtype=torch.float64),) (tensor([[-1.2964]]), tensor([-17.4150], dtype=torch.float64), tensor([[-0.9964, -0.0849, -2.5392]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([76]))
(tensor([[-0.9776, -0.2103, -2.5114]], dtype=torch.float64),) (tensor([[0.8664]]), tensor([-19.1036], dtype=torch.float64), tensor([[-0.9993,  0.0369, -2.4382]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([77]))
(tensor([[-0.9964, -0.0849, -2.5392]], dtype=torch.float64),) (tensor([[1.0977]]), tensor([-20.1199], dtype=torch.float64), tensor([[-0.9876,  0.1572, -2.4183]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([78]))
(tensor([[-0.9993,  0.0369, -2.4382]], dtype=torch.float64),) (tensor([[-0.0524]]), tensor([-19.6259], dtype=torch.float64), tensor([[-0.9622,  0.2723, -2.3590]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([79]))
(tensor([[-0.9876,  0.1572, -2.4183]], dtype=torch.float64),) (tensor([[-0.3904]]), tensor([-18.1698], dtype=torch.float64), tensor([[-0.9248,  0.3805, -2.2911]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([80]))
(tensor([[-0.9622,  0.2723, -2.3590]], dtype=torch.float64),) (tensor([[-0.9090]]), tensor([-16.7839], dtype=torch.float64), tensor([[-0.8851,  0.4654, -1.8749]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([81]))
(tensor([[-0.9248,  0.3805, -2.2911]], dtype=torch.float64),) (tensor([[0.8717]]), tensor([-15.4366], dtype=torch.float64), tensor([[-0.8528,  0.5222, -1.3061]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([82]))
(tensor([[-0.8851,  0.4654, -1.8749]], dtype=torch.float64),) (tensor([[1.4651]]), tensor([-14.2370], dtype=torch.float64), tensor([[-0.8267,  0.5626, -0.9617]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([83]))
(tensor([[-0.8528,  0.5222, -1.3061]], dtype=torch.float64),) (tensor([[-0.3151]]), tensor([-13.3903], dtype=torch.float64), tensor([[-0.8156,  0.5785, -0.3889]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([84]))
(tensor([[-0.8267,  0.5626, -0.9617]], dtype=torch.float64),) (tensor([[1.0057]]), tensor([-12.8913], dtype=torch.float64), tensor([[-0.8144,  0.5804, -0.0448]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([85]))
(tensor([[-0.8156,  0.5785, -0.3889]], dtype=torch.float64),) (tensor([[-0.5988]]), tensor([-12.6897], dtype=torch.float64), tensor([[-0.8207,  0.5713,  0.2215]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([86]))
(tensor([[-0.8144,  0.5804, -0.0448]], dtype=torch.float64),) (tensor([[-1.1266]]), tensor([-12.7269], dtype=torch.float64), tensor([[-0.8467,  0.5321,  0.9398]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([87]))
(tensor([[-0.8207,  0.5713,  0.2215]], dtype=torch.float64),) (tensor([[1.9321]]), tensor([-13.1101], dtype=torch.float64), tensor([[-0.8868,  0.4622,  1.6131]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([88]))
(tensor([[-0.8467,  0.5321,  0.9398]], dtype=torch.float64),) (tensor([[1.8283]]), tensor([-14.0203], dtype=torch.float64), tensor([[-0.9244,  0.3815,  1.7817]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([89]))
(tensor([[-0.8868,  0.4622,  1.6131]], dtype=torch.float64),) (tensor([[-1.1873]]), tensor([-15.1467], dtype=torch.float64), tensor([[-0.9611,  0.2761,  2.2320]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([90]))
(tensor([[-0.9244,  0.3815,  1.7817]], dtype=torch.float64),) (tensor([[1.0945]]), tensor([-16.4845], dtype=torch.float64), tensor([[-0.9887,  0.1499,  2.5863]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([91]))
(tensor([[-0.9611,  0.2761,  2.2320]], dtype=torch.float64),) (tensor([[0.9814]]), tensor([-18.2087], dtype=torch.float64), tensor([[-0.9999,  0.0136,  2.7372]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([92]))
(tensor([[-0.9887,  0.1499,  2.5863]], dtype=torch.float64),) (tensor([[0.2566]]), tensor([-20.0456], dtype=torch.float64), tensor([[-0.9935, -0.1137,  2.5515]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([93]))
(tensor([[-0.9999,  0.0136,  2.7372]], dtype=torch.float64),) (tensor([[-1.3064]]), tensor([-20.2546], dtype=torch.float64), tensor([[-0.9716, -0.2367,  2.4995]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([94]))
(tensor([[-0.9935, -0.1137,  2.5515]], dtype=torch.float64),) (tensor([[0.2219]]), tensor([-18.7783], dtype=torch.float64), tensor([[-0.9403, -0.3402,  2.1644]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([95]))
(tensor([[-0.9716, -0.2367,  2.4995]], dtype=torch.float64),) (tensor([[-1.0502]]), tensor([-17.2466], dtype=torch.float64), tensor([[-0.9009, -0.4341,  2.0370]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([96]))
(tensor([[-0.9403, -0.3402,  2.1644]], dtype=torch.float64),) (tensor([[0.8518]]), tensor([-15.8667], dtype=torch.float64), tensor([[-0.8583, -0.5131,  1.7955]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([97]))
(tensor([[-0.9009, -0.4341,  2.0370]], dtype=torch.float64),) (tensor([[0.5605]]), tensor([-14.6948], dtype=torch.float64), tensor([[-0.8280, -0.5607,  1.1300]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([98]))
(tensor([[-0.8583, -0.5131,  1.7955]], dtype=torch.float64),) (tensor([[-1.8714]]), tensor([-13.6482], dtype=torch.float64), tensor([[-0.8007, -0.5991,  0.9422]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([99]))
(tensor([[-0.8280, -0.5607,  1.1300]], dtype=torch.float64),) (tensor([[1.5518]]), tensor([-12.8853], dtype=torch.float64), tensor([[-0.7870, -0.6170,  0.4501]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([100]))
(tensor([[-0.8007, -0.5991,  0.9422]], dtype=torch.float64),) (tensor([[-0.2853]]), tensor([-12.4295], dtype=torch.float64), tensor([[-0.7938, -0.6082, -0.2229]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([101]))
(tensor([[-0.7870, -0.6170,  0.4501]], dtype=torch.float64),) (tensor([[-1.4019]]), tensor([-12.2886], dtype=torch.float64), tensor([[-0.8135, -0.5815, -0.6635]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([102]))
(tensor([[-0.7938, -0.6082, -0.2229]], dtype=torch.float64),) (tensor([[0.1034]]), tensor([-12.5302], dtype=torch.float64), tensor([[-0.8463, -0.5327, -1.1761]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([103]))
(tensor([[-0.8135, -0.5815, -0.6635]], dtype=torch.float64),) (tensor([[-0.5096]]), tensor([-13.1280], dtype=torch.float64), tensor([[-0.8802, -0.4745, -1.3469]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([104]))
(tensor([[-0.8463, -0.5327, -1.1761]], dtype=torch.float64),) (tensor([[1.5246]]), tensor([-13.9168], dtype=torch.float64), tensor([[-0.9116, -0.4110, -1.4179]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([105]))
(tensor([[-0.8802, -0.4745, -1.3469]], dtype=torch.float64),) (tensor([[1.8998]]), tensor([-14.7059], dtype=torch.float64), tensor([[-0.9423, -0.3349, -1.6406]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([106]))
(tensor([[-0.9116, -0.4110, -1.4179]], dtype=torch.float64),) (tensor([[0.5699]]), tensor([-15.6180], dtype=torch.float64), tensor([[-0.9704, -0.2414, -1.9543]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([107]))
(tensor([[-0.9423, -0.3349, -1.6406]], dtype=torch.float64),) (tensor([[-0.4170]]), tensor([-16.8019], dtype=torch.float64), tensor([[-0.9898, -0.1422, -2.0227]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([108]))
(tensor([[-0.9704, -0.2414, -1.9543]], dtype=torch.float64),) (tensor([[0.7513]]), tensor([-18.0892], dtype=torch.float64), tensor([[-0.9995, -0.0305, -2.2426]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([109]))
(tensor([[-0.9898, -0.1422, -2.0227]], dtype=torch.float64),) (tensor([[-0.7556]]), tensor([-19.4837], dtype=torch.float64), tensor([[-0.9962,  0.0876, -2.3649]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([110]))
(tensor([[-0.9995, -0.0305, -2.2426]], dtype=torch.float64),) (tensor([[-0.6625]]), tensor([-19.9696], dtype=torch.float64), tensor([[-0.9780,  0.2084, -2.4451]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([111]))
(tensor([[-0.9962,  0.0876, -2.3649]], dtype=torch.float64),) (tensor([[-0.9727]]), tensor([-18.9867], dtype=torch.float64), tensor([[-0.9487,  0.3163, -2.2371]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([112]))
(tensor([[-0.9780,  0.2084, -2.4451]], dtype=torch.float64),) (tensor([[0.3440]]), tensor([-17.5610], dtype=torch.float64), tensor([[-0.9083,  0.4184, -2.1974]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([113]))
(tensor([[-0.9487,  0.3163, -2.2371]], dtype=torch.float64),) (tensor([[-1.3168]]), tensor([-16.2014], dtype=torch.float64), tensor([[-0.8639,  0.5036, -1.9214]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([114]))
(tensor([[-0.9083,  0.4184, -2.1974]], dtype=torch.float64),) (tensor([[-0.2522]]), tensor([-14.9559], dtype=torch.float64), tensor([[-0.8248,  0.5654, -1.4628]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([115]))
(tensor([[-0.8639,  0.5036, -1.9214]], dtype=torch.float64),) (tensor([[0.5393]]), tensor([-13.8051], dtype=torch.float64), tensor([[-0.7995,  0.6006, -0.8673]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([116]))
(tensor([[-0.8248,  0.5654, -1.4628]], dtype=torch.float64),) (tensor([[1.1434]]), tensor([-12.9191], dtype=torch.float64), tensor([[-0.7850,  0.6195, -0.4760]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([117]))
(tensor([[-0.7995,  0.6006, -0.8673]], dtype=torch.float64),) (tensor([[-0.3951]]), tensor([-12.3942], dtype=torch.float64), tensor([[-0.7925,  0.6099,  0.2436]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([118]))
(tensor([[-0.7850,  0.6195, -0.4760]], dtype=torch.float64),) (tensor([[1.7004]]), tensor([-12.2669], dtype=torch.float64), tensor([[-0.8105,  0.5857,  0.6036]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([119]))
(tensor([[-0.7925,  0.6099,  0.2436]], dtype=torch.float64),) (tensor([[-0.6498]]), tensor([-12.4877], dtype=torch.float64), tensor([[-0.8374,  0.5466,  0.9482]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([120]))
(tensor([[-0.8105,  0.5857,  0.6036]], dtype=torch.float64),) (tensor([[-0.6308]]), tensor([-12.9608], dtype=torch.float64), tensor([[-0.8757,  0.4829,  1.4873]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([121]))
(tensor([[-0.8374,  0.5466,  0.9482]], dtype=torch.float64),) (tensor([[0.8608]]), tensor([-13.7676], dtype=torch.float64), tensor([[-0.9163,  0.4005,  1.8378]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([122]))
(tensor([[-0.8757,  0.4829,  1.4873]], dtype=torch.float64),) (tensor([[-0.0777]]), tensor([-14.8887], dtype=torch.float64), tensor([[-0.9528,  0.3036,  2.0723]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([123]))
(tensor([[-0.9163,  0.4005,  1.8378]], dtype=torch.float64),) (tensor([[-0.4391]]), tensor([-16.1622], dtype=torch.float64), tensor([[-0.9835,  0.1809,  2.5312]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([124]))
(tensor([[-0.9528,  0.3036,  2.0723]], dtype=torch.float64),) (tensor([[1.5412]]), tensor([-17.7650], dtype=torch.float64), tensor([[-0.9988,  0.0483,  2.6719]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([125]))
(tensor([[-0.9835,  0.1809,  2.5312]], dtype=torch.float64),) (tensor([[0.0340]]), tensor([-19.5811], dtype=torch.float64), tensor([[-0.9955, -0.0946,  2.8609]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([126]))
(tensor([[-0.9988,  0.0483,  2.6719]], dtype=torch.float64),) (tensor([[1.0183]]), tensor([-20.2854], dtype=torch.float64), tensor([[-0.9707, -0.2403,  2.9590]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([127]))
(tensor([[-0.9955, -0.0946,  2.8609]], dtype=torch.float64),) (tensor([[1.1270]]), tensor([-19.2898], dtype=torch.float64), tensor([[-0.9263, -0.3768,  2.8737]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([128]))
(tensor([[-0.9707, -0.2403,  2.9590]], dtype=torch.float64),) (tensor([[0.6329]]), tensor([-17.6127], dtype=torch.float64), tensor([[-0.8676, -0.4972,  2.6792]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([129]))
(tensor([[-0.9263, -0.3768,  2.8737]], dtype=torch.float64),) (tensor([[0.5875]]), tensor([-15.9302], dtype=torch.float64), tensor([[-0.8053, -0.5929,  2.2855]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([130]))
(tensor([[-0.8676, -0.4972,  2.6792]], dtype=torch.float64),) (tensor([[-0.1390]]), tensor([-14.3283], dtype=torch.float64), tensor([[-0.7445, -0.6676,  1.9265]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([131]))
(tensor([[-0.8053, -0.5929,  2.2855]], dtype=torch.float64),) (tensor([[0.5712]]), tensor([-12.9294], dtype=torch.float64), tensor([[-0.6890, -0.7247,  1.5943]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([132]))
(tensor([[-0.7445, -0.6676,  1.9265]], dtype=torch.float64),) (tensor([[1.1231]]), tensor([-11.8162], dtype=torch.float64), tensor([[-0.6581, -0.7529,  0.8369]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([133]))
(tensor([[-0.6890, -0.7247,  1.5943]], dtype=torch.float64),) (tensor([[-1.4256]]), tensor([-10.9467], dtype=torch.float64), tensor([[-0.6447, -0.7644,  0.3527]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([134]))
(tensor([[-0.6581, -0.7529,  0.8369]], dtype=torch.float64),) (tensor([[0.5366]]), tensor([-10.4306], dtype=torch.float64), tensor([[-0.6538, -0.7566, -0.2393]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([135]))
(tensor([[-0.6447, -0.7644,  0.3527]], dtype=torch.float64),) (tensor([[-0.1243]]), tensor([-10.3400], dtype=torch.float64), tensor([[-0.6872, -0.7264, -0.9011]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([136]))
(tensor([[-0.6538, -0.7566, -0.2393]], dtype=torch.float64),) (tensor([[-0.6287]]), tensor([-10.6682], dtype=torch.float64), tensor([[-0.7366, -0.6764, -1.4062]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([137]))
(tensor([[-0.6872, -0.7264, -0.9011]], dtype=torch.float64),) (tensor([[0.2646]]), tensor([-11.3956], dtype=torch.float64), tensor([[-0.7990, -0.6014, -1.9517]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([138]))
(tensor([[-0.7366, -0.6764, -1.4062]], dtype=torch.float64),) (tensor([[-0.2546]]), tensor([-12.4987], dtype=torch.float64), tensor([[-0.8657, -0.5005, -2.4212]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([139]))
(tensor([[-0.7990, -0.6014, -1.9517]], dtype=torch.float64),) (tensor([[-0.1235]]), tensor([-13.9772], dtype=torch.float64), tensor([[-0.9305, -0.3663, -2.9832]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([140]))
(tensor([[-0.8657, -0.5005, -2.4212]], dtype=torch.float64),) (tensor([[-1.2441]]), tensor([-15.8978], dtype=torch.float64), tensor([[-0.9764, -0.2159, -3.1485]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([141]))
(tensor([[-0.9305, -0.3663, -2.9832]], dtype=torch.float64),) (tensor([[0.7293]]), tensor([-17.9922], dtype=torch.float64), tensor([[-0.9981, -0.0622, -3.1062]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([142]))
(tensor([[-0.9764, -0.2159, -3.1485]], dtype=torch.float64),) (tensor([[1.3610]]), tensor([-19.8858], dtype=torch.float64), tensor([[-0.9956,  0.0939, -3.1258]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([143]))
(tensor([[-0.9981, -0.0622, -3.1062]], dtype=torch.float64),) (tensor([[0.1805]]), tensor([-20.6099], dtype=torch.float64), tensor([[-0.9712,  0.2383, -2.9319]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([144]))
(tensor([[-0.9956,  0.0939, -3.1258]], dtype=torch.float64),) (tensor([[0.8237]]), tensor([-19.4484], dtype=torch.float64), tensor([[-0.9271,  0.3748, -2.8705]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([145]))
(tensor([[-0.9712,  0.2383, -2.9319]], dtype=torch.float64),) (tensor([[-0.7822]]), tensor([-17.6190], dtype=torch.float64), tensor([[-0.8710,  0.4913, -2.5892]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([146]))
(tensor([[-0.9271,  0.3748, -2.8705]], dtype=torch.float64),) (tensor([[0.0012]]), tensor([-15.9284], dtype=torch.float64), tensor([[-0.8112,  0.5847, -2.2184]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([147]))
(tensor([[-0.8710,  0.4913, -2.5892]], dtype=torch.float64),) (tensor([[0.0153]]), tensor([-14.3362], dtype=torch.float64), tensor([[-0.7560,  0.6546, -1.7824]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([148]))
(tensor([[-0.8112,  0.5847, -2.2184]], dtype=torch.float64),) (tensor([[-0.0170]]), tensor([-12.9782], dtype=torch.float64), tensor([[-0.7127,  0.7015, -1.2774]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([149]))
(tensor([[-0.7560,  0.6546, -1.7824]], dtype=torch.float64),) (tensor([[0.0942]]), tensor([-11.9079], dtype=torch.float64), tensor([[-0.6908,  0.7231, -0.6145]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([150]))
(tensor([[-0.7127,  0.7015, -1.2774]], dtype=torch.float64),) (tensor([[0.9113]]), tensor([-11.1824], dtype=torch.float64), tensor([[-0.6960,  0.7181,  0.1445]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([151]))
(tensor([[-0.6908,  0.7231, -0.6145]], dtype=torch.float64),) (tensor([[1.4448]]), tensor([-10.9100], dtype=torch.float64), tensor([[-0.7184,  0.6957,  0.6334]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([152]))
(tensor([[-0.6960,  0.7181,  0.1445]], dtype=torch.float64),) (tensor([[-0.3309]]), tensor([-11.0926], dtype=torch.float64), tensor([[-0.7626,  0.6468,  1.3186]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([153]))
(tensor([[-0.7184,  0.6957,  0.6334]], dtype=torch.float64),) (tensor([[1.0898]]), tensor([-11.7263], dtype=torch.float64), tensor([[-0.8169,  0.5768,  1.7717]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([154]))
(tensor([[-0.7626,  0.6468,  1.3186]], dtype=torch.float64),) (tensor([[-0.2139]]), tensor([-12.7515], dtype=torch.float64), tensor([[-0.8798,  0.4754,  2.3895]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([155]))
(tensor([[-0.8169,  0.5768,  1.7717]], dtype=torch.float64),) (tensor([[1.2345]]), tensor([-14.1977], dtype=torch.float64), tensor([[-0.9361,  0.3518,  2.7182]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([156]))
(tensor([[-0.8798,  0.4754,  2.3895]], dtype=torch.float64),) (tensor([[-0.1853]]), tensor([-15.9680], dtype=torch.float64), tensor([[-0.9785,  0.2063,  3.0335]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([157]))
(tensor([[-0.9361,  0.3518,  2.7182]], dtype=torch.float64),) (tensor([[0.3434]]), tensor([-17.9124], dtype=torch.float64), tensor([[-0.9992,  0.0410,  3.3357]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([158]))
(tensor([[-0.9785,  0.2063,  3.0335]], dtype=torch.float64),) (tensor([[0.9834]]), tensor([-20.1484], dtype=torch.float64), tensor([[-0.9927, -0.1204,  3.2338]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([159]))
(tensor([[-0.9992,  0.0410,  3.3357]], dtype=torch.float64),) (tensor([[-0.8847]]), tensor([-20.7971], dtype=torch.float64), tensor([[-0.9622, -0.2724,  3.1044]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([160]))
(tensor([[-0.9927, -0.1204,  3.2338]], dtype=torch.float64),) (tensor([[-0.2601]]), tensor([-19.2564], dtype=torch.float64), tensor([[-0.9101, -0.4144,  3.0275]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([161]))
(tensor([[-0.9622, -0.2724,  3.1044]], dtype=torch.float64),) (tensor([[0.8492]]), tensor([-17.3777], dtype=torch.float64), tensor([[-0.8454, -0.5341,  2.7223]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([162]))
(tensor([[-0.9101, -0.4144,  3.0275]], dtype=torch.float64),) (tensor([[0.0376]]), tensor([-15.5988], dtype=torch.float64), tensor([[-0.7743, -0.6329,  2.4365]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([163]))
(tensor([[-0.8454, -0.5341,  2.7223]], dtype=torch.float64),) (tensor([[0.7649]]), tensor([-13.9507], dtype=torch.float64), tensor([[-0.7035, -0.7107,  2.1052]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([164]))
(tensor([[-0.7743, -0.6329,  2.4365]], dtype=torch.float64),) (tensor([[0.9559]]), tensor([-12.5401], dtype=torch.float64), tensor([[-0.6405, -0.7679,  1.7017]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([165]))
(tensor([[-0.7035, -0.7107,  2.1052]], dtype=torch.float64),) (tensor([[0.8633]]), tensor([-11.3420], dtype=torch.float64), tensor([[-0.5935, -0.8049,  1.1974]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([166]))
(tensor([[-0.6405, -0.7679,  1.7017]], dtype=torch.float64),) (tensor([[0.4778]]), tensor([-10.3857], dtype=torch.float64), tensor([[-0.5640, -0.8258,  0.7216]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([167]))
(tensor([[-0.5935, -0.8049,  1.1974]], dtype=torch.float64),) (tensor([[0.8523]]), tensor([-9.7248], dtype=torch.float64), tensor([[-0.5584, -0.8295,  0.1351]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([168]))
(tensor([[-0.5640, -0.8258,  0.7216]], dtype=torch.float64),) (tensor([[0.2184]]), tensor([-9.3981], dtype=torch.float64), tensor([[-0.5699, -0.8217, -0.2784]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([169]))
(tensor([[-0.5584, -0.8295,  0.1351]], dtype=torch.float64),) (tensor([[1.3916]]), tensor([-9.3848], dtype=torch.float64), tensor([[-0.6106, -0.7920, -1.0066]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([170]))
(tensor([[-0.5699, -0.8217, -0.2784]], dtype=torch.float64),) (tensor([[-0.7467]]), tensor([-9.7616], dtype=torch.float64), tensor([[-0.6750, -0.7379, -1.6829]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([171]))
(tensor([[-0.6106, -0.7920, -1.0066]], dtype=torch.float64),) (tensor([[-0.5485]]), tensor([-10.6358], dtype=torch.float64), tensor([[-0.7474, -0.6644, -2.0650]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([172]))
(tensor([[-0.6750, -0.7379, -1.6829]], dtype=torch.float64),) (tensor([[1.1417]]), tensor([-11.8259], dtype=torch.float64), tensor([[-0.8315, -0.5556, -2.7518]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([173]))
(tensor([[-0.7474, -0.6644, -2.0650]], dtype=torch.float64),) (tensor([[-1.2565]]), tensor([-13.4600], dtype=torch.float64), tensor([[-0.9092, -0.4164, -3.1918]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([174]))
(tensor([[-0.8315, -0.5556, -2.7518]], dtype=torch.float64),) (tensor([[-0.1558]]), tensor([-15.5634], dtype=torch.float64), tensor([[-0.9674, -0.2531, -3.4716]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([175]))
(tensor([[-0.9092, -0.4164, -3.1918]], dtype=torch.float64),) (tensor([[0.2164]]), tensor([-17.8131], dtype=torch.float64), tensor([[-0.9980, -0.0639, -3.8382]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([176]))
(tensor([[-0.9674, -0.2531, -3.4716]], dtype=torch.float64),) (tensor([[-1.1780]]), tensor([-20.3694], dtype=torch.float64), tensor([[-0.9916,  0.1292, -3.8703]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([177]))
(tensor([[-0.9980, -0.0639, -3.8382]], dtype=torch.float64),) (tensor([[0.1054]]), tensor([-21.4102], dtype=torch.float64), tensor([[-0.9516,  0.3074, -3.6592]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([178]))
(tensor([[-0.9916,  0.1292, -3.8703]], dtype=torch.float64),) (tensor([[0.7612]]), tensor([-19.8203], dtype=torch.float64), tensor([[-0.8855,  0.4647, -3.4164]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([179]))
(tensor([[-0.9516,  0.3074, -3.6592]], dtype=torch.float64),) (tensor([[0.0815]]), tensor([-17.4941], dtype=torch.float64), tensor([[-0.8026,  0.5965, -3.1170]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([180]))
(tensor([[-0.8855,  0.4647, -3.4164]], dtype=torch.float64),) (tensor([[-0.3276]]), tensor([-15.3953], dtype=torch.float64), tensor([[-0.7131,  0.7010, -2.7528]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([181]))
(tensor([[-0.8026,  0.5965, -3.1170]], dtype=torch.float64),) (tensor([[-0.5546]]), tensor([-13.5206], dtype=torch.float64), tensor([[-0.6335,  0.7737, -2.1578]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([182]))
(tensor([[-0.7131,  0.7010, -2.7528]], dtype=torch.float64),) (tensor([[0.4616]]), tensor([-11.8560], dtype=torch.float64), tensor([[-0.5799,  0.8147, -1.3485]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([183]))
(tensor([[-0.6335,  0.7737, -2.1578]], dtype=torch.float64),) (tensor([[1.5265]]), tensor([-10.4879], dtype=torch.float64), tensor([[-0.5448,  0.8386, -0.8513]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([184]))
(tensor([[-0.5799,  0.8147, -1.3485]], dtype=torch.float64),) (tensor([[-0.7586]]), tensor([-9.6111], dtype=torch.float64), tensor([[-0.5378,  0.8431, -0.1663]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([185]))
(tensor([[-0.5448,  0.8386, -0.8513]], dtype=torch.float64),) (tensor([[0.3742]]), tensor([-9.2133], dtype=torch.float64), tensor([[-0.5637,  0.8260,  0.6206]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([186]))
(tensor([[-0.5378,  0.8431, -0.1663]], dtype=torch.float64),) (tensor([[1.0299]]), tensor([-9.2770], dtype=torch.float64), tensor([[-0.6208,  0.7839,  1.4197]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([187]))
(tensor([[-0.5637,  0.8260,  0.6206]], dtype=torch.float64),) (tensor([[1.1974]]), tensor([-9.9168], dtype=torch.float64), tensor([[-0.6953,  0.7187,  1.9802]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([188]))
(tensor([[-0.6208,  0.7839,  1.4197]], dtype=torch.float64),) (tensor([[-0.1832]]), tensor([-11.0291], dtype=torch.float64), tensor([[-0.7814,  0.6240,  2.5616]], dtype=torch.float64), tensor([False]), tensor([0]), tensor([189]))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-105-775f5c48a163&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">     12</span> learner=SACLearner(dls,agent=agent,cbs=[ExperienceReplay(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),SACCriticTrainer],
<span class="ansi-green-intense-fg ansi-bold">     13</span>                    metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])
<span class="ansi-green-fg">---&gt; 14</span><span class="ansi-red-fg"> </span>learner<span class="ansi-blue-fg">.</span>fit<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">30</span><span class="ansi-blue-fg">,</span>lr<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.001</span><span class="ansi-blue-fg">,</span>wd<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, n_epoch, lr, wd, cbs, reset_opt)</span>
<span class="ansi-green-intense-fg ansi-bold">    203</span>             self<span class="ansi-blue-fg">.</span>opt<span class="ansi-blue-fg">.</span>set_hypers<span class="ansi-blue-fg">(</span>lr<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>lr <span class="ansi-green-fg">if</span> lr <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span> <span class="ansi-green-fg">else</span> lr<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    204</span>             self<span class="ansi-blue-fg">.</span>n_epoch <span class="ansi-blue-fg">=</span> n_epoch
<span class="ansi-green-fg">--&gt; 205</span><span class="ansi-red-fg">             </span>self<span class="ansi-blue-fg">.</span>_with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>_do_fit<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;fit&#39;</span><span class="ansi-blue-fg">,</span> CancelFitException<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>_end_cleanup<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    206</span> 
<span class="ansi-green-intense-fg ansi-bold">    207</span>     <span class="ansi-green-fg">def</span> _end_cleanup<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>dl<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>xb<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>yb<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>pred<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>loss <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">(</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">(</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span><span class="ansi-green-fg">None</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_with_events</span><span class="ansi-blue-fg">(self, f, event_type, ex, final)</span>
<span class="ansi-green-intense-fg ansi-bold">    152</span> 
<span class="ansi-green-intense-fg ansi-bold">    153</span>     <span class="ansi-green-fg">def</span> _with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> f<span class="ansi-blue-fg">,</span> event_type<span class="ansi-blue-fg">,</span> ex<span class="ansi-blue-fg">,</span> final<span class="ansi-blue-fg">=</span>noop<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 154</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>       self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;before_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>       <span class="ansi-blue-fg">;</span>f<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    155</span>         <span class="ansi-green-fg">except</span> ex<span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_cancel_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    156</span>         <span class="ansi-green-fg">finally</span><span class="ansi-blue-fg">:</span>   self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>        <span class="ansi-blue-fg">;</span>final<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_do_fit</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    194</span>         <span class="ansi-green-fg">for</span> epoch <span class="ansi-green-fg">in</span> range<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>n_epoch<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    195</span>             self<span class="ansi-blue-fg">.</span>epoch<span class="ansi-blue-fg">=</span>epoch
<span class="ansi-green-fg">--&gt; 196</span><span class="ansi-red-fg">             </span>self<span class="ansi-blue-fg">.</span>_with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>_do_epoch<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;epoch&#39;</span><span class="ansi-blue-fg">,</span> CancelEpochException<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    197</span> 
<span class="ansi-green-intense-fg ansi-bold">    198</span>     <span class="ansi-green-fg">def</span> fit<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> n_epoch<span class="ansi-blue-fg">,</span> lr<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> wd<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> cbs<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> reset_opt<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_with_events</span><span class="ansi-blue-fg">(self, f, event_type, ex, final)</span>
<span class="ansi-green-intense-fg ansi-bold">    152</span> 
<span class="ansi-green-intense-fg ansi-bold">    153</span>     <span class="ansi-green-fg">def</span> _with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> f<span class="ansi-blue-fg">,</span> event_type<span class="ansi-blue-fg">,</span> ex<span class="ansi-blue-fg">,</span> final<span class="ansi-blue-fg">=</span>noop<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 154</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>       self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;before_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>       <span class="ansi-blue-fg">;</span>f<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    155</span>         <span class="ansi-green-fg">except</span> ex<span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_cancel_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    156</span>         <span class="ansi-green-fg">finally</span><span class="ansi-blue-fg">:</span>   self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>        <span class="ansi-blue-fg">;</span>final<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_do_epoch</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    188</span> 
<span class="ansi-green-intense-fg ansi-bold">    189</span>     <span class="ansi-green-fg">def</span> _do_epoch<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 190</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_do_epoch_train<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    191</span>         self<span class="ansi-blue-fg">.</span>_do_epoch_validate<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    192</span> 

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_do_epoch_train</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    180</span>     <span class="ansi-green-fg">def</span> _do_epoch_train<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    181</span>         self<span class="ansi-blue-fg">.</span>dl <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dls<span class="ansi-blue-fg">.</span>train
<span class="ansi-green-fg">--&gt; 182</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>all_batches<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;train&#39;</span><span class="ansi-blue-fg">,</span> CancelTrainException<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    183</span> 
<span class="ansi-green-intense-fg ansi-bold">    184</span>     <span class="ansi-green-fg">def</span> _do_epoch_validate<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> ds_idx<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> dl<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_with_events</span><span class="ansi-blue-fg">(self, f, event_type, ex, final)</span>
<span class="ansi-green-intense-fg ansi-bold">    152</span> 
<span class="ansi-green-intense-fg ansi-bold">    153</span>     <span class="ansi-green-fg">def</span> _with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> f<span class="ansi-blue-fg">,</span> event_type<span class="ansi-blue-fg">,</span> ex<span class="ansi-blue-fg">,</span> final<span class="ansi-blue-fg">=</span>noop<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 154</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>       self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;before_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>       <span class="ansi-blue-fg">;</span>f<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    155</span>         <span class="ansi-green-fg">except</span> ex<span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_cancel_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    156</span>         <span class="ansi-green-fg">finally</span><span class="ansi-blue-fg">:</span>   self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>        <span class="ansi-blue-fg">;</span>final<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">all_batches</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    158</span>     <span class="ansi-green-fg">def</span> all_batches<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    159</span>         self<span class="ansi-blue-fg">.</span>n_iter <span class="ansi-blue-fg">=</span> len<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>dl<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 160</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">for</span> o <span class="ansi-green-fg">in</span> enumerate<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>dl<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>one_batch<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>o<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    161</span> 
<span class="ansi-green-intense-fg ansi-bold">    162</span>     <span class="ansi-green-fg">def</span> _do_one_batch<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">one_batch</span><span class="ansi-blue-fg">(self, i, b)</span>
<span class="ansi-green-intense-fg ansi-bold">    176</span>         self<span class="ansi-blue-fg">.</span>iter <span class="ansi-blue-fg">=</span> i
<span class="ansi-green-intense-fg ansi-bold">    177</span>         self<span class="ansi-blue-fg">.</span>_split<span class="ansi-blue-fg">(</span>b<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 178</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>_with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>_do_one_batch<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;batch&#39;</span><span class="ansi-blue-fg">,</span> CancelBatchException<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    179</span> 
<span class="ansi-green-intense-fg ansi-bold">    180</span>     <span class="ansi-green-fg">def</span> _do_epoch_train<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_with_events</span><span class="ansi-blue-fg">(self, f, event_type, ex, final)</span>
<span class="ansi-green-intense-fg ansi-bold">    152</span> 
<span class="ansi-green-intense-fg ansi-bold">    153</span>     <span class="ansi-green-fg">def</span> _with_events<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> f<span class="ansi-blue-fg">,</span> event_type<span class="ansi-blue-fg">,</span> ex<span class="ansi-blue-fg">,</span> final<span class="ansi-blue-fg">=</span>noop<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 154</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>       self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;before_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>       <span class="ansi-blue-fg">;</span>f<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    155</span>         <span class="ansi-green-fg">except</span> ex<span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_cancel_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    156</span>         <span class="ansi-green-fg">finally</span><span class="ansi-blue-fg">:</span>   self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">f&#39;after_{event_type}&#39;</span><span class="ansi-blue-fg">)</span>        <span class="ansi-blue-fg">;</span>final<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastai/learner.py</span> in <span class="ansi-cyan-fg">_do_one_batch</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    163</span>         self<span class="ansi-blue-fg">.</span>pred <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>model<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>self<span class="ansi-blue-fg">.</span>xb<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    164</span>         self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;after_pred&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 165</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">if</span> len<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>yb<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>loss <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>loss_func<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>pred<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>self<span class="ansi-blue-fg">.</span>yb<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    166</span>         self<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;after_loss&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    167</span>         <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> self<span class="ansi-blue-fg">.</span>training <span class="ansi-green-fg">or</span> <span class="ansi-green-fg">not</span> len<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>yb<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span>

<span class="ansi-green-fg">&lt;ipython-input-101-802ee9451656&gt;</span> in <span class="ansi-cyan-fg">update_parameters</span><span class="ansi-blue-fg">(self, learn, *yb)</span>
<span class="ansi-green-intense-fg ansi-bold">     97</span>         policy_loss <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>alpha <span class="ansi-blue-fg">*</span> log_pi<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-</span> min_qf_pi<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>mean<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># J = stD,tN[ * log(f(t;st)|st)  Q(st,f(t;st))]</span>
<span class="ansi-green-intense-fg ansi-bold">     98</span> 
<span class="ansi-green-fg">---&gt; 99</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>policy_optim<span class="ansi-blue-fg">.</span>zero_grad<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    100</span>         policy_loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    101</span>         self<span class="ansi-blue-fg">.</span>policy_optim<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/optim/optimizer.py</span> in <span class="ansi-cyan-fg">zero_grad</span><span class="ansi-blue-fg">(self, set_to_none)</span>
<span class="ansi-green-intense-fg ansi-bold">    189</span>                             p<span class="ansi-blue-fg">.</span>grad<span class="ansi-blue-fg">.</span>detach_<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    190</span>                         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 191</span><span class="ansi-red-fg">                             </span>p<span class="ansi-blue-fg">.</span>grad<span class="ansi-blue-fg">.</span>requires_grad_<span class="ansi-blue-fg">(</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    192</span>                         p<span class="ansi-blue-fg">.</span>grad<span class="ansi-blue-fg">.</span>zero_<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    193</span> 

<span class="ansi-green-fg">/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/tensor.py</span> in <span class="ansi-cyan-fg">grad</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    951</span>                           <span class="ansi-blue-fg">&#34;non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See &#34;</span>
<span class="ansi-green-intense-fg ansi-bold">    952</span>                           &#34;github.com/pytorch/pytorch/pull/30531 for more informations.&#34;, stacklevel=2)
<span class="ansi-green-fg">--&gt; 953</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>_grad
<span class="ansi-green-intense-fg ansi-bold">    954</span> 
<span class="ansi-green-intense-fg ansi-bold">    955</span>     <span class="ansi-blue-fg">@</span>grad<span class="ansi-blue-fg">.</span>setter

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

