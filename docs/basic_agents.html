---

title: Basic Action Selection

keywords: fastai
sidebar: home_sidebar

summary: "Methods of exploratively selecting actions based on a model state input."
description: "Methods of exploratively selecting actions based on a model state input."
nb_path: "nbs/03_basic_agents.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/03_basic_agents.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_LAUNCH_BLOCKING&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#34;Note these are modified versions of &#39;Shmuma/Ptan&#39;. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.&#34;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ActionSelector" class="doc_header"><code>class</code> <code>ActionSelector</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L26" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ActionSelector</code>()</p>
</blockquote>
<p>Abstract class which converts scores to the actions.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ArgmaxActionSelector" class="doc_header"><code>class</code> <code>ArgmaxActionSelector</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L30" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ArgmaxActionSelector</code>() :: <a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a></p>
</blockquote>
<p>Selects actions using argmax.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EpsilonGreedyActionSelector" class="doc_header"><code>class</code> <code>EpsilonGreedyActionSelector</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L37" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EpsilonGreedyActionSelector</code>(<strong><code>epsilon</code></strong>:<code>float</code>=<em><code>0.05</code></em>, <strong><code>selector</code></strong>:<a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a>=<em><code>&lt;fastrl.basic_agents.ArgmaxActionSelector object at 0x7fd038312150&gt;</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a></p>
</blockquote>
<p>EpsilonGreedyActionSelector(epsilon: float = 0.05, selector: fastrl.basic_agents.ActionSelector = &lt;fastrl.basic_agents.ArgmaxActionSelector object at 0x7fd038312150&gt;)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ProbabilityActionSelector" class="doc_header"><code>class</code> <code>ProbabilityActionSelector</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L50" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ProbabilityActionSelector</code>() :: <a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a></p>
</blockquote>
<p>Converts probabilities of actions into action by sampling them.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Basic-Agents">Basic Agents<a class="anchor-link" href="#Basic-Agents"> </a></h1><blockquote><p>Basic Agent classes for handling models and actions. Details, please ref <code>basic_train</code></p>
</blockquote>
<p>There is an important difference between <code>Learner</code>'s, <code>nn.Module</code>'s, and <code>Agent</code>'s.</p>
<p><code>Learners</code>:- Ref <code>basic_train</code>
<code>nn.Module</code>:</p>
<ul>
<li>Contain only <code>pytorch</code> related code.</li>
<li>Function as the brain of any of these agents and are the objects to be optimized.</li>
<li>Are highly portable, however for runtime usage are too "dumb" or simple to be practical. If by themselves, extra code needs to wrap them to handle environments.</li>
</ul>
<p><code>Agent</code> (<code>agent_core</code>):</p>
<ul>
<li>Contain a <code>nn.Module</code> and a limited number of <code>fastrl</code> objects. Unlike <code>nn.Module</code>, these can maintain a state.</li>
<li>Function as the interface between the <code>nn.Module</code> and the environments. They have 2 goals:<ul>
<li>Convert states into something the <code>nn.Module</code> can interpret.</li>
<li>Modify the <code>nn.Module</code> output (actions) for randomized exploration.</li>
</ul>
</li>
<li>Designed to be highly portable only requiring <a href="/fast-reinforcement-learning-2/basic_agents.html"><code>basic_agents</code></a> as a dependency. These should allow for easily saving, and using in environments where <code>fastrl</code> might not necessarily be installed.</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_states_preprocessor" class="doc_header"><code>default_states_preprocessor</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L58" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_states_preprocessor</code>(<strong><code>s</code></strong>, <strong><code>dtype</code></strong>=<em><code>float32</code></em>)</p>
</blockquote>
<p>Convert list of states into the form suitable for model. By default we assume Variable.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="float32_preprocessor" class="doc_header"><code>float32_preprocessor</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L63" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>float32_preprocessor</code>(<strong><code>s</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BaseAgent" class="doc_header"><code>class</code> <code>BaseAgent</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BaseAgent</code>(<strong><code>model</code></strong>:<code>Module</code>=<em><code>None</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#BaseAgent"><code>BaseAgent</code></a></p>
</blockquote>
<p>BaseAgent(model: torch.nn.modules.module.Module = None)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TestAgent" class="doc_header"><code>class</code> <code>TestAgent</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TestAgent</code>(<strong><code>model</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>env</code></strong>:<code>object</code>=<em><code>None</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#BaseAgent"><code>BaseAgent</code></a></p>
</blockquote>
<p>TestAgent(model: torch.nn.modules.module.Module = None, env: object = None)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">add_docs</span><span class="p">(</span><span class="n">BaseAgent</span><span class="o">.</span><span class="n">initial_state</span><span class="p">,</span><span class="s2">&quot;Should create initial empty state for the agent. It will be called for the start of the episode.&quot;</span><span class="p">)</span>
<span class="n">add_docs</span><span class="p">(</span><span class="n">BaseAgent</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span><span class="n">textwrap</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Convert observations and state list `sl` into actions to take. Agent state list `asl` may also be used by the agent.</span>
<span class="s2">         It is expected that `asl` is likely either going to be an internal state tracked by the agent, or it is simply a parameter used during subclassing.</span>
<span class="s2">         The `include_batch_dim` should toggle whether to remove/include the batch dim of an action. Naturally, `gym` envs don&#39;t understand batch dimensions.&quot;&quot;&quot;</span>
        <span class="p">))</span>
<span class="n">add_docs</span><span class="p">(</span><span class="n">BaseAgent</span><span class="p">,</span><span class="s2">&quot;Abstract Agent interface&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">IN_NOTEBOOK</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
    <span class="kn">import</span> <span class="nn">PIL.Image</span>

<span class="n">env</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
<span class="n">agent</span><span class="o">=</span><span class="n">TestAgent</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>

<span class="n">done</span><span class="p">,</span><span class="n">episode_count</span><span class="p">,</span><span class="n">max_episodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span><span class="n">s</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">s</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">_</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">agent</span><span class="p">(</span><span class="n">s</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">im</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">)</span>
    <span class="n">new_im</span><span class="o">=</span><span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
    <span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">new_im</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">episode_count</span><span class="o">&gt;</span><span class="n">max_episodes</span><span class="p">:</span><span class="k">break</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span><span class="n">episode_count</span><span class="o">+=</span><span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAIAAAD9V4nPAAAGmElEQVR4nO3dzU0CURSAUTE0QR1ahnVATVCHZWgdljEuTAjxh5Dg8Ea/c1bAgtwN+fIuw7CapukOAKruRw8AACMJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghA2nr0ANDyetgdHz9s9wMnAT4IIczoNHvAMlmNApAmhDCj88tP50VYAiEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQhjp9bAbPQLUCSHM62G7Hz0CcI4QApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhHCR1RVGvTNwCSEEIE0IAUhbjx4AQp7ftqdPnzaHUZMAR06EcCOfKvjtK8DtCSHcwk/N00IYTggBSBNCmJ1jHyyZEMLszl8U87KXSRhJCGF2jztXh8JyCSGM5BcUMJwQwi08bQ5fm6eCsASraZpGzwB/wJU39jz9IvAXN6U+v3A9IYSLLPMO1z6/cD2rUQAAgCqrUbiI1Sj8V1ajAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKT59wkA0pwIAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIO0dCGs+V6FnPbIAAAAASUVORK5CYII=
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DiscreteAgent" class="doc_header"><code>class</code> <code>DiscreteAgent</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L91" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DiscreteAgent</code>(<strong><code>model</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>a_selector</code></strong>:<a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a>=<em><code>None</code></em>, <strong><code>device</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>preprocessor</code></strong>:<code>Callable</code>=<em><code>default_states_preprocessor</code></em>, <strong><code>apply_softmax</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#BaseAgent"><code>BaseAgent</code></a></p>
</blockquote>
<p>DiscreteAgent a simple discrete action selector.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DQNAgent" class="doc_header"><code>class</code> <code>DQNAgent</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L117" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DQNAgent</code>(<strong><code>model</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>a_selector</code></strong>:<a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a>=<em><code>None</code></em>, <strong><code>device</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>preprocessor</code></strong>:<code>Callable</code>=<em><code>default_states_preprocessor</code></em>, <strong><code>apply_softmax</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#DiscreteAgent"><code>DiscreteAgent</code></a></p>
</blockquote>
<p>DQNAgent is a memoryless DQN agent which calculates Q values from the observations and  converts them into the actions using a_selector.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">add_docs</span><span class="p">(</span><span class="n">DQNAgent</span><span class="p">,</span><span class="fm">__call__</span><span class="o">=</span><span class="s1">&#39;DQNAgents will likely never have `asl` passed and used. This is however here for novel DQN implimentations.&#39;</span><span class="p">,</span>
         <span class="n">safe_unbatch</span><span class="o">=</span><span class="s1">&#39;Will remove the batch dim from `o` if `o` represents a single item.&#39;</span><span class="p">,</span>
         <span class="n">split_v</span><span class="o">=</span><span class="s1">&#39;In the event that `v` is a tuple, then there is multle ouputs from the `model`. Primarly used for A2C.&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
<span class="n">agent</span><span class="o">=</span><span class="n">DQNAgent</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">()),</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>

<span class="n">done</span><span class="p">,</span><span class="n">episode_count</span><span class="p">,</span><span class="n">max_episodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span><span class="n">s</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">s</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">_</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">agent</span><span class="p">(</span><span class="n">s</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">im</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">)</span>
    <span class="n">new_im</span><span class="o">=</span><span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
    <span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">new_im</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">episode_count</span><span class="o">&gt;</span><span class="n">max_episodes</span><span class="p">:</span><span class="k">break</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span><span class="n">episode_count</span><span class="o">+=</span><span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TargetNet" class="doc_header"><code>class</code> <code>TargetNet</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L123" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TargetNet</code>(<strong><code>model</code></strong>)</p>
</blockquote>
<p>Wrapper around model which provides copy of it instead of trained weights.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PolicyAgent" class="doc_header"><code>class</code> <code>PolicyAgent</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L141" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PolicyAgent</code>(<strong><code>model</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>a_selector</code></strong>:<a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a>=<em><code>None</code></em>, <strong><code>device</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>preprocessor</code></strong>:<code>Callable</code>=<em><code>default_states_preprocessor</code></em>, <strong><code>apply_softmax</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#DiscreteAgent"><code>DiscreteAgent</code></a></p>
</blockquote>
<p>Policy agent gets action probabilities from the model and samples actions from it.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ActorCriticAgent" class="doc_header"><code>class</code> <code>ActorCriticAgent</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/basic_agents.py#L147" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ActorCriticAgent</code>(<strong><code>model</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>a_selector</code></strong>:<a href="/fast-reinforcement-learning-2/basic_agents.html#ActionSelector"><code>ActionSelector</code></a>=<em><code>None</code></em>, <strong><code>device</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>preprocessor</code></strong>:<code>Callable</code>=<em><code>default_states_preprocessor</code></em>, <strong><code>apply_softmax</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#PolicyAgent"><code>PolicyAgent</code></a></p>
</blockquote>
<p>Policy agent which returns policy and value tensors from observations. Value are stored in agent's state      and could be reused for rollouts calculations by ExperienceSource.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LinearA2C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearA2C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">fx</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">fx</span><span class="p">)</span>
<span class="n">model</span><span class="o">=</span><span class="n">LinearA2C</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">agent</span><span class="o">=</span><span class="n">PolicyAgent</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">()),</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">agent</span><span class="p">([</span> <span class="mf">0.044186</span><span class="p">,</span><span class="o">-</span><span class="mf">0.021265</span><span class="p">,</span><span class="mf">0.033516</span><span class="p">,</span><span class="o">-</span><span class="mf">0.011447</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[ 0.0442, -0.0213,  0.0335, -0.0114]])
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(1, array([[0., 0., 0., 0.]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

