---

title: DADS


keywords: fastai
sidebar: home_sidebar

summary: "Diversity Is All You Need"
description: "Diversity Is All You Need"
nb_path: "nbs/14c_actorcritic.dads.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/14c_actorcritic.dads.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_LAUNCH_BLOCKING&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GMM">GMM<a class="anchor-link" href="#GMM"> </a></h2><blockquote><p>A neural net with a gaussian probability distribution component.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Gaussian Mixture Models parameterized by neural nets can be used for:</p>
<ul>
<li>Giving the probability that a given <code>state</code> + <code>action</code> will result in <code>next_state</code></li>
<li>Predict what the <code>next_state</code> will look like given a <code>state</code> + <code>action</code></li>
</ul>
<p>Hopefully from the list above, it is understandable <em>why</em> a GMM can be useful.
Since it has a probabilistic component, you can avoid instances of (easily) overfitting.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="OptionalClampLinear" class="doc_header"><code>class</code> <code>OptionalClampLinear</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/dads.py#L38" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>OptionalClampLinear</code>(<strong><code>num_inputs</code></strong>, <strong><code>state_dims</code></strong>, <strong><code>fix_variance</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>clip_min</code></strong>=<em><code>0.3</code></em>, <strong><code>clip_max</code></strong>=<em><code>10.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MultiCompGMM" class="doc_header"><code>class</code> <code>MultiCompGMM</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/dads.py#L49" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MultiCompGMM</code>(<strong><code>num_inputs</code></strong>, <strong><code>state_dims</code></strong>, <strong><code>n_components</code></strong>, <strong><code>fix_variance</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SimpleGMM" class="doc_header"><code>class</code> <code>SimpleGMM</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/dads.py#L69" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SimpleGMM</code>(<strong><code>num_inputs</code></strong>, <strong><code>state_dims</code></strong>, <strong><code>fix_variance</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GMM" class="doc_header"><code>class</code> <code>GMM</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/dads.py#L78" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GMM</code>(<strong><code>num_inputs</code></strong>, <strong><code>state_dims</code></strong>, <strong><code>n_components</code></strong>, <strong><code>fix_variance</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The GMM is going to try to predict the next state. So lets make a simple input to optimize against.
This test will be of a hot airballoon. Lets see if we can have a reasonable next state prediction.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hot_air_ballon_start_states</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">hot_air_ballon_start_actions</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">hot_air_ballon_next_states</span><span class="o">=</span><span class="n">hot_air_ballon_start_states</span><span class="o">+</span><span class="n">hot_air_ballon_start_actions</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Starting Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hot_air_ballon_start_actions</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Action&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hot_air_ballon_next_states</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Resulting Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gmm</span><span class="o">=</span><span class="n">GMM</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">fix_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, lets see if the GMM can predict the next state accurately...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">])</span>
<span class="n">dist</span><span class="o">=</span><span class="n">gmm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">hot_air_ballon_start_states</span><span class="o">+</span><span class="n">dist</span><span class="o">.</span><span class="n">mean</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.4285],
        [ 1.1842],
        [-0.0410],
        [ 0.1103],
        [ 0.6167]], grad_fn=&lt;AddBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}
versus...
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.2454],
        [ 0.9318],
        [-0.2890],
        [-0.1740],
        [ 0.2819]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Yeah the GMM is way off...
So lets first see if we can get it to optimize to predict a next state...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">next_timesteps</span><span class="o">=</span><span class="n">hot_air_ballon_next_states</span><span class="o">-</span><span class="n">hot_air_ballon_start_states</span>    
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">])</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>
    <span class="n">dist</span><span class="o">=</span><span class="n">gmm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">=-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">next_timesteps</span><span class="p">))</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">step</span><span class="o">%</span><span class="k">1000</span>==0:print(loss,torch.mean(dist.log_prob(next_timesteps)))
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor(0.9950, grad_fn=&lt;NegBackward&gt;) tensor(-0.9950, grad_fn=&lt;MeanBackward0&gt;)
tensor(0.0267, grad_fn=&lt;NegBackward&gt;) tensor(-0.0267, grad_fn=&lt;MeanBackward0&gt;)
tensor(-0.2519, grad_fn=&lt;NegBackward&gt;) tensor(0.2519, grad_fn=&lt;MeanBackward0&gt;)
tensor(-0.2839, grad_fn=&lt;NegBackward&gt;) tensor(0.2839, grad_fn=&lt;MeanBackward0&gt;)
tensor(-0.2846, grad_fn=&lt;NegBackward&gt;) tensor(0.2846, grad_fn=&lt;MeanBackward0&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">])</span>
<span class="n">dist</span><span class="o">=</span><span class="n">gmm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">hot_air_ballon_start_states</span><span class="o">+</span><span class="n">dist</span><span class="o">.</span><span class="n">mean</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.2533],
        [ 1.0486],
        [-0.3542],
        [-0.2287],
        [ 0.2790]], grad_fn=&lt;AddBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.2454],
        [ 0.9318],
        [-0.2890],
        [-0.1740],
        [ 0.2819]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is much better, the GMM is making a better prediction of what the next state is going to be...</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Skill-Dynamics">Skill Dynamics<a class="anchor-link" href="#Skill-Dynamics"> </a></h2><blockquote><p>Now that we have a working GMM, we need a way to modify our input data to be useful for the DADS agent.</p>
</blockquote>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SkillDynamics" class="doc_header"><code>class</code> <code>SkillDynamics</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/dads.py#L88" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SkillDynamics</code>(<strong><code>s_dim</code></strong>, <strong><code>a_dim</code></strong>, <strong><code>n_components</code></strong>, <strong><code>fix_variance</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>use_model_mean</code></strong>:<code>bool</code>=<em><code>None</code></em>, <strong><code>use_batch_norm</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>fc_params</code></strong>:<code>tuple</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <a href="/fast-reinforcement-learning-2/actorcritic.dads.html#SkillDynamics"><code>SkillDynamics</code></a> module is actually pretty simple. It is tasked with getting the mean and log probability from the <a href="/fast-reinforcement-learning-2/actorcritic.dads.html#GMM"><code>GMM</code></a> and providing a convenient way to get a predicted state. It additionaly has the capability of batch normalization, noise, and input shuffling.</p>
<p>Going back to our hot air balloon example, lets feed the states, actions, and resulting states into it...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skill_dyn</span><span class="o">=</span><span class="n">SkillDynamics</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">fix_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">skill_dyn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SkillDynamics(
  (fcs): Sequential(
    (0): Linear(in_features=2, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=256, bias=True)
  )
  (gmm): GMM(
    (distribution): MultiCompGMM(
      (logit_fc): Linear(in_features=256, out_features=2, bias=True)
      (mean_fcs): ModuleList(
        (0): Linear(in_features=256, out_features=1, bias=True)
        (1): Linear(in_features=256, out_features=1, bias=True)
      )
      (std_fcs): ModuleList(
        (0): OptionalClampLinear(
          (fc): Linear(in_features=256, out_features=1, bias=True)
        )
        (1): OptionalClampLinear(
          (fc): Linear(in_features=256, out_features=1, bias=True)
        )
      )
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skill_dyn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SkillDynamics(
  (fcs): Sequential(
    (0): Linear(in_features=2, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=256, bias=True)
  )
  (gmm): GMM(
    (distribution): MultiCompGMM(
      (logit_fc): Linear(in_features=256, out_features=2, bias=True)
      (mean_fcs): ModuleList(
        (0): Linear(in_features=256, out_features=1, bias=True)
        (1): Linear(in_features=256, out_features=1, bias=True)
      )
      (std_fcs): ModuleList(
        (0): OptionalClampLinear(
          (fc): Linear(in_features=256, out_features=1, bias=True)
        )
        (1): OptionalClampLinear(
          (fc): Linear(in_features=256, out_features=1, bias=True)
        )
      )
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets do a regular feed through and look at the output of our <a href="/fast-reinforcement-learning-2/actorcritic.dads.html#SkillDynamics"><code>SkillDynamics</code></a> instance.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skill_dyn</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">,</span>
                           <span class="n">hot_air_ballon_next_states</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(MixtureSameFamily(
   Categorical(probs: torch.Size([5, 2]), logits: torch.Size([5, 2])),
   Independent(Normal(loc: torch.Size([5, 2, 1]), scale: torch.Size([5, 2, 1])), 1)),
 tensor([-0.6671, -0.8020, -0.7059, -0.7239, -0.6713],
        grad_fn=&lt;LogsumexpBackward&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have 3 components: <code>dist</code>, <code>mean</code>, and <code>log_prob</code>.\
<code>dist</code> lets the user do more advanced operations with the results.\
<code>mean</code> will always return since at minimum <code>s</code> and <code>a</code> need to be passed in.\
<code>log_prob</code> can be returned if <code>sp</code> is not None.</p>
<p>The most immediately useful result is <code>log_prob</code> since it can be used for operations that need to know <em>what is the probability of this state occuring.</em></p>
<p>However, like we said earlier, <code>dist</code> is returned because we might want to do more advanced operations. For example <code>predict_state</code>...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="s1">&#39;Prediction:&#39;</span><span class="p">,</span>\
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">predict_state</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">),</span>\
<span class="s1">&#39;Actual&#39;</span><span class="p">,</span>\
<span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;Prediction:&#39;,
 tensor([[0.0307],
         [0.5117],
         [0.0398],
         [0.1779],
         [0.5066]], grad_fn=&lt;AddBackward0&gt;),
 &#39;Actual&#39;,
 tensor([[ 0.2454],
         [ 0.9318],
         [-0.2890],
         [-0.1740],
         [ 0.2819]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This state prediction seems terrible! Obviously, we need to train <code>skill_dyn</code> so this is more accurate. You may notice the number of steps is 100 as opposed to 5000! This is because we are actually feeding a linear layer's output into the <a href="/fast-reinforcement-learning-2/actorcritic.dads.html#GMM"><code>GMM</code></a> as opposed to the <code>s+a</code> tensor directly. This is called a <code>latent space</code>, and these have friendlier values for the <a href="/fast-reinforcement-learning-2/actorcritic.dads.html#GMM"><code>GMM</code></a> to learn against. You will also see later that the results are a little more accurate also.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">print_every</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">skill_dyn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span><span class="n">log_prob</span><span class="o">=</span><span class="n">skill_dyn</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">,</span>
                       <span class="n">hot_air_ballon_next_states</span><span class="p">)</span>

        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">=-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">step</span><span class="o">%</span><span class="k">print_every</span>==0:print(loss,torch.mean(dist.log_prob(next_timesteps)))
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">skill_dyn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor(0.7140, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(0.1064, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(-0.2720, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(-0.2847, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(-0.2850, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="s1">&#39;Prediction:&#39;</span><span class="p">,</span>\
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">predict_state</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">),</span>\
<span class="s1">&#39;Actual&#39;</span><span class="p">,</span>\
<span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;Prediction:&#39;,
 tensor([[ 0.2460],
         [ 0.9291],
         [-0.2875],
         [-0.1732],
         [ 0.2805]], grad_fn=&lt;AddBackward0&gt;),
 &#39;Actual&#39;,
 tensor([[ 0.2454],
         [ 0.9318],
         [-0.2890],
         [-0.1740],
         [ 0.2819]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Much better! It is ok they are not exact, we want <a href="/fast-reinforcement-learning-2/actorcritic.dads.html#SkillDynamics"><code>SkillDynamics</code></a> to have a general idea on how current states and their actions can cause the state to change.</p>
<p>But here is a small wrench in the works. You will notice that the state spaces we are feeding into the model have values [-1, 1] which are nice and friendly to train our model on but also <strong>highly unrealistic in the real world</strong>. We will likely see values [-10,1000] possibly. Can we train on these? Let's convert our ballon problem into meters. We don't need to worry about actions because the values of actions are usually <em>within our control</em>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hot_air_ballon_start_states</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span>
<span class="n">hot_air_ballon_start_actions</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">hot_air_ballon_next_states</span><span class="o">=</span><span class="n">hot_air_ballon_start_states</span><span class="o">+</span><span class="n">hot_air_ballon_start_actions</span><span class="o">*</span><span class="mi">100</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Starting Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hot_air_ballon_start_actions</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Action&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hot_air_ballon_next_states</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Resulting Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These are much bigger values... Training a model on these without normalzing can be very hard. 
The worst part is that in order to normalize something like states,
you would need to know <em>what is the absolute minimum and maximum this state will ever be</em>. This is a general problem in RL because when the agent is first
starting the states might start small, such as being newer the origin 0,0, but everntually as the agent gets better it starts to get to states like -20,100.
Our model needs to convert these into nice [0,1] or [-1,1] values.</p>
<p>So first, lets see if we can even learn anything without batch norm...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skill_dyn</span><span class="o">=</span><span class="n">SkillDynamics</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">fix_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">skill_dyn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
<span class="s1">&#39;Prediction:&#39;</span><span class="p">,</span>\
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">predict_state</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">),</span>\
<span class="s1">&#39;Actual&#39;</span><span class="p">,</span>\
<span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;Prediction:&#39;,
 tensor([[ 79.1728],
         [ 79.7524],
         [  0.9721],
         [ 99.2698],
         [103.9120]], grad_fn=&lt;AddBackward0&gt;),
 &#39;Actual&#39;,
 tensor([[ 25.6075],
         [ 83.0189],
         [ 24.5584],
         [107.6932],
         [ 54.7154]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor(98.0331, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(8.4126, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(7.6876, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(7.2951, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(7.0609, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor(6.8792, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="s1">&#39;Prediction:&#39;</span><span class="p">,</span>\
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">predict_state</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">),</span>\
<span class="s1">&#39;Actual&#39;</span><span class="p">,</span>\
<span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;Prediction:&#39;,
 tensor([[58.9789],
         [60.3023],
         [ 9.3156],
         [75.1989],
         [77.8273]], grad_fn=&lt;AddBackward0&gt;),
 &#39;Actual&#39;,
 tensor([[ 25.6075],
         [ 83.0189],
         [ 24.5584],
         [107.6932],
         [ 54.7154]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hm... It's diffinitely not amazing. The worst part is that this will likely be made worst with larger and differing batches. Lets see if using batch norm makes this better...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skill_dyn</span><span class="o">=</span><span class="n">SkillDynamics</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">fix_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">skill_dyn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
<span class="s1">&#39;Prediction:&#39;</span><span class="p">,</span>\
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">predict_state</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">),</span>\
<span class="s1">&#39;Actual&#39;</span><span class="p">,</span>\
<span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[ 0.1762],
        [ 0.1925],
        [-1.9264],
        [ 0.7169],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) None
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;Prediction:&#39;,
 tensor([[ 80.7160],
         [ 81.6525],
         [  3.0034],
         [100.8246],
         [104.2937]], grad_fn=&lt;AddBackward0&gt;),
 &#39;Actual&#39;,
 tensor([[ 25.6075],
         [ 83.0189],
         [ 24.5584],
         [107.6932],
         [ 54.7154]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[ 0.1762],
        [ 0.1925],
        [-1.9264],
        [ 0.7169],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2993],
        [ 0.5607],
        [ 1.0850],
        [ 0.7649],
        [-1.1113]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor(1.6850, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor([[ 0.1764],
        [ 0.1928],
        [-1.9255],
        [ 0.7170],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2986],
        [ 0.5608],
        [ 1.0850],
        [ 0.7650],
        [-1.1107]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1767],
        [ 0.1930],
        [-1.9251],
        [ 0.7172],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2984],
        [ 0.5605],
        [ 1.0846],
        [ 0.7647],
        [-1.1105]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1769],
        [ 0.1933],
        [-1.9249],
        [ 0.7174],
        [ 0.8413]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2980],
        [ 0.5604],
        [ 1.0843],
        [ 0.7645],
        [-1.1101]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1771],
        [ 0.1934],
        [-1.9249],
        [ 0.7176],
        [ 0.8415]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2977],
        [ 0.5601],
        [ 1.0839],
        [ 0.7642],
        [-1.1099]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1772],
        [ 0.1936],
        [-1.9248],
        [ 0.7178],
        [ 0.8417]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2975],
        [ 0.5598],
        [ 1.0834],
        [ 0.7638],
        [-1.1097]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1773],
        [ 0.1937],
        [-1.9248],
        [ 0.7179],
        [ 0.8418]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2973],
        [ 0.5595],
        [ 1.0829],
        [ 0.7634],
        [-1.1096]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1774],
        [ 0.1938],
        [-1.9247],
        [ 0.7180],
        [ 0.8418]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2972],
        [ 0.5591],
        [ 1.0824],
        [ 0.7630],
        [-1.1095]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1774],
        [ 0.1938],
        [-1.9244],
        [ 0.7180],
        [ 0.8418]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2970],
        [ 0.5587],
        [ 1.0819],
        [ 0.7626],
        [-1.1094]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1775],
        [ 0.1938],
        [-1.9240],
        [ 0.7179],
        [ 0.8418]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2969],
        [ 0.5584],
        [ 1.0815],
        [ 0.7622],
        [-1.1094]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1775],
        [ 0.1939],
        [-1.9236],
        [ 0.7179],
        [ 0.8416]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2967],
        [ 0.5582],
        [ 1.0811],
        [ 0.7619],
        [-1.1092]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1776],
        [ 0.1939],
        [-1.9230],
        [ 0.7178],
        [ 0.8415]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2964],
        [ 0.5580],
        [ 1.0808],
        [ 0.7617],
        [-1.1090]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1776],
        [ 0.1940],
        [-1.9224],
        [ 0.7177],
        [ 0.8414]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2961],
        [ 0.5580],
        [ 1.0807],
        [ 0.7616],
        [-1.1087]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1777],
        [ 0.1940],
        [-1.9218],
        [ 0.7176],
        [ 0.8413]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2957],
        [ 0.5580],
        [ 1.0806],
        [ 0.7616],
        [-1.1083]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1778],
        [ 0.1941],
        [-1.9212],
        [ 0.7176],
        [ 0.8413]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2953],
        [ 0.5580],
        [ 1.0805],
        [ 0.7616],
        [-1.1080]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1779],
        [ 0.1943],
        [-1.9206],
        [ 0.7176],
        [ 0.8413]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2949],
        [ 0.5580],
        [ 1.0804],
        [ 0.7616],
        [-1.1076]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1780],
        [ 0.1943],
        [-1.9202],
        [ 0.7176],
        [ 0.8412]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2946],
        [ 0.5580],
        [ 1.0803],
        [ 0.7615],
        [-1.1073]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1780],
        [ 0.1944],
        [-1.9200],
        [ 0.7176],
        [ 0.8412]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2944],
        [ 0.5578],
        [ 1.0800],
        [ 0.7613],
        [-1.1071]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1780],
        [ 0.1943],
        [-1.9200],
        [ 0.7175],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2942],
        [ 0.5576],
        [ 1.0797],
        [ 0.7610],
        [-1.1070]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1779],
        [ 0.1943],
        [-1.9200],
        [ 0.7175],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2941],
        [ 0.5573],
        [ 1.0793],
        [ 0.7607],
        [-1.1069]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1779],
        [ 0.1942],
        [-1.9201],
        [ 0.7174],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2940],
        [ 0.5570],
        [ 1.0788],
        [ 0.7603],
        [-1.1068]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1778],
        [ 0.1941],
        [-1.9202],
        [ 0.7174],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2938],
        [ 0.5567],
        [ 1.0784],
        [ 0.7600],
        [-1.1067]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1777],
        [ 0.1940],
        [-1.9203],
        [ 0.7173],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2936],
        [ 0.5565],
        [ 1.0781],
        [ 0.7597],
        [-1.1065]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1776],
        [ 0.1940],
        [-1.9202],
        [ 0.7171],
        [ 0.8407]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2932],
        [ 0.5564],
        [ 1.0779],
        [ 0.7596],
        [-1.1062]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1775],
        [ 0.1939],
        [-1.9200],
        [ 0.7170],
        [ 0.8406]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2928],
        [ 0.5564],
        [ 1.0777],
        [ 0.7595],
        [-1.1059]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1775],
        [ 0.1938],
        [-1.9198],
        [ 0.7168],
        [ 0.8404]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2923],
        [ 0.5564],
        [ 1.0776],
        [ 0.7594],
        [-1.1055]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1774],
        [ 0.1937],
        [-1.9196],
        [ 0.7167],
        [ 0.8402]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2919],
        [ 0.5564],
        [ 1.0775],
        [ 0.7594],
        [-1.1050]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1773],
        [ 0.1936],
        [-1.9194],
        [ 0.7165],
        [ 0.8401]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2914],
        [ 0.5564],
        [ 1.0773],
        [ 0.7593],
        [-1.1046]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1773],
        [ 0.1936],
        [-1.9192],
        [ 0.7165],
        [ 0.8400]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2910],
        [ 0.5562],
        [ 1.0770],
        [ 0.7591],
        [-1.1042]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1773],
        [ 0.1936],
        [-1.9191],
        [ 0.7165],
        [ 0.8400]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2906],
        [ 0.5560],
        [ 1.0766],
        [ 0.7588],
        [-1.1040]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1774],
        [ 0.1937],
        [-1.9189],
        [ 0.7165],
        [ 0.8401]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2903],
        [ 0.5557],
        [ 1.0761],
        [ 0.7585],
        [-1.1037]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1775],
        [ 0.1939],
        [-1.9188],
        [ 0.7167],
        [ 0.8402]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2901],
        [ 0.5554],
        [ 1.0756],
        [ 0.7581],
        [-1.1035]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1777],
        [ 0.1940],
        [-1.9185],
        [ 0.7168],
        [ 0.8403]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2898],
        [ 0.5550],
        [ 1.0751],
        [ 0.7577],
        [-1.1033]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1779],
        [ 0.1942],
        [-1.9182],
        [ 0.7170],
        [ 0.8405]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2895],
        [ 0.5547],
        [ 1.0747],
        [ 0.7573],
        [-1.1031]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1780],
        [ 0.1944],
        [-1.9178],
        [ 0.7170],
        [ 0.8405]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2892],
        [ 0.5545],
        [ 1.0743],
        [ 0.7570],
        [-1.1028]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1782],
        [ 0.1945],
        [-1.9174],
        [ 0.7171],
        [ 0.8406]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2889],
        [ 0.5543],
        [ 1.0740],
        [ 0.7568],
        [-1.1025]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1783],
        [ 0.1946],
        [-1.9170],
        [ 0.7171],
        [ 0.8406]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5542],
        [ 1.0737],
        [ 0.7565],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1784],
        [ 0.1947],
        [-1.9166],
        [ 0.7172],
        [ 0.8406]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2883],
        [ 0.5540],
        [ 1.0734],
        [ 0.7563],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1785],
        [ 0.1948],
        [-1.9162],
        [ 0.7172],
        [ 0.8406]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2882],
        [ 0.5538],
        [ 1.0731],
        [ 0.7561],
        [-1.1020]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1786],
        [ 0.1949],
        [-1.9159],
        [ 0.7173],
        [ 0.8407]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2882],
        [ 0.5536],
        [ 1.0728],
        [ 0.7558],
        [-1.1020]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1787],
        [ 0.1950],
        [-1.9156],
        [ 0.7174],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2882],
        [ 0.5534],
        [ 1.0725],
        [ 0.7556],
        [-1.1020]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1788],
        [ 0.1951],
        [-1.9154],
        [ 0.7174],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2882],
        [ 0.5532],
        [ 1.0724],
        [ 0.7555],
        [-1.1020]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1789],
        [ 0.1952],
        [-1.9152],
        [ 0.7175],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2882],
        [ 0.5532],
        [ 1.0723],
        [ 0.7554],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1790],
        [ 0.1953],
        [-1.9149],
        [ 0.7175],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2882],
        [ 0.5531],
        [ 1.0723],
        [ 0.7554],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1791],
        [ 0.1954],
        [-1.9147],
        [ 0.7175],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2883],
        [ 0.5531],
        [ 1.0723],
        [ 0.7554],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1791],
        [ 0.1954],
        [-1.9144],
        [ 0.7176],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5531],
        [ 1.0723],
        [ 0.7554],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1792],
        [ 0.1955],
        [-1.9142],
        [ 0.7176],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5532],
        [ 1.0724],
        [ 0.7554],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1793],
        [ 0.1956],
        [-1.9139],
        [ 0.7176],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2887],
        [ 0.5532],
        [ 1.0724],
        [ 0.7555],
        [-1.1025]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1793],
        [ 0.1956],
        [-1.9137],
        [ 0.7176],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2888],
        [ 0.5532],
        [ 1.0725],
        [ 0.7555],
        [-1.1026]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1794],
        [ 0.1957],
        [-1.9136],
        [ 0.7176],
        [ 0.8409]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2889],
        [ 0.5533],
        [ 1.0726],
        [ 0.7556],
        [-1.1026]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1794],
        [ 0.1957],
        [-1.9134],
        [ 0.7177],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2889],
        [ 0.5533],
        [ 1.0727],
        [ 0.7557],
        [-1.1027]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor(-0.2563, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor([[ 0.1795],
        [ 0.1958],
        [-1.9132],
        [ 0.7177],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2890],
        [ 0.5534],
        [ 1.0728],
        [ 0.7558],
        [-1.1027]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1796],
        [ 0.1959],
        [-1.9130],
        [ 0.7177],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2890],
        [ 0.5535],
        [ 1.0729],
        [ 0.7558],
        [-1.1027]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1796],
        [ 0.1959],
        [-1.9127],
        [ 0.7177],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2890],
        [ 0.5535],
        [ 1.0730],
        [ 0.7559],
        [-1.1028]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1797],
        [ 0.1960],
        [-1.9125],
        [ 0.7177],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2890],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1028]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1798],
        [ 0.1960],
        [-1.9122],
        [ 0.7178],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2890],
        [ 0.5536],
        [ 1.0731],
        [ 0.7560],
        [-1.1027]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1798],
        [ 0.1961],
        [-1.9120],
        [ 0.7178],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2889],
        [ 0.5537],
        [ 1.0732],
        [ 0.7561],
        [-1.1027]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1799],
        [ 0.1962],
        [-1.9118],
        [ 0.7178],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2889],
        [ 0.5538],
        [ 1.0732],
        [ 0.7561],
        [-1.1026]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1799],
        [ 0.1962],
        [-1.9116],
        [ 0.7178],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2888],
        [ 0.5538],
        [ 1.0733],
        [ 0.7562],
        [-1.1025]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1799],
        [ 0.1962],
        [-1.9115],
        [ 0.7178],
        [ 0.8410]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2887],
        [ 0.5538],
        [ 1.0733],
        [ 0.7562],
        [-1.1025]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1800],
        [ 0.1963],
        [-1.9114],
        [ 0.7178],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2887],
        [ 0.5538],
        [ 1.0733],
        [ 0.7562],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1800],
        [ 0.1963],
        [-1.9113],
        [ 0.7178],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5538],
        [ 1.0732],
        [ 0.7562],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1800],
        [ 0.1963],
        [-1.9111],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5538],
        [ 1.0732],
        [ 0.7562],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1801],
        [ 0.1964],
        [-1.9110],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5538],
        [ 1.0732],
        [ 0.7562],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1801],
        [ 0.1964],
        [-1.9109],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5539],
        [ 1.0732],
        [ 0.7562],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1801],
        [ 0.1964],
        [-1.9108],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5539],
        [ 1.0732],
        [ 0.7562],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1802],
        [ 0.1964],
        [-1.9107],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5539],
        [ 1.0732],
        [ 0.7562],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1802],
        [ 0.1965],
        [-1.9106],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5538],
        [ 1.0732],
        [ 0.7562],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1802],
        [ 0.1965],
        [-1.9105],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5538],
        [ 1.0731],
        [ 0.7561],
        [-1.1021]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1802],
        [ 0.1965],
        [-1.9104],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5538],
        [ 1.0731],
        [ 0.7561],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1803],
        [ 0.1965],
        [-1.9103],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5538],
        [ 1.0731],
        [ 0.7561],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1803],
        [ 0.1966],
        [-1.9102],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5537],
        [ 1.0731],
        [ 0.7561],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1803],
        [ 0.1966],
        [-1.9101],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5537],
        [ 1.0731],
        [ 0.7561],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1803],
        [ 0.1966],
        [-1.9100],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0731],
        [ 0.7560],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1803],
        [ 0.1966],
        [-1.9099],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1966],
        [-1.9098],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9098],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9097],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9097],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9096],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9096],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9095],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1804],
        [ 0.1967],
        [-1.9095],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1024]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1967],
        [-1.9095],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2886],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1967],
        [-1.9094],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1967],
        [-1.9094],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1967],
        [-1.9094],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9094],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9093],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9093],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9093],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9093],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2884],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1022]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9092],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor(-0.2849, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5537],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0729],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor(-0.2850, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor(-0.2850, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7559],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor(-0.2850, grad_fn=&lt;NegBackward&gt;) tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tensor([[ 0.1805],
        [ 0.1968],
        [-1.9091],
        [ 0.7179],
        [ 0.8411]], grad_fn=&lt;NativeBatchNormBackward&gt;) tensor([[-1.2885],
        [ 0.5536],
        [ 1.0730],
        [ 0.7560],
        [-1.1023]], grad_fn=&lt;NativeBatchNormBackward&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skill_dyn</span><span class="o">=</span><span class="n">SkillDynamics</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">fix_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">skill_dyn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
<span class="s1">&#39;Prediction:&#39;</span><span class="p">,</span>\
<span class="n">skill_dyn</span><span class="o">.</span><span class="n">predict_state</span><span class="p">(</span><span class="n">hot_air_ballon_start_states</span><span class="p">,</span><span class="n">hot_air_ballon_start_actions</span><span class="p">),</span>\
<span class="s1">&#39;Actual&#39;</span><span class="p">,</span>\
<span class="n">hot_air_ballon_next_states</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[ 0.1762],
        [ 0.1925],
        [-1.9264],
        [ 0.7169],
        [ 0.8408]], grad_fn=&lt;NativeBatchNormBackward&gt;) None
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;Prediction:&#39;,
 tensor([[ 79.7528],
         [ 79.5636],
         [  8.6890],
         [ 97.2797],
         [100.0082]], grad_fn=&lt;AddBackward0&gt;),
 &#39;Actual&#39;,
 tensor([[ 25.6075],
         [ 83.0189],
         [ 24.5584],
         [107.6932],
         [ 54.7154]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Full-Training">Full Training<a class="anchor-link" href="#Full-Training"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class Discriminator(Module):</span>
<span class="c1">#     &quot;`Module` for storing skills. Receives input (`num_inputs`+`num_actions`) -&gt; `num_skills`.&quot;</span>
<span class="c1">#     def __init__(self, num_inputs,num_actions,num_skills,hidden_dim):</span>
<span class="c1">#         self.linear1 = nn.Linear(num_inputs+num_skills, hidden_dim)</span>
<span class="c1">#         self.linear2 = nn.Linear(hidden_dim, hidden_dim)</span>
<span class="c1">#         self.linear3 = nn.Linear(hidden_dim,num_skills)</span>

<span class="c1">#         self.apply(weights_init_)</span>

<span class="c1">#     def forward(self, state):</span>
<span class="c1">#         x = F.relu(self.linear1(state.float()))</span>
<span class="c1">#         x = F.relu(self.linear2(x))</span>
<span class="c1">#         return self.linear3(x)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># @delegates(SAC)</span>
<span class="c1"># class DADS(SAC):</span>
<span class="c1">#     def __init__(self,num_inputs,action_space,discriminator:Module=None,num_skills:int=20,</span>
<span class="c1">#                  find_best_skill_interval:int=10,scale_entropy:float=1,</span>
<span class="c1">#                  best_skill_n_rollouts:int=10,include_actions:bool=False,</span>
<span class="c1">#                  learn_p_z:bool=False,add_p_z:bool=True,hidden_size=100,lr=0.003,**kwargs):</span>
<span class="c1">#         store_attr()</span>
<span class="c1">#         self.num_inputs=num_inputs+self.num_skills</span>
<span class="c1">#         self.original_num_inputs=num_inputs</span>
<span class="c1">#         self.p_z=np.full(self.num_skills,1.0/self.num_skills)</span>
<span class="c1">#         self.discriminator=Discriminator(self.original_num_inputs,action_space.shape[0],</span>
<span class="c1">#                                          num_skills,hidden_size)</span>

<span class="c1">#         self.discriminator_optim = Adam(self.discriminator.parameters(), lr=self.lr)</span>

<span class="c1">#         self.log_p_z_episode=[]</span>
<span class="c1">#         self.z=0</span>
<span class="c1">#         self.reset_z()</span>


<span class="c1">#         super().__init__(self.num_inputs,action_space,hidden_size=hidden_size,lr=lr,**kwargs)</span>

<span class="c1">#     def sample_z(self):</span>
<span class="c1">#         &quot;&quot;&quot;Samples z from p(z), using probabilities in self._p_z.&quot;&quot;&quot;</span>
<span class="c1">#         return np.random.choice(self.num_skills,p=self.p_z)</span>

<span class="c1">#     def reset_z(self): self.z=self.sample_z()</span>
<span class="c1">#     def __call__(self,s,asl):</span>
<span class="c1">#         aug_s=self.concat_obs_z(s,self.z)</span>
<span class="c1">#         return super().__call__(aug_s,asl)</span>

<span class="c1">#     def concat_obs_z(self,obs,z):</span>
<span class="c1">#         &quot;&quot;&quot;Concatenates the observation to a one-hot encoding of Z.&quot;&quot;&quot;</span>
<span class="c1">#         assert np.isscalar(z)</span>
<span class="c1">#         if type(obs)==list and len(obs)==1: obs=obs[0]</span>
<span class="c1">#         if len(obs.shape)==2 and obs.shape[0]==1: obs=obs[0]</span>

<span class="c1">#         z_one_hot=np.zeros(self.num_skills)</span>
<span class="c1">#         z_one_hot[z]=1</span>
<span class="c1">#         if type(obs)==Tensor: obs=obs.cpu()</span>
<span class="c1">#         return torch.FloatTensor(np.hstack([obs,z_one_hot])).reshape(1,-1)</span>

<span class="c1">#     def skill_p(self,skill,next_state):</span>
<span class="c1">#         unnorm_skill_dist=self.discriminator(next_state).unsqueeze(0)</span>
<span class="c1">#         skill_p=F.softmax(unnorm_skill_dist)[:,skill]</span>
<span class="c1">#         return skill_p,unnorm_skill_dist</span>

<span class="c1">#     def discriminator_learn(self,skill,out):</span>
<span class="c1">#         self.discriminator_optim.zero_grad()</span>
<span class="c1">#         loss=nn.CrossEntropyLoss()(out,torch.LongTensor([skill]))</span>
<span class="c1">#         loss.backward()</span>
<span class="c1">#         self.discriminator_optim.step()</span>

<span class="c1">#     def intrinsic_reward(self,next_state):</span>
<span class="c1">#         skill_p,disc_out=self.skill_p(self.z,next_state)</span>
<span class="c1">#         intrinsic_reward=np.log(skill_p.cpu().detach()+1e-8)-np.log(self.p_z[self.z])</span>
<span class="c1"># #         print(skill_p,self.p_z,intrinsic_reward)</span>
<span class="c1">#         return intrinsic_reward,disc_out</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class DiscriminatorTrainer(ExperienceReplay):</span>

<span class="c1">#     def __init__(self,*args,**kwargs):</span>
<span class="c1">#         self.log_p_z_episode=[]</span>
<span class="c1">#         super().__init__(*args,**kwargs)</span>
        
<span class="c1">#     def before_fit(self):</span>
<span class="c1">#         self.learn.agent.warming_up=True</span>
<span class="c1">#         while len(self.queue)&lt;self.starting_els:</span>
<span class="c1">#             for i,o in enumerate(self.dls.train):</span>
<span class="c1">#                 z=self.learn.agent.z</span>
<span class="c1">#                 batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(o[0][i],z)[0],</span>
<span class="c1">#                                            action=o[1][i],</span>
<span class="c1">#                                            reward=o[2][i],</span>
<span class="c1">#                                            last_state=self.learn.agent.concat_obs_z(o[3][i],z)[0], </span>
<span class="c1">#                                            done=(o[4][i] and self.max_steps!=o[6][i]),</span>
<span class="c1">#                                            episode_reward=o[5][i],steps=o[6][i])</span>
<span class="c1">#                                     for i in range(len(o[0]))]</span>
<span class="c1"># #                 print(self.max_steps,max([o.steps for o in batch]))</span>
<span class="c1"># #                 print(batch[0])</span>
<span class="c1">#                 for k in range(len(batch)):</span>
<span class="c1">#                     intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))</span>
<span class="c1">#                     self.learn.agent.discriminator_learn(self.agent.z,disc_out)</span>
<span class="c1">#                     batch[k]=ExperienceFirstLast(</span>
<span class="c1">#                         state=batch[k].state.to(device=default_device()),</span>
<span class="c1">#                         action=batch[k].action,</span>
<span class="c1">#                         reward=intrinsic_reward,</span>
<span class="c1">#                         last_state=batch[k].last_state.to(device=default_device()),</span>
<span class="c1">#                         done=batch[k].done,</span>
<span class="c1">#                         episode_reward=batch[k].episode_reward,</span>
<span class="c1">#                         steps=batch[k].steps</span>
<span class="c1">#                     )</span>


<span class="c1"># #                 print(batch[0])</span>
<span class="c1">#                 for _b in batch:self.queue.append(_b)</span>
<span class="c1">#                 if any([_b.done for _b in batch]): self.learn.agent.reset_z()</span>
<span class="c1">#                 if len(self.queue)&gt;self.starting_els:break</span>
<span class="c1">#         self.learn.agent.warming_up=False</span>

<span class="c1"># # #     def after_epoch(self):</span>
<span class="c1"># # #         print(len(self.queue))</span>
<span class="c1">#     def before_batch(self):</span>
<span class="c1"># #         print(len(self.queue))</span>
<span class="c1">#         b=list(self.learn.xb)+list(self.learn.yb)</span>
<span class="c1">#         z=self.learn.agent.z</span>
<span class="c1">#         batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(b[0][i],z)[0],</span>
<span class="c1">#                                    action=b[1][i],</span>
<span class="c1">#                                    reward=b[2][i],</span>
<span class="c1">#                                    last_state=self.learn.agent.concat_obs_z(b[3][i],z)[0], </span>
<span class="c1">#                                    done=(b[4][i] and self.max_steps!=b[6][i]),</span>
<span class="c1">#                                    episode_reward=b[5][i],steps=b[6][i])</span>
<span class="c1">#               for i in range(len(b[0]))]</span>
        
<span class="c1">#         for k in range(len(batch)):</span>
<span class="c1">#             intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))</span>
<span class="c1">#             self.learn.agent.discriminator_learn(self.agent.z,disc_out)</span>
<span class="c1">#             batch[k]=ExperienceFirstLast(</span>
<span class="c1">#                 state=batch[k].state.to(device=default_device()),</span>
<span class="c1">#                 action=batch[k].action,</span>
<span class="c1">#                 reward=intrinsic_reward,</span>
<span class="c1">#                 last_state=batch[k].last_state.to(device=default_device()),</span>
<span class="c1">#                 done=batch[k].done,</span>
<span class="c1">#                 episode_reward=batch[k].episode_reward,</span>
<span class="c1">#                 steps=batch[k].steps</span>
<span class="c1">#             )</span>
        
<span class="c1"># #         print(self.learn.xb)</span>
<span class="c1">#         self.learn.xb=(torch.stack([e.state for e in batch]),)</span>
<span class="c1"># #         print(self.learn.yb)</span>
<span class="c1">#         self.learn.yb=(torch.stack([o.action for o in batch]),</span>
<span class="c1">#                        torch.stack([o.reward for o in batch]),</span>
<span class="c1">#                        torch.stack([o.last_state for o in batch]),</span>
<span class="c1">#                        torch.stack([o.done for o in batch]),</span>
<span class="c1">#                        torch.stack([o.episode_reward for o in batch]),</span>
<span class="c1">#                        torch.stack([o.steps for o in batch]))</span>
<span class="c1"># #         print(self.learn.yb)</span>
        
<span class="c1">#         for _b in batch: self.queue.append(_b)</span>
<span class="c1">#         idxs=np.random.randint(0,len(self.queue), self.bs)</span>
<span class="c1">#         self.learn.sample_yb=[self.queue[i] for i in idxs]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># env=&#39;InvertedPendulumPyBulletEnv-v0&#39;</span>
<span class="c1"># agent=DADS(5,gym.make(env).action_space,gamma=0.99,tau=0.005,alpha=0.1,hidden_size=300,num_skills=5)</span>
<span class="c1"># block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,exclude_nones=True,</span>
<span class="c1">#                                dls_kwargs={&#39;bs&#39;:1,&#39;num_workers&#39;:0,&#39;verbose&#39;:False,&#39;indexed&#39;:True,&#39;shuffle_train&#39;:False})</span>
<span class="c1"># blk=IterableDataBlock(blocks=(block),splitter=FuncSplitter(lambda x:False))</span>
<span class="c1"># dls=blk.dataloaders([env]*1,n=10,device=default_device())</span>

<span class="c1"># learner=SACLearner(dls,agent=agent,cbs=[DiscriminatorTrainer(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),</span>
<span class="c1">#                                         SACCriticTrainer],</span>
<span class="c1">#                    metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])</span>
<span class="c1"># learner.fit(1,lr=0.003,wd=0)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # slow</span>
<span class="c1"># import gym</span>
<span class="c1"># from IPython import display</span>
<span class="c1"># import PIL.Image</span>
<span class="c1"># %matplotlib inline</span>

<span class="c1"># # env=gym.make(&#39;InvertedPendulumPyBulletEnv-v0&#39;)</span>
<span class="c1"># # s=env.reset()</span>

<span class="c1"># # for z in range(5):</span>
<span class="c1"># #     for i in range(0,100,20):</span>
<span class="c1"># #         s=env.reset()</span>
<span class="c1"># #         env.seed(i)</span>
<span class="c1"># #         for _ in range(200):</span>
<span class="c1"># #             display.clear_output(wait=True)</span>
<span class="c1"># #             display.display(PIL.Image.fromarray(env.render(mode=&#39;rgb_array&#39;)))</span>

<span class="c1"># #             agent.z=z</span>
<span class="c1"># #             a,_=agent(s,None)</span>

<span class="c1"># #             s,r,d,_=env.step(a)</span>
<span class="c1"># #             if d:</span>
<span class="c1"># #                 break</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

