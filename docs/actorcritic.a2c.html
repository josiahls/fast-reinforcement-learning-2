---

title: A2C

keywords: fastai
sidebar: home_sidebar

summary: "Synchronous Actor Critic"
description: "Synchronous Actor Critic"
nb_path: "nbs/16_actorcritic.a2c.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/16_actorcritic.a2c.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_LAUNCH_BLOCKING&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LinearA2C" class="doc_header"><code>class</code> <code>LinearA2C</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/a2c.py#L28" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LinearA2C</code>(<strong><code>input_shape</code></strong>, <strong><code>n_actions</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="unbatch" class="doc_header"><code>unbatch</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/a2c.py#L49" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>unbatch</code>(<strong><code>batch</code></strong>, <strong><code>net</code></strong>, <strong><code>val_gamma</code></strong>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">A2CTrainer</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="c1">#         print(&#39;clipping&#39;,self.learn.clip_grad,np.mean([o.detach().numpy().mean() for o in self.learn.model.parameters()]))</span>
        <span class="n">nn_utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">clip_grad</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">loss</span><span class="o">+=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">loss_policy_v</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">sp</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">episode_rewards</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="c1">#     print(len(learn.xb[0]),len(a),len(r),len(sp),len(d))</span>
<span class="c1">#     print(learn.xb[0],a,r,sp,d)</span>
<span class="c1">#     print(len(learn.xb[0]))</span>
    <span class="n">bs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">yb</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
<span class="c1">#         print(learn.xb[0][i],a[i],r[i],sp[i],d[i])</span>
<span class="c1">#         print(a[i])</span>
        <span class="n">yb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ExperienceFirstLast</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="n">action</span><span class="o">=</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">reward</span><span class="o">=</span><span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">last_state</span><span class="o">=</span><span class="n">sp</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">done</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">episode_reward</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    
    <span class="n">s_t</span><span class="p">,</span><span class="n">a_t</span><span class="p">,</span><span class="n">r_est</span><span class="o">=</span><span class="n">unbatch</span><span class="p">(</span><span class="n">yb</span><span class="p">,</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">,</span><span class="n">learn</span><span class="o">.</span><span class="n">discount</span><span class="o">**</span><span class="n">learn</span><span class="o">.</span><span class="n">reward_steps</span><span class="p">,</span><span class="n">default_device</span><span class="p">())</span>
<span class="c1">#     print(r_est.mean(),np.mean([o.r.numpy() for o in yb]))</span>
<span class="c1">#     print(sum([o.d for o in yb]))</span>
<span class="c1">#     print(s_t.shape,a_t.shape,r_est.shape)</span>
<span class="c1">#     r_est=r_est.squeeze(1)</span>

    <span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">logits_v</span><span class="p">,</span><span class="n">value_v</span><span class="o">=</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>

    <span class="n">loss_value_v</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">value_v</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">r_est</span><span class="p">)</span>
<span class="c1">#     loss_value_v=F.mse_loss(value_v,r_est)</span>

    <span class="n">log_prob_v</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits_v</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">adv_v</span><span class="o">=</span><span class="n">r_est</span><span class="o">-</span><span class="n">value_v</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="n">log_prob_actions_v</span><span class="o">=</span><span class="n">adv_v</span><span class="o">*</span><span class="n">log_prob_v</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span><span class="n">a_t</span><span class="p">]</span>
    <span class="n">loss_policy_v</span><span class="o">=-</span><span class="n">log_prob_actions_v</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">prob_v</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits_v</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">entropy_loss_v</span><span class="o">=</span><span class="n">learn</span><span class="o">.</span><span class="n">entropy_beta</span><span class="o">*</span><span class="p">(</span><span class="n">prob_v</span><span class="o">*</span><span class="n">log_prob_v</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># calculate the polocy gradients only</span>
    <span class="n">loss_policy_v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    

    <span class="n">loss_v</span><span class="o">=</span><span class="n">entropy_loss_v</span><span class="o">+</span><span class="n">loss_value_v</span>
    
    <span class="nb">setattr</span><span class="p">(</span><span class="n">learn</span><span class="p">,</span><span class="s1">&#39;loss_policy_v&#39;</span><span class="p">,</span><span class="n">loss_policy_v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss_v</span>

<span class="k">class</span> <span class="nc">A2CLearner</span><span class="p">(</span><span class="n">AgentLearner</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dls</span><span class="p">,</span><span class="n">discount</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">entropy_beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">clip_grad</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">reward_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="p">),</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">=</span><span class="n">OptimWrapper</span><span class="p">(</span><span class="n">AdamW</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount</span><span class="o">=</span><span class="n">discount</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entropy_beta</span><span class="o">=</span><span class="n">entropy_beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_steps</span><span class="o">=</span><span class="n">reward_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad</span><span class="o">=</span><span class="n">clip_grad</span>
        
<span class="c1">#     def _do_one_batch(self):</span>
<span class="c1">#         self.pred = self.model(*self.xb);                self(&#39;after_pred&#39;)</span>
<span class="c1">#         if len(self.yb) == 0: return</span>
<span class="c1">#         self.loss = self.loss_func(self.pred, *self.yb); self(&#39;after_loss&#39;)</span>
<span class="c1">#         if not self.training: return</span>
<span class="c1">#         self(&#39;before_backward&#39;)</span>
<span class="c1">#         self._backward();                                </span>
<span class="c1">#         print(&#39;did backward&#39;)</span>
        
<span class="c1">#         self(&#39;after_backward&#39;)</span>
<span class="c1">#         self._step();                                    </span>
<span class="c1">#         print(&#39;storpped&#39;)</span>
<span class="c1">#         self(&#39;after_step&#39;)</span>
<span class="c1">#         self.opt.zero_grad()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">LinearA2C</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model start&#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">o</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>model start 0.00830515
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is important to note that A2C without extra augmentation will lose memoiry very quickly. If it succeeds at 200+ reward, it will eventually <strong>forget</strong> what strategies got it there and you will see the graph for rewards drop back down. It is recommended to use some kind of early stopping for rewards. The more fun solution would be expermenting with ways to keep the agent stable possibly by keeping samples that it deemed as "important".</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">=</span><span class="s1">&#39;CartPole-v1&#39;</span>
<span class="c1"># model=LinearA2C((4,),2)</span>
<span class="n">agent</span><span class="o">=</span><span class="n">ActorCriticAgent</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">()),</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>

<span class="n">block</span><span class="o">=</span><span class="n">FirstLastExperienceBlock</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">dls_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;bs&#39;</span><span class="p">:</span><span class="mi">128</span><span class="p">,</span><span class="s1">&#39;num_workers&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s1">&#39;verbose&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">,</span><span class="s1">&#39;indexed&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span><span class="s1">&#39;shuffle_train&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">})</span>
<span class="n">blk</span><span class="o">=</span><span class="n">IterableDataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">block</span><span class="p">),</span>
                      <span class="n">splitter</span><span class="o">=</span><span class="n">FuncSplitter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="kc">False</span><span class="p">),</span>
<span class="c1">#                       batch_tfms=lambda x:(x[&#39;s&#39;],x),</span>
                     <span class="p">)</span>
<span class="n">dls</span><span class="o">=</span><span class="n">blk</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">([</span><span class="n">env</span><span class="p">]</span><span class="o">*</span><span class="mi">15</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">128</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>

<span class="n">learner</span><span class="o">=</span><span class="n">A2CLearner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">A2CTrainer</span><span class="p">],</span><span class="n">reward_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AvgEpisodeRewardMetric</span><span class="p">()])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;model start&#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">o</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">learner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]))</span>
<span class="n">learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">wd</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>model start 0.00830515
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>train_avg_episode_r</th>
      <th>valid_loss</th>
      <th>valid_avg_episode_r</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>18.185844</td>
      <td>25.235353</td>
      <td>None</td>
      <td>25.235353</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>1</td>
      <td>26.704281</td>
      <td>34.280167</td>
      <td>None</td>
      <td>34.280167</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>2</td>
      <td>37.509953</td>
      <td>57.380167</td>
      <td>None</td>
      <td>57.380167</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>3</td>
      <td>49.414097</td>
      <td>91.272333</td>
      <td>None</td>
      <td>91.272333</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>4</td>
      <td>62.172638</td>
      <td>123.625167</td>
      <td>None</td>
      <td>123.625167</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>5</td>
      <td>67.281517</td>
      <td>161.537667</td>
      <td>None</td>
      <td>161.537667</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>6</td>
      <td>83.693108</td>
      <td>172.535000</td>
      <td>None</td>
      <td>172.535000</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>7</td>
      <td>86.530220</td>
      <td>187.985000</td>
      <td>None</td>
      <td>187.985000</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>8</td>
      <td>106.677887</td>
      <td>211.361667</td>
      <td>None</td>
      <td>211.361667</td>
      <td>00:10</td>
    </tr>
    <tr>
      <td>9</td>
      <td>147.460388</td>
      <td>212.576667</td>
      <td>None</td>
      <td>212.576667</td>
      <td>00:10</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.
  warn(&#34;Your generator is empty.&#34;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

