---

title: Data Transforms


keywords: fastai
sidebar: home_sidebar

summary: "Fastrl transforms for iterating through environments"
description: "Fastrl transforms for iterating through environments"
nb_path: "nbs/05_data.test_async.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/05_data.test_async.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MKL_THREADING_LAYER&#39;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;GNU&quot;</span>
<span class="c1">#     os.environ[&#39;MKL_SERVICE_FORCE_INTEL&#39;]=&quot;1&quot;</span>

    
    <span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
    <span class="k">try</span><span class="p">:</span><span class="n">mp</span><span class="o">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s1">&#39;spawn&#39;</span><span class="p">,</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    
    <span class="kn">import</span> <span class="nn">torch</span>
        <span class="c1"># Python native modules</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="c1"># Third party libs</span>
    <span class="kn">from</span> <span class="nn">fastai.torch_basics</span> <span class="kn">import</span> <span class="n">default_device</span>
<span class="c1">#     from fastai.data.all import DataLoader</span>
    <span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
    <span class="c1"># Local modules</span>
    <span class="kn">from</span> <span class="nn">fastrl.data.block</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1">#     import numpy as np</span>
<span class="c1">#     import gym</span>

    <span class="n">dl</span><span class="o">=</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">TestDataset</span><span class="p">(</span><span class="n">DQN</span><span class="p">(),</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">()),</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[ 0.0029,  0.2243, -0.0268, -0.2859],
        [-0.0092,  0.2149,  0.0272, -0.3320],
        [-0.0032,  0.1805,  0.0036, -0.2889],
        [ 0.0207,  0.2242,  0.0251, -0.3285],
        [-0.0018,  0.1739, -0.0041, -0.3238],
        [-0.0148,  0.2259,  0.0138, -0.3320],
        [-0.0209,  0.2315,  0.0425, -0.2785],
        [ 0.0137,  0.2178, -0.0129, -0.2591],
        [-0.0478,  0.1588,  0.0134, -0.2671],
        [-0.0123,  0.1720,  0.0453, -0.3096],
        [ 0.0170,  0.1961,  0.0188, -0.2525],
        [-0.0008,  0.1811, -0.0423, -0.2739],
        [-0.0218,  0.2282, -0.0361, -0.2851],
        [ 0.0457,  0.2301, -0.0402, -0.2755],
        [-0.0208,  0.1965, -0.0096, -0.2739],
        [ 0.0109,  0.2173, -0.0476, -0.3193],
        [ 0.0323,  0.2370, -0.0153, -0.2499],
        [ 0.0178,  0.2359, -0.0393, -0.2687],
        [ 0.0159,  0.1767,  0.0286, -0.2339],
        [-0.0192,  0.1692, -0.0251, -0.2674],
        [-0.0031,  0.2286, -0.0155, -0.2981],
        [ 0.0347,  0.1735, -0.0456, -0.3214],
        [ 0.0185,  0.2433,  0.0251, -0.3282],
        [-0.0089,  0.1710,  0.0463, -0.2364],
        [ 0.0052,  0.1845, -0.0241, -0.2957],
        [-0.0028,  0.2168, -0.0360, -0.2636],
        [ 0.0176,  0.2263, -0.0004, -0.2739],
        [ 0.0024,  0.2003,  0.0253, -0.3260],
        [-0.0230,  0.2368,  0.0205, -0.2966],
        [-0.0445,  0.1556,  0.0080, -0.3242],
        [-0.0211,  0.2180, -0.0113, -0.2494],
        [-0.0084,  0.1757, -0.0023, -0.3143],
        [ 0.0453,  0.1684,  0.0480, -0.2413],
        [ 0.0406,  0.2326,  0.0095, -0.3210],
        [-0.0123,  0.1817,  0.0407, -0.3033],
        [-0.0509,  0.1479, -0.0037, -0.2874],
        [ 0.0416,  0.2414, -0.0115, -0.3038],
        [ 0.0215,  0.1802, -0.0127, -0.3308],
        [-0.0507,  0.1577,  0.0015, -0.2707],
        [ 0.0136,  0.2131, -0.0371, -0.3353],
        [ 0.0015,  0.1484,  0.0330, -0.2645],
        [-0.0220,  0.1805, -0.0382, -0.3275],
        [-0.0454,  0.2036,  0.0300, -0.2466],
        [ 0.0276,  0.1548, -0.0257, -0.2556],
        [ 0.0308,  0.2446, -0.0173, -0.2585],
        [ 0.0296,  0.2234,  0.0222, -0.3344],
        [-0.0156,  0.1950,  0.0316, -0.2728],
        [ 0.0235,  0.2100, -0.0428, -0.3323],
        [ 0.0266,  0.2276, -0.0034, -0.2732],
        [ 0.0286,  0.2400, -0.0051, -0.2821]], device=&#39;cuda:0&#39;)
tensor([[-8.3691e-03,  2.0179e-01,  3.8112e-03, -2.9175e-01],
        [ 5.1506e-03,  1.8751e-01, -2.6569e-02, -3.3057e-01],
        [ 2.2314e-02,  2.0335e-01, -3.5689e-03, -3.0936e-01],
        [-2.8781e-03,  2.0828e-01,  2.5476e-02, -2.9808e-01],
        [-4.0183e-02,  1.8376e-01,  3.7913e-02, -3.1762e-01],
        [ 4.5876e-02,  1.5224e-01,  7.8895e-03, -2.9146e-01],
        [ 4.8383e-02,  1.9133e-01,  9.1852e-03, -3.3569e-01],
        [-3.8062e-02,  1.7461e-01,  2.3713e-02, -2.7468e-01],
        [ 1.0976e-02,  2.3605e-01,  1.5371e-02, -2.8039e-01],
        [-2.6792e-02,  2.0525e-01, -4.1664e-02, -3.3935e-01],
        [-3.9871e-02,  1.8716e-01,  3.1068e-02, -2.5313e-01],
        [-4.1199e-03,  1.9856e-01, -1.3566e-03, -2.4573e-01],
        [-1.5005e-02,  1.6323e-01,  4.1994e-02, -2.4890e-01],
        [-2.9621e-02,  1.8917e-01, -4.0450e-02, -2.6065e-01],
        [-2.8711e-02,  2.3691e-01,  2.7711e-02, -3.1707e-01],
        [ 2.5281e-02,  1.8824e-01, -2.2418e-02, -3.3106e-01],
        [-1.3550e-02,  1.9176e-01,  1.3949e-03, -3.2847e-01],
        [ 2.0196e-02,  1.8428e-01,  3.7789e-02, -2.6403e-01],
        [-2.3815e-02,  2.3396e-01,  1.1536e-02, -2.4843e-01],
        [-4.5149e-02,  2.1698e-01, -4.6656e-03, -3.3617e-01],
        [-4.0768e-03,  1.8751e-01, -5.9630e-04, -2.5924e-01],
        [ 1.0813e-02,  1.8120e-01, -1.2237e-02, -2.9450e-01],
        [ 2.9677e-02,  1.9443e-01, -3.2088e-02, -3.1432e-01],
        [ 1.2545e-02,  1.5336e-01, -2.2137e-02, -3.1149e-01],
        [ 3.3201e-02,  2.0852e-01,  2.9176e-02, -2.3487e-01],
        [ 3.0130e-02,  1.4710e-01,  9.7984e-04, -2.6547e-01],
        [ 4.6237e-03,  1.7177e-01, -1.8884e-02, -3.4595e-01],
        [-1.9321e-02,  1.9178e-01, -1.6898e-02, -2.5092e-01],
        [ 2.4451e-02,  1.8404e-01,  1.5687e-04, -3.0132e-01],
        [-1.0250e-02,  1.8176e-01, -2.9362e-02, -3.3899e-01],
        [ 2.5786e-02,  1.8460e-01, -2.7727e-02, -3.3918e-01],
        [-3.6707e-02,  2.1038e-01,  4.8188e-02, -2.9456e-01],
        [-3.9906e-02,  1.6935e-01, -2.9133e-03, -2.6095e-01],
        [ 2.9344e-02,  2.3238e-01,  2.0604e-02, -2.9346e-01],
        [ 4.9238e-02,  1.8641e-01, -6.8075e-03, -3.4132e-01],
        [ 1.6508e-02,  1.7193e-01,  1.2243e-02, -3.3530e-01],
        [ 4.0230e-02,  1.9666e-01, -3.5264e-02, -2.7026e-01],
        [ 2.9217e-02,  2.2746e-01,  2.1643e-02, -2.6296e-01],
        [-4.2447e-02,  1.6682e-01,  2.3985e-03, -2.4363e-01],
        [-3.3341e-02,  1.6699e-01,  7.8623e-03, -2.9578e-01],
        [-4.4401e-02,  1.9257e-01,  9.3763e-03, -2.8691e-01],
        [ 1.8716e-02,  1.4667e-01,  5.4878e-03, -2.4575e-01],
        [-1.9566e-02,  1.6187e-01,  6.2961e-03, -3.2844e-01],
        [-7.7962e-03,  1.6258e-01,  3.8627e-02, -3.2527e-01],
        [-3.0605e-02,  1.8889e-01, -3.7927e-02, -3.5330e-01],
        [ 2.9742e-02,  2.0599e-01, -3.7405e-02, -2.6445e-01],
        [ 5.2508e-03,  1.8063e-01, -1.4197e-02, -2.9927e-01],
        [-1.3265e-02,  2.0179e-01,  9.2818e-03, -3.1558e-01],
        [ 2.5906e-02,  1.5520e-01,  2.0920e-02, -2.6156e-01],
        [ 3.4256e-02,  1.5273e-01, -1.8483e-02, -2.6947e-01]], device=&#39;cuda:0&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

