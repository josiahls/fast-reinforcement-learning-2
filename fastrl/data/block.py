# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_data.block.ipynb (unless otherwise specified).

__all__ = ['DQN', 'init_experience', 'obj2tensor', 'TensorBatch', 'BD', 'cast_dtype', 'FakeAgent', 'ExperienceSource',
           'SourceDataset']

# Cell
# Python native modules
import os
from collections import deque
from time import sleep
# Third party libs
from fastcore.all import *
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.basics import *
from torch.utils.data import Dataset
from torch import nn
import torch
import gym
# Local modules
from ..core import *

# Cell
class DQN(Module):
    def __init__(self):
        self.policy=nn.Sequential(
            nn.Linear(4,50),
            nn.ReLU(),
            nn.Linear(50,2),
            nn.ReLU()
        )

    def forward(self,x):
        return torch.argmax(self.policy(x),dim=0)

# Cell
def init_experience(but='',**kwargs):
    "Returns dictionary with default values that can be overridden."
    experience=D(
        state=0,action=0,next_state=0,reward=0,done=False,
        step=0,env=0,image=0
    )
    for s in but.split(','):
        if s in experience:del experience[s]
    return BD.merge(experience,kwargs)

# Cell
def obj2tensor(o):
    return (o if isinstance(o,TensorBatch) else
            TensorBatch(o) if isinstance(o,(L,list,np.ndarray)) else
            TensorBatch([o]))

def _get_bs(o): return o.bs

class TensorBatch(TensorBase):
    "A tensor that maintains and propagates a batch dimension"
    def __new__(cls, x, bs=1,**kwargs):
        res=super(TensorBatch,cls).__new__(cls,x, bs=bs,**kwargs)

        if res.bs==1:
            if len(res.shape)<2: res=res.unsqueeze(0)
        else:
            if res.shape[0]!=bs and res.shape[1]==bs and len(res.shape)==2:
                res=torch.transpose(res)

        assert len(res.shape)>1,f'Tensor has shape {res.shape} while bs is {bs}'
        return res

    @classmethod
    def vstack(cls,*args):
        new_bs=sum(map(_get_bs,*args))
        return cls(torch.vstack(*args),bs=new_bs)

# Cell
class BD(D):
    def __init__(self,*args,v=False,bs=1,**kwargs):
        store_attr()
        super().__init__(*args,**kwargs)
        if not self.mapping: self.update(self.mapv(obj2tensor))
        self.bs=list(self.values())[0].bs

    # I think this is fine, I wouldnt change this
    def __radd__(self,o): return self if isinstance(o,int) else self.__add__(o)
    # This is "ok", I think that the resulting tensor should be a batch tensor
    def __add__(self,o):
        return BD({k:TensorBatch.vstack((self[k],o[k])) for k in self})

    # I think this needs to change a little.
    # instead of bs=1 if isinstance(o,int) else None
    # it should just pass it in and the tensor should simply maintain its shape
    def __getitem__(self,o):
        if is_listy(o) or isinstance(o,(TensorBatch,int,Tensor)):
            bs=1 if isinstance(o,int) else None
            return type(self)({k:self[k][o] for k in self},bs=bs)
        return super().__getitem__(o)

    # This is fine
    @classmethod
    def merge(cls,*ds,**kwargs): return cls(merge(*ds),**kwargs)
    # This is ok
    @delegates(pd.DataFrame)
    def pandas(self,mu=False,**kwargs):
        "Turns a `BD` into a pandas Dataframe optionally showing `mu` of values."
        return pd.DataFrame(merge(
            *tuple(tensor2shape(k,v) for k,v in self.items()),
            *(tuple(tensor2mu(k,v) for k,v in self.items()) if mu else ())
        ),**kwargs)

# Cell
def _state2experience(s,**kwargs):   return init_experience(state=s,next_state=s,step=torch.zeros((1,1)),**kwargs)
def _env_reset(o):                   return o.reset()
def _env_seed(o,seed):               return o.seed(seed)
def _env_render(o,mode='rgb_array'): return TensorBatch(o.render(mode=mode).copy())
def _env_step(o,*args,**kwargs):     return o.step(*args,**kwargs)

def cast_dtype(t,dtype):
    if dtype==torch.float:    return t.float()
    elif dtype==torch.double: return t.double()
    elif dtype==torch.long:   return t.long()

class FakeAgent:
    def __init__(self,action_space): store_attr()
    def __call__(self,state,**kwargs):
        return (L([self.action_space.sample() for _ in range(state.shape[0])]),
                D(merge(kwargs,{'random_action':np.random.randint(0,3,(state.shape[0],1))})))

class ExperienceSource(Stateful):
    _stateattrs=('pool',)
    def __init__(self,env:str,agent=None,n_envs:int=1,steps_count:int=1,steps_delta:int=1,
                 seed:int=None,render=None,num_workers=0,but='',**kwargs):
        store_attr()
        self.env_kwargs=kwargs
        self.pool=L()
        if self.render is None: self.but+=',image'

    def _init_state(self):
        "Inits the histories, experiences, and the environment pool when sent to a `Process`"
        self.history,self.pool=L((deque(maxlen=self.steps_count),
                                  gym.make(self.env,**self.env_kwargs))
                                  for _ in range(self.n_envs)).zip().map(L)
        self.pool.map(_env_seed,seed=self.seed)
        if self.agent is None: self.agent=FakeAgent(self.pool[0].action_space)
        self.reset_all()

    def reset_all(self):
        self.experiences=self.pool.map(_env_reset)
        self.experiences=self.experiences.map(_state2experience,but=self.but)
        self.experiences=sum(self.experiences)
        self.attempt_render(self.experiences)

    def attempt_render(self,experiences,indexes=None):
        if self.render is not None:
            pool=self.pool if indexes is None else self.pool[indexes]
            renders=pool.map(_env_render,mode=self.render)
            # No idea why we have to do this, but multiprocessing hangs forever otherwise
            if self.num_workers>0:sleep(0.1)
            experiences['image']=torch.stack(tuple(renders)).unsqueeze(0)

    def __iter__(self):
        "Iterates through a list of environments."
        if not self.pool:self._init_state()
        while True:
            try:
#                 print('starting loop')
                # Only work on envs that are not done
                not_done_idxs=(self.experiences['done']==False).nonzero()[:,0]
                if len(not_done_idxs)==0:
                    self.reset_all()
                    not_done_idxs=(self.experiences['done']==False).nonzero()[:,0]
                not_done_idxs=not_done_idxs.reshape(-1,)
                not_done_experiences=self.experiences[not_done_idxs]
#                 print(not_done_experiences.bs,self.experiences,not_done_experiences,not_done_idxs)
                # Pass current experiences into agent
                actions,experiences=self.agent(**not_done_experiences)
                # Step through all envs.
                step_res=self.pool[not_done_idxs].zipwith(actions).starmap(_env_step)
                next_states,rewards,dones=step_res.zip()[:3].map(TensorBatch)
                rewards,dones=(v.reshape(len(not_done_idxs),-1) for v in (rewards,dones))
                # Add the image field if available
                self.attempt_render(self.experiences,not_done_idxs)
                new_exp=BD(next_state=next_states,reward=rewards,done=dones,
                           env=not_done_idxs.reshape(not_done_experiences.bs,-1),
                           step=not_done_experiences['step']+1,
                           bs=not_done_experiences.bs)

                experiences=BD.merge(not_done_experiences,experiences,new_exp,
                                     bs=not_done_experiences.bs)
#                 print(not_done_experiences.bs,experiences.bs,new_exp.bs)
#                 print(not_done_experiences,'\n',experiences,'\n',new_exp)
#                 print(not_done_experiences)
#                 print(not_done_idxs)
                for idx in not_done_idxs:
#                     print('adding histpru')
#                     print(experiences,idx,experiences.bs)
                    self.history[idx].append(experiences[idx])
                    if len(self.history[idx])==self.steps_count and \
                           int(experiences[idx]['step'][0])%self.steps_delta==0:
                        yield tuple(self.history[idx])

                    if bool(experiences[idx]['done'][0]):
                        if 0<len(self.history[idx])<self.steps_count:
                            yield tuple(self.history[idx])
                        while len(self.history[idx])>1:
                            self.history[idx].popleft()
                            yield tuple(self.history[idx])

                for k in experiences:
                    dtype=experiences[k][not_done_idxs].dtype
                    if k not in self.experiences:
                        self.experiences[k]=torch.zeros(self.experiences.bs,
                                                        *experiences[k][not_done_idxs].shape[1:])
                    if self.experiences[k][not_done_idxs].dtype!=dtype:
                        self.experiences[k]=cast_dtype(self.experiences[k],dtype)
                    self.experiences[k][not_done_idxs]=experiences[k][not_done_idxs]
            except ValueError:
                self.reset_all()

add_docs(ExperienceSource,
        """Iterates through `n_envs` of `env` feeding experience or states into `agent`.
           If `agent` is None, then random actions will be taken instead.
           It will return `steps_count` experiences every `steps_delta`.
           At the end of an env, it will return `steps_count-1` experiences per next. """,
        reset_all="resets the envs and experience",
        attempt_render="Updates `experiences` with images if `render is not None`. Optionally indexes can be passed.")

# Cell
class SourceDataset(IterableDataset):
    "Iterates through a `source` object. Allows for re-initing source connections when `num_workers>0`"
    def __init__(self,source=None): store_attr('source')
    def __iter__(self):             return iter(self.source)
    def wif(self):                  self.source._init_state()