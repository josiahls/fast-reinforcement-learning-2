# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/15_actorcritic.a3c_data.ipynb (unless otherwise specified).

__all__ = ['LinearA2C', 'unbatch', 'loss_func', 'A3CLearner', 'A3CTrainer']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class LinearA2C(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(LinearA2C, self).__init__()

        self.policy = nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

        self.value = nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def forward(self,x):
#         print(x)
        fx=x.float()
        return self.policy(fx),self.value(fx)

# Cell
def unbatch(batch, net, last_val_gamma, device='cpu'):
    states = []
    actions = []
    rewards = []
    not_done_idx = []
    last_states = []

    for idx, exp in enumerate(batch):
#         print(exp.state.numpy()[0].shape,int(exp.action),float(exp.reward),exp.last_state.numpy()[0].shape if not bool(exp.done) else None,exp.done)
        states.append(np.array(exp.state.numpy()[0], copy=False))
        actions.append(int(exp.action))
        rewards.append(float(exp.reward))
        if not exp.done: #exp.last_state is not None:
            not_done_idx.append(idx)
            # if exp.last_state is None:print(exp)
            last_states.append(np.array(exp.last_state.numpy()[0], copy=False))
        # else:
        #     print(exp,'is done, so skipping')
    states_v = torch.FloatTensor(states).to(device)
    actions_t = torch.LongTensor(actions).to(device)

    # handle rewards
    rewards_np = np.array(rewards, dtype=np.float32)
    if not_done_idx:
        # print(last_states)
        last_states_v = torch.FloatTensor(last_states).to(device)
        last_vals_v = net(last_states_v)[1]
        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]
        # print(last_vals_v.data.cpu().numpy().mean())
        rewards_np[not_done_idx] += last_val_gamma * last_vals_np

    # print(len(not_done_idx),len(rewards_np),len(last_states))
    ref_vals_v = torch.FloatTensor(rewards_np).to(device)
    return states_v, actions_t, ref_vals_v

# Cell
def loss_func(pred,a,r,sp,d,episode_rewards,learn=None):
#     print(len(learn.xb[0]),len(a),len(r),len(sp),len(d))
#     print(learn.xb[0],a,r,sp,d)
#     print(len(learn.xb[0]))

    yb=[]
    for i in range(len(learn.xb[0])):
#         print(learn.xb[0][i],a[i],r[i],sp[i],d[i])
#         print(a[i])
        yb.append(ExperienceFirstLast(state=learn.xb[0][i],action=a[i],reward=r[i],last_state=sp[i],done=d[i],episode_reward=0))

    s_t,a_t,r_est=unbatch(yb,learn.model,learn.discount**learn.reward_steps)
#     r_est=r_est.squeeze(1)
#     print(r_est.mean(), np.mean([o.reward for o in yb]))
#     print(s_t.shape,a_t.shape,r_est.shape)

#     print(r_est.mean(),np.mean([o.r.numpy() for o in yb]))
#     print(sum([o.d for o in yb]))
#     print(s_t.shape,a_t.shape,r_est.shape,len(yb))

    learn.opt.zero_grad()
    logits_v,value_v=learn.model(s_t)
#     print(logits_v.shape,value_v.shape)

    loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)

    log_prob_v=F.log_softmax(logits_v,dim=1)
    adv_v=r_est-value_v.detach()

    log_prob_actions_v=adv_v*log_prob_v[range(len(learn.xb[0][0])),a_t]
    loss_policy_v=-log_prob_actions_v.mean()

    prob_v=F.softmax(logits_v,dim=1)
    entropy_loss_v=learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()

    loss_v=entropy_loss_v+loss_value_v+loss_policy_v
#     print(loss_v.detach(),entropy_loss_v.detach(),loss_value_v.detach(),loss_policy_v.detach(),'\n')

    return loss_v

class A3CLearner(AgentLearner):
    def __init__(self,dls,discount=0.99,entropy_beta=0.01,clip_grad=0.1,reward_steps=1,**kwargs):
        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)
        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))
        self.model.share_memory()
        self.discount=discount
        self.entropy_beta=entropy_beta
        self.reward_steps=reward_steps
        self.clip_grad=clip_grad

#     def _split(self, b):
#         if len(b)==1 and type(b[0])==tuple:b=b[0]
#         super()._split(b)

# Cell
class A3CTrainer(Callback):

    def after_backward(self):
        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)