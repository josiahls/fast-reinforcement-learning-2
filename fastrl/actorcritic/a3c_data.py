# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/15_actorcritic.a3c_data.ipynb (unless otherwise specified).

__all__ = ['LinearA2C', 'r_estimate', 'unbatch', 'loss_func', 'A3CLearner', 'A3CTrainer', 'FirstLastTfm',
           'FirstLastExperienceBlock']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class LinearA2C(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(LinearA2C, self).__init__()

        self.policy = nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

        self.value = nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def _get_conv_out(self, shape):
        o=self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self,x):
        fx=x.float()
        return self.policy(fx),self.value(fx)

# Cell
def r_estimate(s,r,d_mask,non_d_mask,model,val_gamma,device):
    "Returns rewards `r` estimated direction by `model` from states `s`"
    r_np = np.array(r, dtype=np.float32)
    if len(d_mask) != 0:
        s_v = torch.FloatTensor(s).to(device)
        v = model(s_v)[1]  # Remember that models are going to return the actions and the values
        v_np = v.data.cpu().numpy()[:, 0]
        r_np[d_mask] += val_gamma * v_np
    return r_np

def unbatch(batch,model,last_val_gamma,device='cpu'):
    s, a, r, d_mask, sp = [], [], [], [], []
    non_d_mask = []
    for i, exp in enumerate(batch):
#         print(exp.s.shape,exp.r.shape,exp.sp.shape,exp.a.shape,exp.d.shape)
#         raise Exception
        s.append(exp.s.numpy()[0])
        a.append(int(exp.a.numpy()))  # TODO can we change this to toggle between discrete and continuous actions?
        r.append(exp.r.numpy().astype(np.float32).reshape(1,))
        if not bool(exp.d):
            d_mask.append(i)
            sp.append(exp.sp.numpy()[0].reshape(1,-1))
        else:
            non_d_mask.append(i)
    s_t = torch.FloatTensor(s).to(device)
    a_t = torch.LongTensor(a).to(device)
    r_np = r_estimate(sp, r, d_mask, non_d_mask,model, last_val_gamma, device)
    estimated_r = torch.FloatTensor(r_np).to(device)
    return s_t, a_t, estimated_r

# Cell
def loss_func(pred,yb,learn):
#     print(yb)
    yb=[Experience(**{k:yb[k][i] for k in yb}) for i in range(learn.dls.bs)]
    s_t,a_t,r_est=unbatch(yb,learn.model,learn.discount**learn.reward_steps)
#     print(r_est.mean(),np.mean([o.r.numpy() for o in yb]))
#     print(sum([o.d for o in yb]))
#     print(s_t.shape,a_t.shape,r_est.shape)
#     r_est=r_est.squeeze(1)

    learn.opt.zero_grad()
    logits_v,value_v=learn.model(s_t)

    loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)

    log_prob_v=F.log_softmax(logits_v,dim=1)
    adv_v=r_est-value_v.detach()

    log_prob_actions_v=adv_v*log_prob_v[range(learn.dls.bs),a_t]
    loss_policy_v=-log_prob_actions_v.mean()

    prob_v=F.softmax(logits_v,dim=1)
    entropy_loss_v=learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()

    loss_v=entropy_loss_v+loss_value_v+loss_policy_v

    return loss_v

class A3CLearner(AgentLearner):
    def __init__(self,dls,discount=0.99,entropy_beta=0.01,clip_grad=0.1,reward_steps=1,**kwargs):
        self.create_m=True
        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)
        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))
        self.model.share_memory()
        self.discount=discount
        self.entropy_beta=entropy_beta
        self.reward_steps=reward_steps
        self.clip_grad=clip_grad

    def _split(self, b):
        if len(b)==1 and type(b[0])==tuple:b=b[0]
        super()._split(b)

# Cell
class A3CTrainer(Callback):

    def after_backward(self):
        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)

# Cell
class FirstLastTfm(Transform):
    def __init__(self,discount=0.99):self.discount=discount

    def reset(self,items):
        if items.extra_len!=0:items.extra_len=0

    def encodes(self,o):
        first_o=o[0]
        first_o.sp=o[-1].sp
        total_reward=first_o.r
        elms=list(o)[:-1]

        for exp in elms: # reversed(elms):
            total_reward*=self.discount
            total_reward+=exp.r
        first_o.r=total_reward
#         if any([t.absolute_end for t in o]): print(first_o)

        return asdict(first_o)


@delegates(ResetAndStepTfm)
def FirstLastExperienceBlock(dls_kwargs=None,**kwargs):
    return TransformBlock(type_tfms=[MakeTfm(),ResetAndStepTfm(hist2dict=False,**kwargs),FirstLastTfm],dl_type=TfmdSourceDL,dls_kwargs=dls_kwargs)