# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14c_actorcritic.dads.ipynb (unless otherwise specified).

__all__ = ['OptionalClampLinear', 'MultiCompGMM', 'SimpleGMM', 'GMM', 'SkillDynamics']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
import torch.nn.functional as F
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *
from dataclasses import dataclass

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *
from fastai.callback.progress import *
from ..ptan_extension import *
from .sac import *
from .diayn import *

from torch.distributions import *

import matplotlib.pyplot as plt

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class OptionalClampLinear(Module):
    def __init__(self,num_inputs,state_dims,fix_variance:bool=False,
                 clip_min=0.3,clip_max=10.0):
        "Linear layer or constant block used for std."
        store_attr()
        if not self.fix_variance: self.fc=nn.Linear(self.num_inputs,self.state_dims)

    def forward(self,x):
        if self.fix_variance: return torch.full((x.shape[0],self.state_dims),1.0)
        else:                 return torch.clamp(nn.Softplus()(self.fc(x)),self.clip_min,self.clip_max)

class MultiCompGMM(Module):
    def __init__(self,num_inputs,state_dims,n_components,fix_variance:bool=False):
        "Multi-component GMM parameterized by a fully connected layer with optional std layer."
        store_attr()
        self.logit_fc=nn.Linear(self.num_inputs,self.n_components)
        self.mean_fcs=nn.ModuleList([nn.Linear(self.num_inputs,self.state_dims)
                                     for _ in range(self.n_components)])
        self.std_fcs=nn.ModuleList([OptionalClampLinear(self.num_inputs,self.state_dims,fix_variance)
                                    for _ in range(self.n_components)])
        self.means,self.logits,self.stds=[],[],[]

    def forward(self,x):
        self.means=torch.stack([o(x) for o in self.mean_fcs],dim=1)
        self.stds=torch.stack([o(x) for o in self.std_fcs],dim=1)
        self.logits=self.logit_fc(x)
        return MixtureSameFamily(
            mixture_distribution=Categorical(self.logits),
            component_distribution=Independent(Normal(self.means,self.stds),1)
        )

class SimpleGMM(Module):
    def __init__(self,num_inputs,state_dims,fix_variance:bool=False):
        "Single-component GMM parameterized by a fully connected layer with optional std layer."
        store_attr()
        self.mean_fc=nn.Linear(self.num_inputs,self.state_dims)
        self.std_fc=OptionalClampLinear(self.num_inputs,self.state_dims,fix_variance)

    def forward(self,x): return Independent(Normal(self.mean_fc(x),self.std_fc(x)),1)

class GMM(Module):
    def __init__(self,num_inputs,state_dims,n_components,fix_variance:bool=False):
        "N-component GMM parameterized by fully connected layers with optional std layers."
        store_attr()
        if self.n_components>1: self.distribution=MultiCompGMM(num_inputs,state_dims,n_components,fix_variance)
        else:                   self.distribution=SimpleGMM(num_inputs,state_dims,fix_variance)

    def forward(self,x): return self.distribution(x)

# Cell
class SkillDynamics(Module):
    def __init__(self,s_dim,a_dim,n_components,fix_variance:bool=False,
                 use_model_mean:bool=None,use_batch_norm:bool=True,fc_params:tuple=None):
        store_attr(but='fc_params,use_model_mean')
        self.fc_params=ifnone(fc_params,(256,256))
        self.use_model_mean=ifnone(use_model_mean,n_components>1)
        if self.use_batch_norm:
            self.s_bn,self.sp_bn=nn.BatchNorm1d(s_dim),nn.BatchNorm1d(s_dim)
        self.fcs=nn.Sequential(*[nn.Linear((s_dim+a_dim) if i==0 else self.fc_params[i-1],p)
                                for i,p in enumerate(self.fc_params)])

        self.gmm=GMM(self.fc_params[-1],s_dim,n_components,fix_variance)

    def forward(self,s,a,sp=None,ignore_bn=False):
        "Returns the `GMM` distribution of `s` and `a`, mean, and **optionally**"\
        "log(p) of the state transition between `s` and `sp` if `sp` is not None."
        if sp is not None: sp=sp-s

        if self.use_batch_norm and not ignore_bn:
            s=self.s_bn(s)
            if sp is not None: sp=self.sp_bn(sp)

        sa=torch.hstack([s,a])

        x=self.fcs(sa)

        dist=self.gmm(x)
        return dist,(sp if sp is None else dist.log_prob(sp))

    def log_prob(self,s,a,sp): return self(s,a,sp,ignore_bn=True)[1]

    def predict_state(self,s,a):
        "Returns the predicted state that `s` and `a` will result in."
        dist,_=self(s,a)
        if self.use_model_mean:
            means,idx=dist.component_distribution.mean,torch.argmax(dist.mixture_distribution.logits,dim=1)
            pred_s=means[[torch.arange(means.shape[0]),idx]]
        else:
            pred_s=dist.mean

        if self.use_batch_norm:
            pred_s=pred_s*(self.s_bn.running_var+1e-3).sqrt()+self.s_bn.running_mean

        pred_s+=s
        return pred_s