# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14c_actorcritic.dads.ipynb (unless otherwise specified).

__all__ = ['Discriminator', 'DADS', 'DiscriminatorTrainer']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
import torch.nn.functional as F
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *
from dataclasses import dataclass

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *
from fastai.callback.progress import *
from ..ptan_extension import *
from .sac import *
from .diayn import *

from torch.distributions import *

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class Discriminator(Module):
    "`Module` for storing skills. Receives input (`num_inputs`+`num_actions`) -> `num_skills`."
    def __init__(self, num_inputs,num_actions,num_skills,hidden_dim):
        self.linear1 = nn.Linear(num_inputs+num_skills, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        self.linear3 = nn.Linear(hidden_dim,num_skills)

        self.apply(weights_init_)

    def forward(self, state):
        x = F.relu(self.linear1(state.float()))
        x = F.relu(self.linear2(x))
        return self.linear3(x)

# Cell
@delegates(SAC)
class DADS(SAC):
    def __init__(self,num_inputs,action_space,discriminator:Module=None,num_skills:int=20,
                 find_best_skill_interval:int=10,scale_entropy:float=1,
                 best_skill_n_rollouts:int=10,include_actions:bool=False,
                 learn_p_z:bool=False,add_p_z:bool=True,hidden_size=100,lr=0.003,**kwargs):
        store_attr()
        self.num_inputs=num_inputs+self.num_skills
        self.original_num_inputs=num_inputs
        self.p_z=np.full(self.num_skills,1.0/self.num_skills)
        self.discriminator=Discriminator(self.original_num_inputs,action_space.shape[0],
                                         num_skills,hidden_size)

        self.discriminator_optim = Adam(self.discriminator.parameters(), lr=self.lr)

        self.log_p_z_episode=[]
        self.z=0
        self.reset_z()


        super().__init__(self.num_inputs,action_space,hidden_size=hidden_size,lr=lr,**kwargs)

    def sample_z(self):
        """Samples z from p(z), using probabilities in self._p_z."""
        return np.random.choice(self.num_skills,p=self.p_z)

    def reset_z(self): self.z=self.sample_z()
    def __call__(self,s,asl):
        aug_s=self.concat_obs_z(s,self.z)
        return super().__call__(aug_s,asl)

    def concat_obs_z(self,obs,z):
        """Concatenates the observation to a one-hot encoding of Z."""
        assert np.isscalar(z)
        if type(obs)==list and len(obs)==1: obs=obs[0]
        if len(obs.shape)==2 and obs.shape[0]==1: obs=obs[0]

        z_one_hot=np.zeros(self.num_skills)
        z_one_hot[z]=1
        if type(obs)==Tensor: obs=obs.cpu()
        return torch.FloatTensor(np.hstack([obs,z_one_hot])).reshape(1,-1)

    def skill_p(self,skill,next_state):
        unnorm_skill_dist=self.discriminator(next_state).unsqueeze(0)
        skill_p=F.softmax(unnorm_skill_dist)[:,skill]
        return skill_p,unnorm_skill_dist

    def discriminator_learn(self,skill,out):
        self.discriminator_optim.zero_grad()
        loss=nn.CrossEntropyLoss()(out,torch.LongTensor([skill]))
        loss.backward()
        self.discriminator_optim.step()

    def intrinsic_reward(self,next_state):
        skill_p,disc_out=self.skill_p(self.z,next_state)
        intrinsic_reward=np.log(skill_p.cpu().detach()+1e-8)-np.log(self.p_z[self.z])
#         print(skill_p,self.p_z,intrinsic_reward)
        return intrinsic_reward,disc_out


# Cell
class DiscriminatorTrainer(ExperienceReplay):

    def __init__(self,*args,**kwargs):
        self.log_p_z_episode=[]
        super().__init__(*args,**kwargs)

    def before_fit(self):
        self.learn.agent.warming_up=True
        while len(self.queue)<self.starting_els:
            for i,o in enumerate(self.dls.train):
                z=self.learn.agent.z
                batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(o[0][i],z)[0],
                                           action=o[1][i],
                                           reward=o[2][i],
                                           last_state=self.learn.agent.concat_obs_z(o[3][i],z)[0],
                                           done=(o[4][i] and self.max_steps!=o[6][i]),
                                           episode_reward=o[5][i],steps=o[6][i])
                                    for i in range(len(o[0]))]
#                 print(self.max_steps,max([o.steps for o in batch]))
#                 print(batch[0])
                for k in range(len(batch)):
                    intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))
                    self.learn.agent.discriminator_learn(self.agent.z,disc_out)
                    batch[k]=ExperienceFirstLast(
                        state=batch[k].state.to(device=default_device()),
                        action=batch[k].action,
                        reward=intrinsic_reward,
                        last_state=batch[k].last_state.to(device=default_device()),
                        done=batch[k].done,
                        episode_reward=batch[k].episode_reward,
                        steps=batch[k].steps
                    )


#                 print(batch[0])
                for _b in batch:self.queue.append(_b)
                if any([_b.done for _b in batch]): self.learn.agent.reset_z()
                if len(self.queue)>self.starting_els:break
        self.learn.agent.warming_up=False

# #     def after_epoch(self):
# #         print(len(self.queue))
    def before_batch(self):
#         print(len(self.queue))
        b=list(self.learn.xb)+list(self.learn.yb)
        z=self.learn.agent.z
        batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(b[0][i],z)[0],
                                   action=b[1][i],
                                   reward=b[2][i],
                                   last_state=self.learn.agent.concat_obs_z(b[3][i],z)[0],
                                   done=(b[4][i] and self.max_steps!=b[6][i]),
                                   episode_reward=b[5][i],steps=b[6][i])
              for i in range(len(b[0]))]

        for k in range(len(batch)):
            intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))
            self.learn.agent.discriminator_learn(self.agent.z,disc_out)
            batch[k]=ExperienceFirstLast(
                state=batch[k].state.to(device=default_device()),
                action=batch[k].action,
                reward=intrinsic_reward,
                last_state=batch[k].last_state.to(device=default_device()),
                done=batch[k].done,
                episode_reward=batch[k].episode_reward,
                steps=batch[k].steps
            )

#         print(self.learn.xb)
        self.learn.xb=(torch.stack([e.state for e in batch]),)
#         print(self.learn.yb)
        self.learn.yb=(torch.stack([o.action for o in batch]),
                       torch.stack([o.reward for o in batch]),
                       torch.stack([o.last_state for o in batch]),
                       torch.stack([o.done for o in batch]),
                       torch.stack([o.episode_reward for o in batch]),
                       torch.stack([o.steps for o in batch]))
#         print(self.learn.yb)

        for _b in batch: self.queue.append(_b)
        idxs=np.random.randint(0,len(self.queue), self.bs)
        self.learn.sample_yb=[self.queue[i] for i in idxs]