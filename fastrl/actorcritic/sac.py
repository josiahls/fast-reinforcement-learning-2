# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_actorcritic.sac.ipynb (unless otherwise specified).

__all__ = ['Critic', 'Actor', 'SACTrainer', 'loss_func', 'SACLearner']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *
from fastai.callback.progress import *

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class Critic(Module):
    def __init__(self, input_shape, n_actions):
        self.q=nn.Sequential(
            nn.Linear(input_shape[0]+n_actions, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def forward(self,x):
        fx=x.float()
        return self.q(fx)

class Actor(Module):
    def __init__(self, input_shape, n_actions):
        self.actions=nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

    def forward(self,x):
        fx=x.float()
        return self.actions(fx)

# Cell
class SACTrainer(Callback):
    def __init__(self):     self.batch_n=0
    def before_fit(self):   self.batch_n=0
    def after_batch(self):
        if self.batch_n%self.soft_copy_freq==0: self.model.soft_copy()
        self.batch_n+=1

    def after_backward(self):
        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)

# Cell
def loss_func(pred,yb,learn):



    return loss_v

class SACLearner(AgentLearner):
    def __init__(self,dls,action_shape,critic_tau=0.1,discount=0.99,action_range:Tuple=None,temp=0.9,init_temp=0.1,
                 actor_copy_freq=1,critic_copy_freq=1,**kwargs):
        store_attr()
        self.action_range=ifnone(self.action_range,(-1,1))
        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)
        self.log_alpha=torch.FloatTensor(torch.log(init_temp))
        self.log_alpha.requires_grad=True
        self.tgt_entropy= -action_shape

#     def _split(self, b):
#         if len(b)==1 and type(b[0])==tuple:b=b[0]
#         super()._split(b)