# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_actorcritic.sac.ipynb (unless otherwise specified).

__all__ = ['Critic', 'Actor', 'SACTrainer', 'loss_func', 'SACLearner']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *
from fastai.callback.progress import *

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class Critic(Module):
    def __init__(self, input_shape, n_actions):
        super(Critic, self).__init__()

        self.q=nn.Sequential(
            nn.Linear(input_shape[0]+n_actions, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def forward(self,x):
        fx=x.float()
        return self.q(fx)

class Actor(Module):
    def __init__(self, input_shape, n_actions):
        super(Actor, self).__init__()

        self.actions=nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

    def forward(self,x):
        fx=x.float()
        return self.actions(fx)

# Cell
class SACTrainer(Callback):
    def __init__(self):     self.batch_n=0
    def before_fit(self):   self.batch_n=0
    def after_batch(self):  self.batch_n%
    def after_backward(self):
        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)

# Cell
def loss_func(pred,yb,learn):



    return loss_v

class SACLearner(AgentLearner):
    def __init__(self,dls,**kwargs):
        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)

    def _split(self, b):
        if len(b)==1 and type(b[0])==tuple:b=b[0]
        super()._split(b)