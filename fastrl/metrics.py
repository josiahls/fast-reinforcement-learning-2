# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_metrics.ipynb (unless otherwise specified).

__all__ = ['TotalRewards', 'RewardMetric']

# Cell
from fastai.callback import *
from fastai.basic_train import *
from fastai.core import *
from fastai.torch_core import *
from dataclasses import dataclass
import torch.multiprocessing as mp
import logging

logging.basicConfig(format='[%(asctime)s] p%(process)s line:%(lineno)d %(levelname)s - %(message)s',
                    datefmt='%m-%d %H:%M:%S')
logging.getLogger('fastrl.data_block').setLevel('CRITICAL')
_logger=logging.getLogger(__name__)

# Cell
@dataclass
class TotalRewards(object):
    rewards:float

class RewardMetric(LearnerCallback):
    _order=-20

    def on_train_begin(self, **kwargs):
        metric_names = ['train_reward'] if self.learn.recorder.no_val or self.learn.data.empty_val else ['train_reward', 'valid_reward']
        self.learn.recorder.add_metric_names(metric_names)
        for ds in [self.learn.data.train_ds,None if self.learn.data.empty_val else self.learn.data.valid_ds]:
            if hasattr(ds,'metric_queue') and ds.metric_queue is None:
                ds.metric_queue=mp.JoinableQueue(ds.queue_sz*len(ds)) # Make sure this queue has more space to prevent locking


    def on_epoch_end(self,last_metrics,**kwargs: Any):
        rewards=[]
        for ds in [self.learn.data.train_ds,None if self.learn.data.empty_val else self.learn.data.valid_ds]:
            if ds is None:continue
            rs=[]
            if hasattr(ds,'metric_queue'):
                if ds.metric_queue is not None:
                    while not ds.metric_queue.empty():
                        rs.append(ds.metric_queue.get().rewards)
            else:rs=ds.pop_total_r()
            rewards.append(np.mean(rs))
        return add_metrics(last_metrics,rewards)