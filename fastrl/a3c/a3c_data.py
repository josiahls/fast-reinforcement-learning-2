# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/12_a3c.a3c_data.ipynb (unless otherwise specified).

__all__ = ['a3c_data_fitter', 'A3CLearner', 'r_estimate', 'unbatch', 'A3CTrainer', 'debug_batch']

# Cell
# from fastai.basic_data import *
import torch.nn.utils as nn_utils
from fastai.torch_core import *
from fastai.callbacks import *
from fastai.basic_train import *
from fastai.callback import *
from fastai.basic_data import *
from ..wrappers import *
from ..basic_agents import *
from ..basic_train import *
from ..data_block import *
from ..metrics import *
from dataclasses import asdict
from functools import partial
from fastprogress.fastprogress import IN_NOTEBOOK
from fastcore.utils import *
import torch.multiprocessing as mp
import torch.optim as optim
from queue import Empty
import textwrap
import logging
import gym

logging.basicConfig(format='[%(asctime)s] p%(process)s line:%(lineno)d %(levelname)s - %(message)s',
                    datefmt='%m-%d %H:%M:%S')
_logger=logging.getLogger(__name__)

# Cell
def a3c_data_fitter(model:Optional[nn.Module],learner_cls:Optional['AgentLearner'],agent:Optional['BaseAgent'],ds_cls:ExperienceSourceDataset,
            pause_event:mp.Event,cancel_event:mp.Event,main_queue:Optional[mp.JoinableQueue],metric_queue:Optional[mp.JoinableQueue],display=False,
            rows=1,cols=1,max_w=800):
    "A3C fitter for AsyncExperienceSourceDataset."
    ds=ds_cls()
    if display:ds=DatasetDisplayWrapper(ds,rows=rows,cols=cols,max_w=max_w)
    dl=DataLoader(ds,batch_size=1,num_workers=0)
    if learner_cls is not None:
        learn=learner_cls(data=DataBunch(dl,dl),model=model,agent=agent)
        ds.learn=learn
    try:
        while not cancel_event.is_set():
            for xb,yb in ds:
#                 print(yb)
                while pause_event.is_set() and not self.cancel_event.is_set():cancel_event.wait(0.1)
                if main_queue is not None:main_queue.put(yb)
            if metric_queue is not None:
                total_rewards=ds.pop_total_rewards()
                if total_rewards:
#                     print(total_rewards)
                    sys.stdout.flush()
                    if metric_queue.full():_logger.warning('Metric queue is full. Increase its size,empty it, or set metric_queue to None.')
                    metric_queue.put(TotalRewards(total_rewards))
            while pause_event.is_set():pass
    finally:
        main_queue.put(None)
        metric_queue.put(None)
        cancel_event.set()
        sys.stdout.flush()

@dataclass
class A3CLearner(AgentLearner):
    fitter_fn:Callable=a3c_data_fitter
    batch_sz:int=128
    discount:float=0.99
    entropy_beta:float=0.01
    clip_grad:float=0.1
    def init(self, init):print('skipping')

    def __post_init__(self):
        super(A3CLearner,self).__post_init__()
        if self.model is None:self.model=self.agent.model
        if self.agent.model is None: self.agent.model=self.model
        self.model.share_memory()
        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))

    def predict(self,s):
        out=self.agent(s)
        if type(out)==tuple:return out[0],None
        return out,None

# Cell
def r_estimate(s, r, d_mask,non_d_mask, model, val_gamma, device):
    "Returns rewards `r` estimated direction by `model` from states `s`"
    r_np = np.array(r, dtype=np.float32)
#     print(r_np[d_mask].mean(), r_np[non_d_mask].mean())
    #     print(len(d_mask),len(r),len(s))
    if len(d_mask) != 0:
        s_v = torch.FloatTensor(s).to(device)
        v = model(s_v)[1]  # Remember that models are going to return the actions and the values
        v_np = v.data.cpu().numpy()[:, 0]
        r_np[d_mask] += val_gamma * v_np
    return r_np

def unbatch(batch, model, last_val_gamma, device='cpu'):
    # print(batch[0])
    s, a, r, d_mask, sp = [], [], [], [], []
    non_d_mask = []
    for i, exp in enumerate(batch):
        s.append(exp.s.numpy())
        a.append(int(exp.a.numpy()))  # TODO can we change this to toggle between discrete and continuous actions?
        r.append(exp.r.numpy().astype(np.float32).reshape(1,))
        if not bool(exp.d):
            d_mask.append(i)
            sp.append(exp.sp.numpy().reshape(1,-1))
        else:
            non_d_mask.append(i)
    s_t = torch.FloatTensor(s).to(device)
    a_t = torch.LongTensor(a).to(device)
    # print(batch[0].r)
    # print(np.array(sp).mean(),np.array(r).mean())
    r_np = r_estimate(sp, r, d_mask, non_d_mask,model, last_val_gamma, device)
    estimated_r = torch.FloatTensor(r_np).to(device)
    # print(s_t.shape,a_t.shape,estimated_r.shape)
    return s_t, a_t, estimated_r

# Cell
debug_batch=[]

class A3CTrainer(LearnerCallback):
    def __init__(self,*args,**kwargs):
        super(A3CTrainer,self).__init__(*args,**kwargs)
        self.batch=[]

    @property
    def skip_process_batch(self):return len(self.batch)<self.learn.data.bs
    def on_train_begin(self,**kwargs):self.batch.clear()

    def on_batch_begin(self,last_target,**kwargs):
        self.batch.extend([Experience(**{k:v[i] if len(v)!=0 else None for k,v in last_target.items()}) for i in range(len(last_target['s']))])

    def on_backward_begin(self,last_loss,**kwargs):
        if self.skip_process_batch:return {'skip_bwd':self.skip_process_batch}
        s_t,a_t,r_est=unbatch(self.batch,self.learn.model,self.learn.discount**self.data.ds_kwargs['skip_n_steps'])
        r_est=r_est.squeeze(1)
#         print(a_t.float().mean(),s_t.mean(),r_est.mean())
#         print(a_t.shape,s_t.shape,r_est.shape)
#         r_est=r_est.squeeze(1)
        self.learn.opt.zero_grad()
        logits_v,value_v=self.learn.model(s_t)
#         print(logits_v.shape,s_t.shape)

        loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)
#         print((r_est.mean(),value_v.mean()))
        log_prob_v=F.log_softmax(logits_v,dim=1)
        adv_v=r_est-value_v.detach()
#         print(log_prob_v.shape)
        log_prob_actions_v=adv_v*log_prob_v[range(self.learn.data.bs),a_t]
        loss_policy_v=-log_prob_actions_v.mean()

        prob_v=F.softmax(logits_v,dim=1)
#         print(prob_v.max(),log_prob_v.max(),prob_v.min(),log_prob_v.min())
        entropy_loss_v=self.learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()

        loss_v=entropy_loss_v+loss_value_v+loss_policy_v

        self.learn.loss_func.loss=loss_v.detach()
        return {'last_loss':loss_v,'skip_bwd':self.skip_process_batch}


    def on_backward_end(self,*args,**kwargs):
        if not self.skip_process_batch:
            nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)
#             print(getModelconf(self.learn.model,True))
        return {'skip_bwd':self.skip_process_batch,
                'skip_step':self.skip_process_batch,
                'skip_zero':self.skip_process_batch}
    def on_step_end(self,last_loss,*args,**kwargs):
#         getBack(last_loss.grad_fn)
        if self.skip_process_batch:return
        self.batch.clear()