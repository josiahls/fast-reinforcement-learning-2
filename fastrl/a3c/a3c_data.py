# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/12_a3c.a3c_data.ipynb (unless otherwise specified).

__all__ = ['a3c_data_fitter', 'A3CLearner', 'A3CTrainer']

# Cell
# from fastai.basic_data import *
from fastai.torch_core import *
from fastai.callbacks import *
from ..wrappers import *
from ..basic_agents import *
from ..basic_train import *
from ..data_block import *
from ..metrics import *
from fastai.basic_train import *
from dataclasses import asdict
from functools import partial
from fastprogress.fastprogress import IN_NOTEBOOK
from fastcore.utils import *
import torch.multiprocessing as mp
from queue import Empty
import textwrap
import logging
import gym

logging.basicConfig(format='[%(asctime)s] p%(process)s line:%(lineno)d %(levelname)s - %(message)s',
                    datefmt='%m-%d %H:%M:%S')
_logger=logging.getLogger(__name__)

# Cell
@safe_fit
def a3c_data_fitter(model,agent,ds,data_queue,pause_event,
                    cancel_event,metric_queue):
    dataset=ds()
    while not cancel_event.is_set():
        for xb,yb in dataset:
            data_queue.put(yb)
            if pause_event.is_set():cancel_event.wait(0.1)
            if cancel_event.is_set():break
        if cancel_event.is_set():break
        if metric_queue is not None:
            rs=dataset.pop_total_r()
            if len(rs)!=0:metric_queue.put(TotalRewards(np.mean(rs)))

@dataclass
class A3CLearner(AgentLearner):
    fitter:Callable=a3c_data_fitter
    batch_sz:int=100

    def __post_init__(self):
        super(A3CLearner,self).__post_init__()
        if self.model is None:self.model=self.agent.model
        if self.agent.model is None: self.agent.model=self.model
        self.model.share_memory()

    def predict(self,s):
        out=self.model(s)
        if type(out)==tuple:return out[0]
        return out

# Cell
class A3CTrainer(LearnerCallback):
    def __init__(self,*args,**kwargs):
        super(A3CTrainer,self).__init__(*args,**kwargs)
        self.batch=[]

    def on_train_begin(self,**kwargs):
        self.batch.clear()

    def on_batch_begin(self,last_target,**kwargs):
        self.batch.extend([Experience(**o) for o in last_target])
        if len(self.batch)<self.learn.batch_sz:return

    def on_backward_begin(self,*args,**kwargs): return {'skip_bwd':True,'skip_validate':True}
