# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"PixelObservationWrapper": "01_wrappers.ipynb",
         "STATE_KEY": "01_wrappers.ipynb",
         "ActionSelector": "03_basic_agents.ipynb",
         "ArgmaxActionSelector": "03_basic_agents.ipynb",
         "EpsilonGreedyActionSelector": "03_basic_agents.ipynb",
         "ProbabilityActionSelector": "03_basic_agents.ipynb",
         "default_states_preprocessor": "03_basic_agents.ipynb",
         "float32_preprocessor": "03_basic_agents.ipynb",
         "BaseAgent": "03_basic_agents.ipynb",
         "TestAgent": "05c_data.ipynb",
         "DiscreteAgent": "03_basic_agents.ipynb",
         "DQNAgent": "03_basic_agents.ipynb",
         "TargetNet": "03_basic_agents.ipynb",
         "PolicyAgent": "03_basic_agents.ipynb",
         "ActorCriticAgent": "03_basic_agents.ipynb",
         "AgentLearner": "04_learner.ipynb",
         "Experience": "05a_ptan_extend.ipynb",
         "ExperienceSource": "05a_ptan_extend.ipynb",
         "ExperienceFirstLast": "05a_ptan_extend.ipynb",
         "ExperienceSourceFirstLast": "05a_ptan_extend.ipynb",
         "noopo": "05b_async_data.ipynb",
         "template_data_fit": "05b_async_data.ipynb",
         "DataFitProcess": "05b_async_data.ipynb",
         "safe_get": "05b_async_data.ipynb",
         "MultiProcessTfm": "05b_async_data.ipynb",
         "TotalReward": "05b_async_data.ipynb",
         "AsyncExperienceBlock": "05b_async_data.ipynb",
         "is_single_nested_tuple": "05c_data.ipynb",
         "TfmdSourceDL": "05c_data.ipynb",
         "TfmdSource": "05c_data.ipynb",
         "IterableDataBlock": "05c_data.ipynb",
         "SeedZeroWrapper": "05c_data.ipynb",
         "MakeTfm": "05c_data.ipynb",
         "env_display": "05c_data.ipynb",
         "envlen": "05c_data.ipynb",
         "ResetAndStepTfm": "05c_data.ipynb",
         "ExperienceBlock": "05c_data.ipynb",
         "FirstLastTfm": "05c_data.ipynb",
         "FirstLastExperienceBlock": "05c_data.ipynb",
         "AvgEpisodeRewardMetric": "13_metrics.ipynb",
         "weights_init_": "14_actorcritic.sac.ipynb",
         "ValueNetwork": "14_actorcritic.sac.ipynb",
         "QNetwork": "14_actorcritic.sac.ipynb",
         "GaussianPolicy": "14_actorcritic.sac.ipynb",
         "DeterministicPolicy": "14_actorcritic.sac.ipynb",
         "create_log_gaussian": "14_actorcritic.sac.ipynb",
         "logsumexp": "14_actorcritic.sac.ipynb",
         "soft_update": "14_actorcritic.sac.ipynb",
         "hard_update": "14_actorcritic.sac.ipynb",
         "SAC": "14_actorcritic.sac.ipynb",
         "ExperienceReplay": "20a_qlearning.dqn.ipynb",
         "SACCriticTrainer": "14_actorcritic.sac.ipynb",
         "SACLearner": "14_actorcritic.sac.ipynb",
         "LOG_SIG_MAX": "14_actorcritic.sac.ipynb",
         "LOG_SIG_MIN": "14_actorcritic.sac.ipynb",
         "epsilon": "14_actorcritic.sac.ipynb",
         "LinearA2C": "16_actorcritic.a2c.ipynb",
         "unbatch": "16_actorcritic.a2c.ipynb",
         "loss_func": "19_policy_gradient.trpo.ipynb",
         "A3CLearner": "15_actorcritic.a3c_data.ipynb",
         "A3CTrainer": "15_actorcritic.a3c_data.ipynb",
         "data_fit": "15_actorcritic.a3c_data.ipynb",
         "A2CTrainer": "16_actorcritic.a2c.ipynb",
         "A2CLearner": "16_actorcritic.a2c.ipynb",
         "ModelActor": "19_policy_gradient.trpo.ipynb",
         "ModelCritic": "19_policy_gradient.trpo.ipynb",
         "AgentA2C": "19_policy_gradient.trpo.ipynb",
         "HID_SIZE": "19_policy_gradient.trpo.ipynb",
         "calc_logprob": "19_policy_gradient.trpo.ipynb",
         "calc_adv_ref": "19_policy_gradient.trpo.ipynb",
         "GAMMA": "19_policy_gradient.trpo.ipynb",
         "GAE_LAMBDA": "19_policy_gradient.trpo.ipynb",
         "LEARNING_RATE_ACTOR": "19_policy_gradient.trpo.ipynb",
         "LEARNING_RATE_CRITIC": "19_policy_gradient.trpo.ipynb",
         "PPO_EPS": "19_policy_gradient.trpo.ipynb",
         "PPO_EPOCHES": "19_policy_gradient.trpo.ipynb",
         "PPO_BATCH_SIZE": "19_policy_gradient.trpo.ipynb",
         "PPOTrainer": "18_policy_gradient.ppo.ipynb",
         "PPOLearner": "18_policy_gradient.ppo.ipynb",
         "get_flat_params_from": "19_policy_gradient.trpo.ipynb",
         "set_flat_params_to": "19_policy_gradient.trpo.ipynb",
         "conjugate_gradients": "19_policy_gradient.trpo.ipynb",
         "linesearch": "19_policy_gradient.trpo.ipynb",
         "trpo_step": "19_policy_gradient.trpo.ipynb",
         "TRPO_MAX_KL": "19_policy_gradient.trpo.ipynb",
         "TRPO_DAMPING": "19_policy_gradient.trpo.ipynb",
         "TRPOTrainer": "19_policy_gradient.trpo.ipynb",
         "TRPOLearner": "19_policy_gradient.trpo.ipynb",
         "LinearDQN": "20a_qlearning.dqn.ipynb",
         "EpsilonTracker": "20a_qlearning.dqn.ipynb",
         "calc_target": "20a_qlearning.dqn.ipynb",
         "DQNTrainer": "20b_qlearning.dqn_n_step.ipynb",
         "DQNLearner": "20a_qlearning.dqn.ipynb",
         "NStepDQNLearner": "20b_qlearning.dqn_n_step.ipynb"}

modules = ["wrappers.py",
           "basic_agents.py",
           "learner.py",
           "ptan_extension.py",
           "async_data.py",
           "data.py",
           "metrics.py",
           "actorcritic/sac.py",
           "actorcritic/a3c_data.py",
           "actorcritic/a2c.py",
           "actorcritic/dads.py",
           "qlearning/dqn.py",
           "qlearning/dqn_n_step.py"]

doc_url = "https://josiahls.github.io/fast-reinforcement-learning-2/"

git_url = "https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/"

def custom_doc_links(name): return None
